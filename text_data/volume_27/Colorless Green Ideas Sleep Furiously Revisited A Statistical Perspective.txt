UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Colorless Green Ideas Sleep Furiously Revisited: A Statistical Perspective
Permalink
https://escholarship.org/uc/item/3ch2g3kh
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 27(27)
Authors
Christiansen, Morten H.
Dale, Rick
Reali, Florencia
Publication Date
2005-01-01
Peer reviewed
 eScholarship.org                                  Powered by the California Digital Library
                                                                      University of California

           Colorless Green Ideas Sleep Furiously Revisited: A Statistical Perspective
                                                 Florencia Reali (fr34@cornell.edu)
                                                    Rick Dale (rad28@cornell.edu)
                                          Morten H. Christiansen (mhc27@cornell.edu)
                                Department of Psychology; Cornell University; Ithaca, NY 14853 USA
                              Abstract                                  distributional regularities may provide an important source
                                                                        of information for bootstrapping syntax (e.g., Redington,
   In the present study we provide empirical evidence that              Chater & Finch, 1998; Mintz, 2002)—especially when
   human learners succeed in an artificial-grammar learning task        integrated with prosodic or phonological information (e.g.,
   that involves recognizing grammatical sequences whose                Morgan, Meier & Newport, 1987; Monaghan, Chater &
   bigram frequencies from the training corpus are zero. This
   result begs explanation: Whatever strategy is being used to
                                                                        Christiansen, in press). Moreover, statistical approaches
   perform the task, it cannot rely on the simple co-occurrence of      have been supported by recent research demonstrating that
   elements in the training corpus. While rule-based mechanisms         young infants are sensitive to statistical information inherent
   may offer an account, we propose that a statistical learning         in bigram transitional probabilities (e.g., Saffran, Aslin &
   mechanism is able to capture these behavioral results. A             Newport, 1996; –for a review, see Gómez & Gerken, 2000).
   simple recurrent network is shown to learn sequences that            These studies demonstrate that at least some learning
   contain null-probability bigram information by simply relying        mechanisms employed by infants are statistical in nature.
   on distributional information in a training corpus. The present      However, as suggested by the perceived grammaticality of
   results offer a simple but stark challenge to previous
                                                                        sentences like (1), human learning capacities certainly need
   objections to statistical learning approaches to language
   acquisition that are based on sparseness of the primary
                                                                        to go beyond the information conveyed by item co-
   linguistic data.                                                     occurrences. In the present study we explore the extent to
                                                                        which humans are capable of learning the regularities of an
                                                                        artificial grammar, and generalizing them to new sentences
                          Introduction                                  in which transitional probabilities are completely
                                                                        uninformative. The task involves “discovering” the
The importance of statistical structure in language learning
                                                                        underlying regularities and using them to recognize
and processing has been a matter of intense debate. Initial
                                                                        sequences in which the bigram transitions are completely
data-driven empirical approaches embraced the idea that
                                                                        novel. We find that humans perform well in this task.
word co-occurrences are important sources of information in
                                                                                   Two possible explanations could account for these
language processes (e.g., Harris, 1951). This approach fell
                                                                        results. First, as previously suggested (Marcus, Vijayan,
out of favor in the 1950’s, in part due to the influential work
                                                                        Bandi Rao & Vishton, 1999), it could be that humans
of Noam Chomsky (1957) who believed that language
                                                                        possess at least two learning mechanisms, one for learning
behavior should be analyzed at a much deeper level than its
                                                                        statistical information and another for learning “algebraic”
surface statistics. In one of his most famous examples, he
                                                                        rules. Thus, regardless of available statistics, we could rely
pointed out that it is reasonable to assume that neither the
                                                                        on open-ended abstract relationships into which we
sentence (1) Colorless green ideas sleep furiously nor (2)
                                                                        substitute arbitrary items. In an artificial-grammar learning
Furiously sleep ideas green colorless has ever occurred, and
                                                                        scenario, we could know the structure or rules underlying a
yet (1), though nonsensical, is grammatical, while (2) is not.
                                                                        grammar and substitute variables with specific examples by
Therefore, a common argument against statistical
                                                                        mechanisms independent of the surface statistical
approaches to language is that there are sentences
                                                                        information. This rule-based mechanism could therefore
containing low or zero probability sequences of words that
                                                                        account for our ability to successfully generalize to
can nonetheless be judged as grammatical. As Chomsky
                                                                        sequences with uninformative bigram probabilities.
remarked, “… we are forced to conclude that ... probabilistic
                                                                        Alternatively, we suggest that there is a second and equally
models give no particular insight into some of the basic
                                                                        plausible account. In this paper we demonstrate that this
problems of syntactic structure" (Chomsky, 1957, p. 17).
                                                                        generalization can be accounted for on the basis of
Most theoretical linguists have accepted this argument,
                                                                        distributional learning. In the second part of this paper, we
developing little interest in the role of statistical approaches
                                                                        show that a simple connectionist model, trained purely on
to language.
                                                                        distributional information, is capable of simulating correct
           Recently there has been a reappraisal of statistical
                                                                        grammaticality judgments of test sentences that comprise
approaches, partly motivated by research indicating that
                                                                        bigram transitions absent in the training corpus. These
                                                                   1821

results build on previous work showing that lexical                set contained at least one bigram transition (co-occurrence
categories can emerge naturally from learning processes            of two letters) that had never been presented in the training
inherent to the SRN’s distributionally driven internal             set. To accomplish that, strings containing the following
representations (Elman, 1990). They also demonstrate that          bigram transitions were excluded from the training set: adj1
the distributed nature of SRNs’ storage allows                     n1 (C P), n1 v1 (P W), v1 adv1 (W H).
generalization that goes beyond traditional computational                    Grammatical sequences in the test set fell under
models (such as simple n-gram models) whose limitations            one of three categories: The first category included
motivated a historical shift away from statistical approaches.     sentences with just one null-probability transition ([adj1 n1]
While these models are sensitive only to the information in        X adv1; X [n1 v1] X; adj1 X [v1 adv1], where “X” represents
the co-occurrence of word sequences, SRNs go beyond co-            some arbitrary grammatical word); the second set contained
occurrence information, being capable of forming useful            two null-probability transitions ([adj1 n1 v1] X ; X [n1 v1
representations of lexical classes. This study is therefore        adv1]) and the third category sentences had sentences
important in demonstrating the need to look deeper at              containing three null-probability transitions ([adj1 n1 v1
learning properties of more sophisticated distributional           adv1]). The latter category represents the artificial version of
models, such as connectionist networks, in order to reassess       “colorless green ideas sleep furiously”. The test set itself
the claims of weakness many cast onto a statistical approach       contained six grammatical sequences of the first category,
to language learning and processing.                               two sequences of the second category and one sequence of
                                                                   the third category. However, each sequence was presented
     Experiment 1: Learning Null-Probability                       twice in random order, thus, participants saw a total of
                        Sequences                                  eighteen grammatical sequences in the test session.
                                                                             Ungrammatical sequences fell under one of two
In this experiment, we explore whether learners are capable
                                                                   categories: In the first category two words were
of generalizing to novel sequences after being exposed to
                                                                   interchanged (n1 adj1 v1 adv1; adj1 v 1 n1 adv1; adj1 n1 adv1
examples from a constrained subset of all possible
                                                                   v1; v1 n1 adj1 adv1; adv1 n 1 v1 adj1; adj1 adv1 v1 n1); and in
grammatical sequences. Crucially, participants will be asked
                                                                   the second category all words were interspersed (n1 adj1
to recognize sequences whose bigram transitions did not
                                                                   adv1 v1; v 1 adv1 adj1 n1; adv1 v1 n1 adj1). Each sequence
occur the training corpus.
                                                                   was presented twice in random order, thus, a total of
                                                                   eighteen ungrammatical sequences comprised the test
Method
                                                                   session.
Subjects       Forty-nine undergraduate participants were
recruited at Cornell University in exchange for extra credit       Procedure The experiment was conducted using the
in psychology classes.                                             Psyscope experimental software package (Cohen,
                                                                   MacWhinney, Flatt, & Provost, 1993) with stimuli
Materials The stimuli were sequences of capital letters            presented on a computer monitor. Participants were
generated from a simple artificial phrase-structure grammar        instructed that they were participating in a memory
defined as follows:                                                experiment. They were told that in the first part of the
                                                                   experiment they would see sequences of letters displayed on
          S                Adj N V Adv                            the screen and had to type the sequence they just saw. Each
          Adj              {adj1 adj2 adj3}                       sequence was presented individually for a period of 4
          N                {n1 n2 n3}                             seconds. The 60 sequences of the training set were
          V                {v1 v2 v3}                             presented twice, for a total of 120 input exposures,
          Adv              {adv1 adv2 adv3}                       presented in random order. Immediately after seeing each
                                                                   sequence, participants typed it using the computer keyboard,
Note that the vocabulary of the grammar consists of 12             before going to the next one.
words, 3 in each of lexical categories of adjective (adjn),                  After the training phase, participants were
noun (nn), verb (vn), and adverb (advn). The stimuli we used       instructed that they would be exposed to a new set of
consisted of twelve consonants, C, Q, M, P, X, S, W, Z, K,         sequences some of which were “similar” to the ones they
H, T and L, which represented each of the twelve words of          saw in the first part of the experiment and some
the vocabulary respectively (adj1, adj2, and adj3 = C, Q, and      “dissimilar.” They were instructed to press a button marked
M; n1, n2 and n3 = P, X, and S; v1, v2, and v3 = W, Z, and K;      “YES” or “NO” according to whether they thought a
adv1, adv2, and adv3 = H, T and L). Grammatical sequences          presented string was similar to the ones they saw in the
consisted of a four letters string where the first one is an       previous phase. The participants were instructed that they
adjective, followed by noun, a verb and an adverb in that          would probably find the task difficult and therefore they
order.                                                             should follow their first impression without spending too
          Participants were presented with sixty grammatical       much time thinking about each sequence. Each of the 18
sequences in a training phase. The test session comprised a        sequences comprising the test set (9 grammatical and 9
set of nine grammatical and nine ungrammatical sequences.
Both grammatical and ungrammatical sequences in the test
                                                              1822

ungrammatical) was presented twice, and all of them were                         In order to address this question we performed a
randomly interspersed.                                                  series of computational simulations in which SRNs were
                                                                        trained using purely distributional cues, and without any
Results and discussion                                                  labeling of lexical categories. After the training of the
The mean number of correct endorsements on the 36 test                  network, we tested sentences in which bigram frequencies
items was 25.30 (70.28%). A one-way t-test indicates that               were zero in the training corpus.
this performance is significantly above chance (t(48) =
10.32, p < .0001). We explored the percentage of correct                Experiment 2: Connectionist Learning of Null-
endorsements across grammatical and ungrammatical                                 Probability Sequences.
categories. As illustrated in Fig. 1, the grammatical                   In this simulation, simple recurrent networks (SRN; Elman,
sequences containing one, two and three novel bigram                    1990) are trained to predict the next word in a sentence
transitions were correctly recognized as grammatical 72.2%,             given a corpus of sentences generated by an artificial
64.9% and 59.2 % of the time respectively, whereas                      grammar. Each word was assigned a unique vector
ungrammatical sequences with two or all letters                         consisting of 0s and a single 1 in a so-called localist
interchanged, were incorrectly labeled as grammatical                   representation. The representation deliberately deprives the
28.8% and 27.9% of the time respectively. We also                       network of any information about grammatical category,
computed planned comparisons between the number of yes-                 such as its syntactic distribution or semantics, etc. This type
responses (grammatical-labeling) elicited by each of the                of input and output representation is the same as the one
three grammatical categories vs. the number of yes-                     originally used by Elman (1990), and is often employed in
responses elicited by the ungrammatical sentences. We                   connectionist simulations. It is important to note that the
found that each of the three different types of grammatical             only type of information the network can rely on to learn the
sentences elicited significantly more yes-responses than the            grammar is the distribution of these localist representations
ungrammatical sentences (all p’s < .001). These results                 presented sequentially. As a new word is input, the
indicate that grammatical sequences with one, two or three              network’s task is to predict the next word in the sentence.
null-probability bigram transitions are successfully                    As in the experiment, we prevented the network from being
distinguished from ungrammatical sequences. Importantly,                exposed to certain sequences of words during training. This
subjects are capable of learning the pattern after being                constraint allowed us to create grammatical test sentences in
exposed to only a small number of examples.                             which all transitions had null probability, that is, sentences
                                                                        in which consecutive words never co-occur in the training
                                                                        set.
                                 80                                               While we are not postulating SRNs as exact
   percentage of yes-responses
                                 70                                     emulators of human learning mechanisms here, we argue
                                 60                                     that they can be viewed as a model of what can be acquired
                                 50                                     by a system that is not dependent on rule-based
                                 40                                     mechanisms. Indeed, the SRN is well suited for such
                                 30                                     simulations, and has been successfully applied to a wide
                                 20                                     range of language learning and processing phenomena (e.g.,
                                 10                                     Elman, 1990; Cleeremans, 1993; Christiansen & Chater,
                                  0                                     1999). Importantly, neural networks are not simply lookup
                                      G1   G2   G3   UG1   UG2          tables; instead, they are statistically-driven function
                                                                        approximators capable of complex generalization in a
                                                                        human-like fashion (Elman, 1993).
Fig. 1: Percentage of elicited yes-responses across subjects. G3,               Additionally, although the task performed in
G2, G1 = Grammatical sentences comprising three, two and one
probability-cero transitions respectively; UG1, UG2 =
                                                                        Experiment 1 is not identical to the SRN’s prediction task,
Ungrammatical sentences comprising two words interchanged and           they share the fact that both involve learning an artificial
all words interspersed respectively.                                    grammar and generalizing to new sentences in which
                                                                        transitional probabilities are uninformative.
These results are not necessarily surprising: We know
humans are good at making grammaticality judgments of                   Method
sentences they have not previously encountered. Thus, the               Networks The SRNs were used with initial weight
crucial question is what kind of learning mechanism                     randomizations in the interval [-0.1; 0.1]. Learning rate was
underlies success in this task. In particular, it is not clear          set to 0.1, and momentum to 0.9. Each input to the network
whether these results reflect the manifestation of rule-based           contained a localist representation of the incoming word.
learning mechanism of the kind proposed by Marcus et al.                With a total of 36 different words and a pause marking
(1999), or alternatively, whether these results might reflect           boundaries between utterances, the network had 37 input
emergent learning resulting from acquisition processes that             units. The network was trained to predict the next word in a
rely only on statistical information.                                   sequence, and thus the number of output units was 37. Each
                                                                 1823

network additionally had 40 hidden units and 40 context
units.                                                              Procedure An SRN was trained on a single training set and
                                                                    tested. The training consisted of 5 passes through the
Materials We trained and tested the network on an                   training corpus. Performance was assessed based on the
artificial grammar, containing a vocabulary composed of 8           networks’ ability to predict the next word given the prior
adjectives, 12 nouns, 10 verbs, 6 adverbs. While we have            context. In order to compute statistical comparisons we
equal numbers of category members in Experiment 1, we               repeated the procedure with the ten different training
chose this distribution to meet loosely the distribution of         corpora using different initial connection weights.
such classes in a natural language such as English. The
training corpus contained 500 sentences. Sentences were             Results and discussion
generated from a simple artificial grammar defined as               Each word was represented by the activation of a single unit
follows:                                                            in the output layer. After training, SRNs trained with
                                                                    localist output representations will produce a distributional
         S                Adj Adj N V Adv                          pattern of activation closely corresponding to a probability
         S                Adj N V Adv                              distribution of possible next items. In order to assess the
         S                Adj N V                                  overall performance of the SRNs, we computed the average
         S                N V Adv                                  mean square error (MSE) in predicting the next word across
         S                NV                                       each test sentence.
                                                                         Results are displayed in Fig. 2: The average MSEs were
         Adj              {adj1, …, adj8}                          0.75, 0.82, and 0.95 for grammatical, ungrammatical type I,
         N                {n1, …, n12}                             and ungrammatical type II respectively. We found that the
         V                {v1, …, v10}                             difference between the MSE elicited by grammatical
         Adv              {adv1, …, adv6}                          sentences was significantly lower than the MSE elicited by
                                                                    ungrammatical type I (t(9) = 4.66, p < 0.005) and
Ten different training sets were generated using a random           ungrammatical type II (t(9) = 13.15, p < 0.001). To establish
algorithm to create sentences. Importantly, all sentences           a baseline, we also computed the average MSE elicited
were created according to the following restriction: Some of        across all the sentences contained in the training set after the
the words from each lexical category were prevented from            training stage. Interestingly the difference of MSE between
occurring next to other ones. Specifically, the following           the grammatical test sentence comprising null-probability
sequences were not allowed to co-occur in the training set:         bigram transitions and the MSE elicited by grammatical
Adj2 never occurred after to Adj1, N1 never occurred after          sentences contained in the training set was not statistically
Adj2, V1 never occurred after N1, and Adv1 never occurred           significant (t(9) = 0.13; p = 0.84), suggesting that the
after V1. This generation constraint allowed us to produce          network recognized the novel sentence as one of the training
the following grammatical test sentence in which all                set.
transitional probabilities (bigram frequencies) had null
probability in the training corpus: Adj1 Adj2 N1 V1 Adv1.                      1
This test sentence represents is a toy-model version of the                                                    traning sentences
famous “Colorless green ideas sleep furiously”.                               0.9
         The test set consisted in three target sentences, all                                                 grammatical
                                                                              0.8
                                                                        MSE
of which had probability-zero transitions but varied in
                                                                                                               ungrammatical
degree of grammaticality:                                                     0.7                              type I
                                                                                                               ungrammatical
1) Grammatical:           Adj1 Adj2 N1 V1 Adv1                                0.6                              type II
2) Ungrammatical type I: *Adj1 N1 Adj2 V1 Adv1
                                                                              0.5
3) Ungrammatical type II: *Adv1 V1 N1 Adj2 Adj1
Note that in 2) the ungrammatical sentence consists in only         Fig. 2: Mean square error across words in four type of sentences:
a single interchange of words with respect to Sentence 1,           Striped pattern: average across all words in the training sent;
                                                                    White: grammatical test sentence comprising null-probability
while in 3) the ungrammatical sentence consists in the
                                                                    transitions; Light gray: ungrammatical type I test sentence; Dark
complete reversal of Sentence 1. Thus, 3) corresponds to a          gray: ungrammatical type II test sentence. Displayed values result
toy-model version of the famous “Furiously sleep ideas              from the average across the five simulations using different
green colorless”. We want to explore whether the network is         training sets.
sensitive to distances between the grammatical sentence 1)
and the two ungrammatical versions 2) and 3). We therefore          But how do the SRNs stack up against more traditional
expect that sentence 3) elicits a higher error than sentence        statistical models whose weaknesses compelled Chomsky
2), and conversely, we expect that sentence 2) elicits a            (1957) to abandon probabilistic methods? N-gram models
higher error than sentence 1).                                      are standard statistical models used in psycholinguistics that
                                                             1824

are based on co-occurrences of words in natural language                                            The activation values displayed in Fig. 3 illustrate that the
corpora. Traditional n-gram models trained on the same                                              networks are successfully learning to predict the next lexical
corpus here would therefore assign equal probability to test                                        class. These results demonstrate that SRNs trained purely on
sentences (1), (2), and (3) above. The results obtained here                                        distributional information are sensitive to grammaticality
demonstrate that SRNs are capable of going beyond n-gram                                            differences between different sentences in which bigram
models in generalizing to new input.                                                                transitions have null probabilities.
          As an illustration, Figure 3 shows the mean
activation of the output units at different points in the                                                             General Discussion
sequence. The graph in Fig. 3A shows the averaged mean                                              Even among the billions of words in available databases,
activation of the SRNs after being presented with the test                                          innumerable reasonable sentences remain absent. This so-
sequences of words: “Adj1 Adj2 N1 …”. The figure shows                                              called sparse data problem continues to be a serious
the averaged mean activation of the units corresponding to                                          challenge not only for the study of human language
adjectives (ADJ), noun (N), and adverbs (ADV), while the                                            acquisition and processing, but also in the area of artificial
activations for each of the individual verb units (V1 through                                       intelligence devoted to natural language processing (see
V10) are shown in detail. Even though V1 never occurred                                             Lee, 2004). The results of Experiment 1 reveal that humans
after N1 in the training set, the activation of V1 elicited by                                      become sufficiently sensitive to the regularities of training
the string “Adj1 Adj2 N1 …” is comparable to the activation                                         examples to recognize novel sequences whose bigram
of other verbal verbs, such as V5. Fig. 3B shows the                                                transitions are absent in training. Therefore, subjects must
averaged mean activation of the SRNs after being presented                                          be relying on something other than co-occurrence of
with the test sequences of words: “Adj1 Adj2 N1 V1 …”. The                                          consecutive elements to generalize from our experimentally
activation of adjective, noun, and verbs units are shown                                            induced sparse sentence samples. The remaining question
averaged, while the activations of each of the adverb-units                                         concerns what type of cognitive mechanism can accomplish
(Adv1 through Adv6) are shown in detail. The activation of                                          this task. One such mechanism might be the rule-based
Adv1 elicited by the string “Adj1 Adj2 N1 V1…” is                                                   learning mechanism recommended by Marcus et al. (1990)
comparable to (and in some cases higher than) the activation                                        and others (e.g., Peña, Bonatti, Nespor, & Mehler, 2002),
of other adverbial units, despite the fact that Adv1 never                                          which does not rely on statistical learning. Alternatively, the
occurred after V1.                                                                                  implicit knowledge of the underlying regularities needed to
                                                                                                    succeed in the task could be acquired by distributional
A                                                                                                   learning through training exemplars. Our connectionist
                                     Adj1 Adj2 N1 ...                                               simulations in Experiment 2 provide some evidence that the
                   0.08                                                                             latter alternative should be considered.
 mean activation
                                                                                                               It has been previously argued by Elman (1990, see
                   0.06
                                                                                                    also Elman, 2004) that SRNs are capable of forming internal
                   0.04                                                                             representations of grammatical classes from distributional
                   0.02                                                                             information. The present findings build on that idea,
                                                                                                    showing that SRNs are capable of good performance in the
                      0                                                                             prediction task even in sentences having null transitional
                                N
                          ADJ
                                          v2      v5               v8
                                                                                      ADV
                                     V1   v3
                                          v4      v6
                                                  v7               v9                               probabilities relative to the training corpus. Previous studies
                                                                  v10
                                                                                                    have demonstrated that the network uses distributional
B                                                                                                   information to induce categories. These categories are
                                    Adj1 Adj2 N1 V1 ...                                             reflected in the analysis of hidden unit activations evoked in
                                                                                                    response to each word (e.g., Christiansen & Chater, 1999;
                   0.06
                                                                                                    Reali et al. 2003; see also Elman, 2004). These analyses
 mean activation
                   0.05
                                                                                                    involve measures in terms of Euclidean distance in the
                   0.04
                                                                                                    hidden unit space, representing the similarity of words’
                   0.03
                                                                                                    hidden unit activations, and cluster according to lexical
                   0.02
                                                                                                    categories. Interestingly, the present results show that
                   0.01
                                                                                                    SRNs’ prediction of the next word seems to be at least in
                      0
                                N                                                                   part determined by lexical category membership, rather than
                          ADJ                     adv1    adv2   adv3   adv4   adv5   adv6
                                          V
                                                                                                    being determined by specific word co-occurrences in the
                                                                                                    training corpora. This is an important achievement for a
Fig. 3: Mean activation across different units elicited by previous                                 distributional learning mechanism, seeing as it was not
context. A) Activation elicited by the word substring: “Adj1 Adj2
                                                                                                    provided with information about grammatical classes.
N1 …”. B) Activation elicited by the word substring: “Adj1 Adj2
N1 V1…”. ADJ, N, V, ADV: mean activation across adjective,
                                                                                                    Traditional n-gram models of language are not capable of
noun, verb, and adverb units respectively. v1, v2,…,v10, adv1,                                      representing lexical classes in the same way. Standard
adv2,…,adv6: Individual activation of verbal and adverb units                                       bigram or trigram models trained on our artificial grammar,
respectively.                                                                                       would assign exactly the same probability to all our test
                                                                                             1825

sentences. SRNs seem to be effective in learning something             Research Methods, Instruments, and Computers, 25, 257-
that goes beyond surface properties of language, suggesting            271.
they could be understood as “regularity discoverers” rather          Crain, S., & Pietroski, P. (2001). Nature, nurture and
than mere statistical learners resembling n-gram models.               universal grammar. Linguistics and Philosophy, 24, 139-
The present results are consistent with previous arguments             186.
about connectionist models’ generalization properties                Elman, J.L. (1990). Finding structure in time. Cognitive
(Christiansen & Chater, 1994). Recently, Allen &                       Science, 14, 179-211.
Seidenberg (1999), used connectionist simulations to show            Elman, J.L. (1993). Learning and development in neural
that low probability sentences like (1) could be statistically         networks: The importance of starting small. Cognition,
learned when other information such as word types or                   48, 71-99.
semantics are used in its comprehension. Simulations in              Elman, J.L. (2004). An alternative view of the mental
Experiment 2 build on these previous studies by                        lexicon. Trends in Cognitive Science, 7, 301-306.
demonstrating that pure distributional information can               Gómez, R.L., & Gerken, L.A. (2000). Infant artificial
provide a basis in the process of learning low probability             language learning and language acquisition. Trends in
sentences.                                                             Cognitive Sciences, 4, 178-186.
          In order to dismiss statistical approaches to              Harris, Z. (1951). Methods in Structural Linguistics.
language, particularly through limitations imposed by sparse           University of Chicago Press. Reprinted by Phoenix Books
data issues, it is necessary to thoroughly understand the              in 1960 under the title Structural Linguistics.
learning capabilities of systems such as connectionist               Lee, L. (2004). I’m sorry Dave, I’m afraid I can’t do that:
models. The present results challenge one of the most well-            Linguistics, statistics, and natural language processing
established objections to statistical approaches, which might          circa 2001. In Computer Science: Reflections on the
be based on an underestimation of the ability of                       Field, Reflections from the Field. (pp. 111-118).
connectionist models to deal with sparse input. One of the             Washington, DC: National Academies Press.
principal arguments for innateness of grammar, often                 Marcus, G.F., Vijayan, S., Bandi Rao, S., & Vishton, P.M.
referred to as “Poverty of the Stimulus” logic (e.g., Crain &          (1999). Rule learning by seven-month-old infants.
Pietroski, 2001), is based on precisely that property of the           Science, 283, 77-80.
linguistic data: sparseness. It is therefore crucial to              Mintz, T.H.(2002) Category induction from distributional
determine the extent to which connectionist models and                 cues in an artificial language. Memory & Cognition, 30,
statistical approaches in general can overcome some of the             678-686.
difficulties related to the sparseness of the linguistic input.      Monaghan, P., Chater, N. & Christiansen, M.H. (in press).
The present study constitutes a step in that direction.                The differential role of phonological and distributional
                                                                       cues in grammatical categorisation. Cognition.
                                                                     Morgan, J. L., Meier, R.P., & Newport, E.L. (1987).
                         References                                    Structural packaging in the input to language learning:
Allen, J., & Seidenberg, M.S. (1999). The emergence of                 Contributions of prosodic and morphological marking of
   grammaticality in connectionist networks. In B.                     phrases to the acquisition of language. Cognitive
   MacWhinney (Ed.) The Emergence of Language.                         Psychology, 19, 498–550.
   Mahwah, NJ: Lawrence Erlbaum.                                     Peña, M., Bonatti, L.L., Nespor, M., & Mehler J. (2002).
Botvinick M., & Plaut, D. C. (2004). Doing without schema              Signal-driven computations in speech processing.
   hierarchies: A recurrent connectionist approach to normal           Science, 298, 604-607.
   and impaired routine sequential action. Psychological             Reali, F., Christiansen, M.H. & Monaghan, P. (2003).
   Review, 111, 395-429.                                               Phonological and Distributional Cues in Syntax
Chomsky N. (1957). Syntactic Structures. Mouton and co.:               Acquisition: Scaling up the Connectionist Approach to
   The Hague.                                                          Multiple-Cue In Proceedings of the 25th Annual
Christiansen, M.H. & Chater, N. (1994). Generalization and             Conference of the Cognitive Science Society (pp. 970-
   connectionist language learning. Mind and Language, 9,              975). Mahwah, NJ: Lawrence Erlbaum.
   273-287.                                                          Redington, M., Chater, N., & Finch, S. (1998).
Christiansen, M.H., & Chater, N. (1999). Toward a                      Distributional information: A powerful cue for acquiring
   connectionist model of recursion in human linguistic                syntactic categories. Cognitive Science, 22, 425-469.
   performance. Cognitive Science, 23, 157-205.                      Saffran, J.R., Aslin, R., & Newport, E.L. (1996). Statistical
Cleeremans, A.. (1993). Attention and awareness in                     learning by 8- month-old infants. Science, 274, 1926-
   sequence learning. In Proceedings of the Fifteenth Annual           1928.
   Conference of the Cognitive Science Society. (pp. 330-
   335). Mahwah, NJ: Lawrence Erlbaum.
Cohen, J. D., MacWhinney, B., Flatt, M., & Provost, J.
   (1992). Psyscope: A new graphic interactive environment
   for designing psychology experiments. Behavioral
                                                                1826

