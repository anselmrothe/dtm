UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Computational Laughing: Automatic Recognition of Humorous One-Liners
Permalink
https://escholarship.org/uc/item/6wg3b07c
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 27(27)
Authors
Miwa, Kazuhisa
Nakazawa, Daisuke
Saito, Hitomi
Publication Date
2005-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

                                         Computational Laughing:
                      Automatic Recognition of Humorous One-liners
                                        Rada Mihalcea (rada@cs.unt.edu)
                               Department of Computer Science, University of North Texas
                                                    Denton, Texas, USA
                                       Carlo Strapparava (strappa@itc.it)
                                 ITC-irst, Istituto per la Ricerca Scientifica e Tecnologica
                                                     Povo, Trento, Italy
                          Abstract                               one-liners. A one-liner is a short sentence with comic ef-
                                                                 fects and an interesting linguistic structure: simple syn-
   Humor is one of the most interesting and puzzling as-         tax, deliberate use of rhetoric devices (e.g. alliteration
   pects of human behavior. Despite the attention it has         and/or rhyme), and frequent use of creative language
   received in fields such as philosophy, linguistics, and
   psychology, there have been only few attempts to cre-         constructions meant to attract the readers’ attention.
   ate computational models for humor recognition or gen-        While longer jokes can have a relatively complex narra-
   eration. In this paper, we bring empirical evidence           tive structure, the one-liners must produce the humorous
   that computational approaches can be successfully ap-         effect “in one shot”, with very few words. These char-
   plied to the task of humor recognition. Through ex-
   periments performed on very large data sets, we show          acteristics make this type of humor particularly suitable
   that automatic classification techniques can be effec-        for use in an automatic learning setting, as the humor-
   tively used to distinguish between humorous and non-          producing features are guaranteed to be present in the
   humorous texts, with significant improvements observed        first (and only) sentence.
   over apriori known baselines.
                                                                    We attempt to formulate the humor-recognition prob-
                                                                 lem as a traditional machine learning task, and feed pos-
                      Introduction                               itive (humorous) and negative (non-humorous) examples
                                                                 to an automatic classifier. The humorous data set con-
Humor is an essential element in personal communica-             sists of one-liners collected from the Web using an au-
tion. Although strictly related to themes such as enter-         tomatic bootstrapping process. The non-humorous data
tainment, fun, and emotion, it is an integral part of our        is selected such that it is structurally and stylistically
lives, and arguably humans could not survive without             similar to the one-liners. Specifically, we use three dif-
it. Indeed, while it is merely considered a way to induce        ferent negative data sets: (1) Reuters news titles; (2)
amusement, humor also has a positive effect on the men-          proverbs; and (3) sentences from the British National
tal state of those using it and has the ability to improve       Corpus (BNC). The classification results achieved with
their activity. Therefore computational humor deserves           these data sets are very encouraging, with accuracy fig-
particular attention, as it has the potential of changing        ures ranging from 77.84% (one-liners/BNC) to 96.89%
computers into a creative and motivational tool for hu-          (one-liners/Reuters). Regardless of the non-humorous
man activity [Stock et al., 2002, Nijholt et al., 2003].         data set playing the role of negative examples, the per-
   While previous work in computational hu-                      formance of the automatically learned humor-recognizer
mor has focused mainly on the task of hu-                        is always significantly better than apriori known base-
mor        generation        [Stock and Strapparava, 2003,       lines.
Binsted and Ritchie, 1997], very few attempts have                  The remainder of the paper is organized as follows. We
been made to develop systems for automatic humor                 first describe the humorous and non-humorous data sets,
recognition [Taylor and Mazlack, 2004]. This is not              and provide details on the Web-based bootstrapping pro-
surprising, since, from a computational perspective,             cess employed in building a very large collection of one-
humor recognition appears to be significantly more               liners. We then show experimental results obtained on
subtle and difficult than humor generation.                      these data sets using two different text classifiers. Fi-
   In this paper, we explore the applicability of compu-         nally, we conclude with a discussion and directions for
tational approaches to the recognition of verbally ex-           future work.
pressed humor. In particular, we investigate whether
text classification techniques are a viable approach to
distinguish between humorous and non-humorous text,
                                                                  Humorous and Non-humorous Data Sets
and we bring empirical evidence in support of this hy-           To test our hypothesis that automatic classification tech-
pothesis through experiments performed on very large             niques are a viable approach to humor recognition, we
data sets.                                                       needed in the first place a data set consisting of both hu-
   Since a deep comprehension of humor in all of its as-         morous and non-humorous examples. Once constructed,
pects is probably too ambitious and beyond the exist-            such data sets can be used to automatically learn compu-
ing computational capabilities, we chose to restrict our         tational models for humor recognition, and at the same
investigation only to the type of humor found in the             time evaluate the performance of such models.
                                                            1513

   While there is plenty of non-humorous data that can
play the role of negative examples, it is significantly                              automatically identified
harder to build a very large and at the same time suf-                                      one−liners
ficiently “clean” data set of humorous examples. We
conducted our experiments using two sets of humorous                                    seed one−liners
(positive) examples, each of them maximizing a differ-
ent aspect of the data: (1) Data quality: a small set
of manually assembled data, guaranteed to be “clean”,                                     Web search
and (2) Data quantity: a very large set of examples auto-
matically collected, which is likely to also include noisy
examples.
                                                                                      webpages matching
Humorous Data                                                                         thematic constraint (1)?
For reasons outlined earlier, we restrict our attention
to one-liners, short humorous sentences that have the                                              yes
characteristic of producing a comic effect in very few
words (usually 15 or less). The one-liners humor style                                       candidate
is illustrated in Table 1, which shows three examples of                                     webpages
such one-sentence jokes.
   It is well-known in the machine learning community
that large amounts of training data have the potential
of improving the accuracy of the learning process, and at
                                                                                    enumerations matching
the same time providing insights into how increasingly                              stylistic constraint (2)?
larger data sets can affect the classification precision.
However, the manual construction of a very large one-
                                                                                                   yes
liner data set may be problematic, as most Web sites
and mailing lists that make available such jokes do not
usually list more than 50–100 one-liners. To circum-                  Figure 1: Web-based bootstrapping of one-liners.
vent this problem, we designed and implemented an au-
tomatic bootstrapping approach, which was used to au-
tomatically construct a very large collection of 20,000          sists of six words that explicitly indicate humor-related
one-liners.                                                      content: oneliner, one-liner, humor, humour, joke,
   The main goal of the bootstrapping algorithm is to au-        funny. For example, http://www.berro.com/Jokes or
tomatically collect a large number of one-liners, starting       http://www.mutedfaith.com/funny/life.htm are the
with a short seed list, consisting of few (ten or less) one-     URLs of two webpages that satisfy this first constraint.
liners manually identified. The bootstrapping process is             The second constraint is designed to exploit the
illustrated in Figure 1. Starting with the seed set, the         HTML structure of the webpages, in an attempt to iden-
algorithm automatically identifies a list of webpages that       tify enumerations of texts that include the seed one-liner.
include at least one of the seed one-liners, via a simple        This is based on the hypothesis that enumerations typ-
search performed with a Web search engine1 . Next, the           ically include stylistically similar texts, and thus a list
webpages found in this way are parsed, and additional            including the seed one-liner is very likely to include ad-
one-liners are automatically identified and added to the         ditional one-line jokes. For instance, if a seed one-liner
seed set. The process is then repeated several times,            is found in a webpage preceded by the HTML tag <li>2 ,
until enough one-liners are collected.                           other lines found in the same enumeration preceded by
   An important aspect of any bootstrapping algorithm            the same tag are also likely to be one-liners.
is the set of constraints used to steer the process and              Two iterations of the bootstrapping process, started
prevent as much as possible the addition of noisy en-             with a small seed set of ten one-liners, resulted into
tries. The one-liner bootstrapping algorithm is guided            a large set of about 24,000 one-liners. After removing
by two constraints: (1) a thematic constraint applied on          the duplicates, we were left with a final set of approxi-
the content of each webpage; and (2) a structural con-            mately 20,000 one-liners, which were used in the humor-
straint, exploiting HTML annotations indicating “stylis-          recognition experiments.
tically” similar text.
   The first constraint is implemented using a set of key-        Non-humorous Data
words of which at least one has to appear in the URL of a
retrieved webpage, thus potentially limiting the content          To construct the set of negative examples required by
of the webpage to a theme related to that keyword. The            the humor-recognition models, we tried to identify col-
set of keywords used in the current implementation con-           lections of sentences that were non-humorous, but simi-
                                                                 lar in structure and composition to the one-liners. This
    1
      Current experiments rely on Google, but other search        similarity was sought mainly for the purpose of making
engines can be used to the same effect. A maximum of 100
                                                                     2
candidate URLs are retrieved in return to a search.                    The HTML tag <li> stands for “list item.”
                                                             1514

                           One-liners                               selected at random from the BNC corpus, covering dif-
       Take my advice; I don’t use it anyway.                       ferent styles, genres and domains. Unlike the Reuters
       I get enough exercise just pushing my luck.                  titles or the proverbs, the BNC sentences have typi-
       Beauty is in the eye of the beer holder.                     cally no added creativity and no specific intent. How-
                         Reuters titles                             ever, we decided to add this set of negative examples
       Trocadero expects tripling of revenues.                      to our experimental setting, in order to observe the
       Silver fixes at two-month high, but gold lags.               level of difficulty of a humor-recognition task when
       Oil prices slip as refiners shop for bargains.               performed with respect to simple text.
                        BNC sentences                               Table 1 shows three examples from each data set, to
       They were like spirits, and I loved them.                illustrate their structure and composition.
       I wonder if there is some contradiction here.
       The train arrives three minutes early.                   The “400HS” and “40000HS” Data Sets
                            Proverbs                            To summarize, two data sets were built and used in the
       Creativity is more important than knowledge.             experiments: (1) a small set that emphasizes the quality
       Beauty is in the eye of the beholder.                    aspect of the data, for which the one-liners were manu-
       I believe no tales from an enemy’s tongue.               ally selected; and (2) a very large set automatically ex-
                                                                tracted using a Web-based bootstrapping process, em-
 Table 1: Sample examples of one-liners, Reuters titles,        phasizing the quantity aspect of the data, including a
 BNC sentences, and proverbs.                                   small fraction of potentially noisy examples.
                                                                • The “400HS” data set. In this set, the positive ex-
 the humor-recognition task more difficult and thus more            amples consist of 200 one-liners that were manually
 real. We do not want the automatic classifiers to learn            collected, and thus are guaranteed to be “clean” hu-
 to distinguish between humorous and non-humorous ex-
                                                                    morous examples. The set of negative examples con-
 amples based simply on text length or vocabulary dif-
 ferences. Instead, we seek to enforce the classifiers to           sist of one of the following sets: (1) 200 Reuters titles;
 identify humor-specific features, by supplying them with           (2) 200 sentences randomly selected from BNC; (3)
 negative examples similar in most of their aspects to the          200 proverbs.
 positive examples, but different in their comic effect.
    Structural similarity was enforced by requiring that        • The “40000HS” data set. The positive examples in
 each example in the non-humorous data set follows the              this set consist of 20,000 one-liners automatically iden-
 same length restriction as the one-liners: one sentence            tified on the Web using the bootstrapping method il-
 with an average length of 10–15 words. Composition                 lustrated earlier. Since the collection process was au-
 similarity is sought by trying to identify examples simi-          tomatic, noisy entries are also possible. Manual verifi-
 lar to the one-liners with respect to their creativity and         cation of a randomly selected sample of 200 one-liners
 intent.                                                            resulted into the identification of 18 noisy entries, in-
                                                                    dicating an average of 9% potential noise in the data
    We tested three different sets of negative examples:
                                                                    set, which is within reasonable limits. The negative
1. Reuters titles, extracted from news articles published           examples are drawn from: (1) Reuters titles; or (2)
    in the Reuters newswire over a period of one year               BNC sentences. Since the collection of proverbs that
    (8/20/1996 – 8/19/1997) [Lewis et al., 2004]. The ti-           we could obtain was relatively small, this type of neg-
    tles consist of short sentences with simple syntax, and         ative examples was not included in the large data ex-
    are often phrased to catch the readers’ attention (an           periments.
    effect similar to the one rendered by one-liners).
                                                                        Algorithms for Text Classification
2. Proverbs manually extracted from an “English proverb
                                                                 There is a large body of algorithms previously tested
    collection.” Proverbs are sayings that transmit, usu-
                                                                 on text classification problems, due also to the fact that
    ally in one short sentence, important facts or experi-
                                                                 text categorization is one of the testbeds of choice for
    ences that are considered true by many people. Their
                                                                 machine learning. In the classification experiments we
    property of being condensed, but memorable sayings
                                                                 present here, we compare results obtained with two fre-
    make them very similar to the one-liners. In fact,
                                                                 quently used text classifiers, Naive Bayes and Support
    some one-liners attempt to imitate proverbs, but with
                                                                 Vector Machines, selected based on their performance in
    a comic effect, as in e.g. “Beauty is in the eye of the
                                                                 previously reported work, and for the diversity of their
    beer holder”, derived from “Beauty is in the eye of the
                                                                 learning methodologies.
    beholder”.
                                                                 Naive Bayes. The basic idea in a Naive Bayes text
3. British National Corpus (BNC) sentences, which were           classifier is to estimate the probability of a category
                                                            1515

given a document using joint probabilities of words                                                                           One-liners            One-liners
and documents. Naive Bayes assumes word indepen-                                                              Classifier       Reuters                BNC
dence, which means that the conditional probability of                                                        Naive Bayes      96.89%                73.62%
a word given a category is assumed to be independent                                                          SVM              96.09%                77.84%
of the conditional probability of other words given the
same category. Despite this simplification, Naive Bayes             Table 3: Classification accuracy for the “40000HS” set.
classifiers perform reasonably well on text classification
[Yang and Liu, 1999]. While there are several versions                                                                   Classification learning curves
of Naive Bayes classifiers (variations of multinomial and                                            80
multivariate Bernoulli), we use the multinomial model                                                75
[McCallum and Nigam, 1998], which was shown to be
                                                                       Classification accuracy (%)
                                                                                                     70
more effective.
                                                                                                     65
Support Vector Machines. Support Vector Machines
                                                                                                     60
(SVM) are binary classifiers that attempt to find the
hyperplane that best separates a set of positive examples                                            55
from a set of negative examples, with maximum margin                                                 50
[Vapnik, 1995]. Applications of SVM classifiers to text
                                                                                                     45
categorization led to some of the best results reported in                                                                                          Naive Bayes
                                                                                                                                                           SVM
the literature [Joachims, 1998].                                                                     40
                                                                                                          0         20         40            60            80     100
                                                                                                                             Fraction of data (%)
              Experimental Results
The major goal of the studies reported in this paper was            Figure 2: Classification learning curves for the
to test whether automatic classification techniques can             “40000HS”(one-liners/BNC) data set.
be successfully applied to the task of humor-recognition.
To this end, several experiments were conducted to gain
insights into various aspects of an automatic humor iden-                                                                Discussion
tification task: classification accuracy, learning rates, im-
pact of the type of negative data used in the learning             The results obtained in the automatic classification ex-
process, and impact of the classification methodology.             periments reveal the fact that computational approaches
   In all the experiments, the evaluation is performed             represent a viable solution for the task of humor-
using stratified ten-fold cross validations, to guarantee          recognition, and good performance can be achieved using
accurate precision estimates.                                      standard text classification techniques.
   Due to the methodology used in building the data sets              When a clean, manually constructed data set is used
(equal distribution between positive and negative exam-            (“400HS”), a relatively small number of examples
ples), the baseline for all the experiments is 50%, which          (400) was enough to achieve classification accuracies
represents the classification accuracy obtained if a de-           ranging from 56.75% (one-liners/BNC) to 89.75% (one-
fault label of “humorous” (or “non-humorous”) would                liners/Reuters), representing a significant improvement
be assigned by default to all the examples in the data             over the baseline of 50%.
set.                                                                  Although the results obtained in this first set of ex-
                                                                   periments were already satisfactory, a significantly larger
                  One-liners     One-liners    One-liners          data set was required in order to gain additional insights
  Classifier       Reuters         BNC         Proverbs            into the advantages and potential limitations of this au-
  Naive Bayes      89.75%         56.75%        68.50%             tomatic classification approach to humor recognition. In
  SVM              84.75%         63.75%        70.00%             addition to accuracy figures, we were also interested in
                                                                   the variation of classification performance with respect
Table 2: Classification accuracy for the “400HS” set.              to data size, which is an aspect particularly relevant for
                                                                   directing future research. Depending on the shape of
   Table 2 shows results obtained on the “400HS” data              the learning curves, one could decide to concentrate fu-
set, for the three different sets of negative examples             ture work either on the acquisition of larger data sets, or
(Reuters, BNC, Proverbs), using the Naive Bayes and                toward the identification of more sophisticated features.
SVM text classifiers. Similar classification results, but          In order to perform these analyses, a very large data set
this time for the larger “40000HS” data set, are shown             of humorous and non-humorous texts was required, and
in Table 3, again with different sets of negative examples         we used the “40000HS” data set automatically boot-
(Reuters and BNC), and two different classifiers. Learn-           strapped from the Web.
ing curves for this large data set are plotted in Figures             For this large, even if noisier data set, the overall
2 and 3.                                                           performance increased significantly to accuracy figures
                                                                1516

                                                  Classification learning curves                    the curve becomes completely flat (One-liners/Reuters),
                                100                                                                 or it even has a slight drop (One-liners/BNC). This is
                                                                                                    probably due to the presence of noise in the data set,
                                 90
                                                                                                    which starts to become visible for very large data sets 3 .
  Classification accuracy (%)
                                 80
                                                                                                       The plateau reached at the end of the learning curves
                                                                                                    is also suggesting that more data is not likely to help
                                 70                                                                 improve the quality of an automatic humor-recognizer.
                                                                                                    Instead, more sophisticated features that go beyond sim-
                                 60                                                                 ple bag-of-words analysis are probably required. The
                                                                                                    type of features to use is a matter of future investiga-
                                 50
                                                                             Naive Bayes            tions, and will probably include humor-specific features
                                 40
                                                                                    SVM             previously proposed in linguistic studies on humor such
                                      0      20         40            60            80     100      as [Bucaria, 2004].
                                                      Fraction of data (%)
                                                                                                       Another interesting result refers to the effect achieved
                                                                                                    with the various types of negative data. Despite our ini-
Figure 3: Classification learning curves for the
                                                                                                    tial intuition that one-liners are most similar to other
“40000HS”(one-liners/Reuters) data set.
                                                                                                    creative texts (e.g. Reuters titles, or the sometimes al-
                                                                                                    most identical proverbs), and thus the learning task
                                                                                                    would be more difficult in relation to these data sets,
ranging from 77.84% (one-liners/BNC) to 96.89% (one-                                                comparative experimental results reveal the fact that in
liners/Reuters), representing a major improvement over                                              fact it is more difficult to distinguish humor with respect
both the default baseline of 50%, and over the classifica-                                          to regular text (e.g. BNC sentences).
tion results obtained with the “400HS” data set.
   To evaluate the effect of data quality on the classifi-                                                              Related Work
cation performance, we also ran an experiment where                                                 While humor is relatively well studied in scientific fields
400 examples were randomly selected from the large                                                  such as linguistics (e.g. [Attardo, 1994]) and psychology
“40000HS” corpus, while maintaining the equal dis-                                                  (e.g. [Freud, 1905, Ruch, 2002]), to date there is only a
tribution between positive and negative examples. This                                              limited number of research contributions made toward
new corpus is therefore of comparable size and character-                                           the construction of computational humour prototypes.
istics with the “400HS” corpus, but of different quality.                                              One of the first attempts is perhaps the work described
Table 4 shows the results obtained on this new data set.                                            in [Binsted and Ritchie, 1997], where a formal model of
Comparing the figures in this table with those listed in                                            semantic and syntactic regularities was devised, underly-
Table 2, it is clear that data quality can have an im-                                              ing some of the simplest types of puns (punning riddles).
portant impact on the humor-recognition performance.                                                The model was then exploited in a system called JAPE
However, larger, even if noisier, data sets have the abil-                                          that was able to automatically generate amusing puns.
ity to outweigh this effect, as shown in the results listed                                            Another humor-generation project was the HA-
in Table 3.                                                                                         HAcronym        project    [Stock and Strapparava, 2003],
                                                                                                    whose goal was to develop a system able to automati-
                                                      One-liners          One-liners
                                                                                                    cally generate humorous versions of existing acronyms,
                                      Classifier       Reuters              BNC
                                                                                                    or to produce a new amusing acronym constrained to
                                      Naive Bayes      85.37%              55.00%                   be a valid vocabulary word, starting with concepts
                                      SVM              83.75%              55.75%                   provided by the user. The comic effect was achieved
                                                                                                    mainly by exploiting incongruity theories (e.g. finding a
Table 4: Classification accuracy for a subset of 400 ex-
                                                                                                    religious variation for a technical acronym).
amples from the “40000HS” data set.
                                                                                                       Another related work, devoted this time to the prob-
                                                                                                    lem of humor comprehension, is the study reported in
  The learning curves in Figures 2 and 3 show that re-                                              [Taylor and Mazlack, 2004], focused on a very restricted
gardless of the type of negative data and the classifier                                            type of wordplays, namely the “Knock-Knock” jokes.
used, there is significant learning until about 60% of                                              The goal of the study was to evaluate to what extent
the data (i.e. about 10–12,000 positive examples, and                                               wordplay can be automatically identified in “Knock-
the same number of negative examples). The rather                                                   Knock” jokes, and if such jokes can be reliably recognized
steep ascent of the curve, especially in the first part of                                          from other non-humorous text. The algorithm was based
the learning, suggests that humorous and non-humorous
                                                                                                        3
texts represent well distinguishable types of data.                                                       We also like to think of this behavior as if the computer
                                                                                                    is losing its sense of humor after an overwhelming number of
  An interesting effect can be noticed toward the end                                               jokes, in a way similar to humans when they get bored and
of the learning, where for both Naive Bayes and SVM                                                 stop appreciating humor after hearing too many jokes.
                                                                                                 1517

on automatically extracted structural patterns and on                                  References
heuristics heavily based on the peculiar structure of this       [Attardo, 1994] Attardo, S. (1994). Linguistic Theory of
particular type of jokes. While the wordplay recogni-               Humor. Mouton de Gruyter, Berlin.
tion gave satisfactory results, the identification of jokes
containing such wordplays turned out to be significantly         [Binsted and Ritchie, 1997] Binsted, K. and Ritchie, G.
more difficult.                                                     (1997). Computational rules for punning riddles. Hu-
                                                                    mor, 10(1).
                      Conclusion                                 [Bucaria, 2004] Bucaria, C. (2004). Lexical and syntac-
The creative genres of natural language have been tra-              tic ambiguity as a source of humor. Humor, 17(3).
ditionally considered outside the scope of any compu-
tational treatment. In particular humor, because of its          [Freud, 1905] Freud, S. (1905). Der Witz und Seine
puzzling nature, has received little attention from com-            Beziehung zum Unbewussten. Deutike, Vienna.
putational linguists. However, given the importance of           [Joachims, 1998] Joachims, T. (1998). Text categoriza-
humor in our everyday life, and the increasing impor-               tion with Support Vector Machines: learning with
tance of computers in our work and entertainment, we                many relevant features. In Proceedings of the Euro-
believe that studies related to computational humor will            pean Conference on Machine Learning.
become increasingly important.
   In this paper, we showed that automatic classifica-           [Lewis et al., 2004] Lewis, D., Yang, Y., Rose, T., and
tion techniques can be successfully applied to the task             Li, F. (2004). RCV1: A new benchmark collection for
of humor-recognition. Experimental results obtained on              text categorization research. The Journal of Machine
very large data sets showed that learning approaches can            Learning Research, 5:361–397.
be efficiently used to distinguish between humorous and         [McCallum and Nigam, 1998] McCallum,             A.     and
non-humorous texts, with significant improvements ob-               Nigam, K. (1998). A comparison of event models
served over apriori known baselines. To our knowledge,              for Naive Bayes text classification.       In Proceed-
this is the first result of this kind reported in the lit-          ings of AAAI-98 Workshop on Learning for Text
erature, as we are not aware of any previous work in-               Categorization.
vestigating the interaction between humor and machine
learning.                                                       [Nijholt et al., 2003] Nijholt, A., Stock, O., Dix, A., and
   Moreover, we have also showed that it is possible to             Morkes, J., editors (2003). Proceedings of CHI-2003
bootstrap a very large and relatively clean corpus that             workshop: Humor Modeling in the Interface, Fort
falls under a certain genre (e.g. humor), starting with a           Lauderdale, Florida.
handful of manually selected seeds, and using constraints
                                                                [Ruch, 2002] Ruch, W. (2002). Computers with a per-
based on document structural information and simple
                                                                    sonality? lessons to be learned from studies of the
thematic clues. Although current experiments relying
                                                                    psychology of humor. In [Stock et al., 2002].
on this technique have focused on building a collection
of humorous texts, we believe that this Web-based boot-         [Stock and Strapparava, 2003] Stock, O. and Strappar-
strapping method is not limited to one-liners, but it can           ava, C. (2003). Getting serious about the development
be equally well applied to other creative genres.                   of computational humour. In Proceedings of the 8th In-
   Finally, through the analysis of learning curves plot-           ternational Joint Conference on Artificial Intelligence
ting the classification performance with respect to data            (IJCAI-03), Acapulco, Mexico.
size, we showed that the accuracy of the automatic
                                                                [Stock et al., 2002] Stock, O., Strapparava, C., and Ni-
humor-recognizer stops improving after a certain number
                                                                    jholt, A., editors (2002). Proceedings of the The
of examples. Given that automatic humor-recognition is
                                                                    April Fools Day Workshop on Computational Humour
a rather understudied problem, we believe that this is
                                                                    (TWLT20), Trento.
an important result, as it gives us insights into poten-
tially productive directions for future work. The flat-         [Taylor and Mazlack, 2004] Taylor, J. and Mazlack, L.
tened shape of the curves toward the end of the learning            (2004). Computationally recognizing wordplay in
process suggests that rather than focusing on gathering             jokes. In Proceeding of CogSci 2004, Chicago.
more data, future work should concentrate on identifying
more sophisticated humor-specific features, e.g. semantic       [Vapnik, 1995] Vapnik, V. (1995). The Nature of Statis-
oppositions, ambiguity, and others. We plan to address              tical Learning Theory. Springer, New York.
these aspects in future research.                               [Yang and Liu, 1999] Yang, Y. and Liu, X. (1999). A
                                                                    reexamination of text categorization methods. In Pro-
                                                                    ceedings of the 22nd ACM SIGIR Conference on Re-
                                                                    search and Development in Information Retrieval.
                                                            1518

