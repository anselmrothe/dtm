UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
A Cognitively Based Simulation of Simple Organizations
Permalink
https://escholarship.org/uc/item/0km3c6d0
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 27(27)
Authors
Nawa, Norberto Eiji
Shimohara, Katsunori
Publication Date
2005-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

               A Cognitively Based Simulation of Simple Organizations
                                                Ron Sun (rsun@rpi.edu)
                                     Isaac Naveh (yizchaknaveh@yahoo.com)
                                                 Cognitive Science Department
                                                Rensselaer Polytechnic Institute
                                                     Troy, NY 12180, USA
                           Abstract                                               Organizational Design
   This paper explores cognitively realistic social simula-       Carley et al (1998) introduced an organizational decision
   tions by deploying the CLARION cognitive architecture          task involving different types of organizational structures
   in a simple organizational simulation, which involves the      and agents. By varying agent type and structure sepa-
   interaction of multiple cognitive agents. It argues for        rately, they studied how these factors interact with each
   an integration of the two separate strands of research:
   cognitive modeling and social simulation. Such an inte-        other.
   gration could, on the one hand, enhance the accuracy of            The Task. The task is to determine whether a blip on
   social simulation models by taking into full account the        a screen is a hostile aircraft, a flock of geese, or a civilian
   effects of individual cognitive factors, and on the other       aircraft (Carley et al 1998). It has been extensively used
   hand, it could lead to greater explanatory, predictive,
   and prescriptive power from these models.                       before in studying organizational design.
                                                                      In each case, there is a single object in the airspace.
   Keywords: social simulation; organization; decision             The object has nine different attributes, each of which
   making; cognitive architecture.
                                                                   can take on one of three possible values (e.g., its speed
                                                                   can be low, medium, or high). An organization must
                       Introduction                                determine the status of an observed object: whether it
Most of the current work in social simulation still as-            is friendly, neutral or hostile. There are a total of 19,683
sumes very rudimentary cognition on the part of the                possible objects, and 100 problems are chosen randomly
agents (e.g., Gilbert and Doran 1994). At the same time,           (without replacement) from this set. The true status of
while researchers in cognitive science have devoted con-           an object is determinable by adding up all nine attribute
siderable attention to the workings of individual cogni-           values. If the sum is less than 17, then it is friendly; if
tion (e.g., Anderson and Lebiere 1998; Sun 2002), socio-           the sum is greater than 19, it is hostile; otherwise, it is
cultural processes and their relations to individual cogni-        neutral.
tion have generally not been sufficiently studied by cog-             No one single agent has access to all the information
nitive scientists (with some notable exceptions).                  necessary to make a choice. Decisions are made by inte-
   However, there are reasons to believe that better mod-          grating separate decisions made by different agents, each
els of individual cognition can lead us to a better un-            of which is based on a different subset of information.
derstanding of aggregate processes involving multi-agent              In terms of organizational structures, there are two
interaction (Moss 1999; Castelfranchi 2001; Sun 2001).             archetypal structures of interest: (1) teams, in which
Cognitive models that incorporate realistic tendencies,            agents act autonomously, individual decisions are treated
biases, and capacities of individual cognitive agents can          as votes, and the organization decision is the majority
serve as a more realistic basis for understanding multi-           decision; and (2) hierarchies, which are characterized by
agent interaction. Social interaction is, after all, the           agents organized in a chain of command, such that in-
result of individual cognition (which includes instincts,          formation is passed from subordinates to superiors, and
routines, and patterned behavior, as well as complex               the decision of a superior is based solely on the recom-
symbolic, conceptual processes). Therefore, the mecha-             mendations of his/her subordinates. In this task, only a
nisms underlying individual cognition cannot be ignored            two-level hierarchy with nine subordinates and one su-
in studying multi-agent interactions. At least, the impli-         pervisor is considered.
cations of these mechanisms should be understood before               In addition, organizations are distinguished by the
they are abstracted away.                                          structure of information accessible by each agent. There
   In the remainder of this paper, first, an existing or-          are two varieties of information access: (1) distributed
ganizational decision task is introduced. Then, a more             access, in which each agent sees a different subset of three
realistic cognitive architecture, named CLARION, is de-            attributes (no two agents see the same set of three at-
scribed, which is applied to the organizational task. The          tributes), and (2) blocked access, in which three agents
idea is to substitute the more sophisticated agent, based          see exactly the same attributes. In both cases, each at-
on CLARION, for the simple agents used in the previ-               tribute is accessible to three agents.
ous work, with the aim of studying the interaction of                 Several simulation models were considered in Carley et
cognition and organizational design.                               al (1998). Among them, CORP-ELM produced the most
                                                              2098

probable classification based on an agent’s own experi-
ence, CORP-P-ELM stochastically produced a classifica-
tion in accordance with the estimate of the probability of
each classification based on the agent’s own experience,
CORP-SOP followed organizationally prescribed stan-
dard operating procedure (which involved summing up
the values of the attributes available to an agent) and
thus was not adaptive, and Radar-Soar was a (some-
what) cognitive model built in Soar, which is based on
explicit, elaborate search in problem spaces (Rosenbloom
et al 1991).
   Previous Experimental Results. The experiments
by Carley et al (1998) were done in a 2 x 2 fashion (or-
ganization x information access). In addition, human
data for the experiment were compared to the results of                       Figure 1: The CLARION architecture
the four aforementioned models (Carley et al 1998). The
data appeared to show that agent type interacted with
organizational design. See Table 1.                                learning engaged. Our review of experimental data (e.g.,
                                                                   Reber 1989; Stanley et al 1989; Sun et al 2001) shows
   Agent/Org.   Team(B)   Team(D)   Hierarchy(B) Hierarchy(D)      that although one can manipulate conditions so that one
     Human
   Radar-Soar
                  50.0
                  73.3
                            56.7
                            63.3
                                        46.7
                                        63.3
                                                     55.0
                                                     53.3
                                                                   or the other type of learning is emphasized, both types
  CORP-P-ELM
   CORP-ELM
                  78.3
                  88.3
                            71.7
                            85.0
                                        40.0
                                        45.0
                                                     36.7
                                                     50.0
                                                                   of learning are nonetheless usually present.
   CORP-SOP       81.7      85.0        81.7         85.0             To model the interaction between these two types of
                                                                   learning, the cognitive architecture CLARION was de-
Table 1: Human and simulation data for the organiza-               veloped (Sun et al 2001), which captures the combina-
tional decision task. D indicates distributed information          tion of explicit and implicit learning. CLARION mostly
access, while B indicates blocked information access. All          learns in a “bottom-up” fashion, by extracting explicit
                                                                   knowledge from implicit knowledge (see Sun 2002 for de-
numbers are percent correct.
                                                                   tails). Such processes have also been observed in humans
                                                                   (e.g., Stanley et al 1989; Mandler 1992).
   The human data showed that humans generally per-                   A Sketch of the CLARION Model. CLARION
formed better in team situations, especially when dis-            is an integrative cognitive architecture with a dual rep-
tributed information access was in place. Moreover, dis-          resentational structure (Sun et al 1998; Sun et al 2001;
tributed information access was generally better than             Sun 2002). It consists of two levels: a top level that cap-
blocked information access.                                       tures explicit learning, and a bottom level that captures
   It also suggested that which type of organizational            implicit learning (see Figure 1).
design exhibit the highest performance depends on the                 At the bottom level, the inaccessibility of implicit
type of agent. For example, human subjects performed              learning is captured by subsymbolic distributed repre-
best as a team with distributed information access, while         sentations. This is because representational units in
Radar-Soar and CORP-ELM performed the best in a                   a distributed environment are capable of performing
team with blocked information access.                             tasks but are generally not individually meaningful (Sun
   In their work, the agent models used were very simple.         1995). Learning at the bottom level proceeds in trial-
The “intelligence” level in these models was rather low           and-error fashion, guided by reinforcement learning (i.e.,
(including the Soar model, which essentially encoded a            Q-learning) implemented in backpropagation neural net-
set of simple rules). Moreover, learning in these simula-         works (Sun 2002).
tions was rudimentary: there was no complex learning                  At the top level, explicit learning is captured by a
process as one might observe in humans.                           symbolic representation, in which each element is dis-
                                                                  crete and has a clearer meaning. This accords well with
                       The Model                                  the directly accessible nature of explicit knowledge (Sun
Below, we discuss a more cognitively realistic model, to          2002). Learning at the top level proceeds by first con-
be used for addressing this task (Sun 2002). First, some          structing a rule that corresponds to a “good” decision
major issues that the model captures are as follows.              made by the bottom level, and then refining it (by gen-
   Explicit vs. Implicit Learning. The role of im-                eralizing or specializing it), mainly through the use of
plicit learning in skill acquisition has been widely rec-         an “information gain” measure that compares the suc-
ognized in recent years (e.g., Reber 1989; Seger 1994;            cess ratio of various modifications of the current rule.
Stadler and Frensch 1998). Although explicit and im-                  A high-level pseudo-code algorithm that describes the
plicit learning have both been actively studied, the ques-        action-centered subsystem of CLARION is as follows:
tion of the interaction between these two processes has
                                                                  1. Observe the current state x.
rarely been broached. However, despite the lack of study
of this interaction, it has recently become evident (e.g.,        2. Compute in the bottom level the Q-value of each of the pos-
in Seger 1994) that rarely, if ever, is only one of type of           sible actions (ai ’s) associated with the state x: Q(x, a1 ),
                                                              2099

   Q(x, a2 ), ....., Q(x, an ).                                               The generalization operator is based on the IG mea-
                                                                           sure. Generalization amounts to adding an additional
3. Find out all the possible actions (b1 , b2 , ..., bm ) at the top
   level, based on the state x and the rules in place at the top           value to one input dimension in the condition of a rule,
   level.                                                                  so that the rule will have more opportunities of matching
                                                                           input. For a rule to be generalized, the following must
4. Compare the values of ai ’s with those of bj ’s, and choose             hold:
   an appropriate action a.
                                                                                                                               0
                                                                                  IG(C, all) > thresholdGEN   and   maxC 0 IG(C , C) ≥ 0
5. Perform the action a, and observe the next state y and
   (possibly) the reinforcement r.
6. Update the bottom level in accordance with the Q-                       where C is the current condition of a rule (matching the
   Learning-Backpropagation algorithm, based on the feed-                  current state and action), all refers to the corresponding
   back information.
                                                                           match-all rule (with the same action as specified by the
7. Update the top level using the Rule-Extraction-Refinement               original rule but an input condition that matches any
   algorithm.                                                              state), and C 0 is a modified condition equal to C plus
8. Go back to Step 1.                                                      one input value. If the above holds, the new rule will
                                                                           have the condition C 0 with the highest IG measure. The
   At the bottom level, a Q-value is an evaluation of the                  generalization threshold (denoted thresholdGEN above)
“quality” of an action in a given state: Q(x, a) indicates                 determines how readily an agent will generalize a rule.
how desirable action a is in state x. Actions can be                          The specialization operator works in an analogous
selected based on Q-values. To acquire the Q-values, Q-                    fashion, except that a value in an input dimension is
learning, a reinforcement learning algorithm (Watkins                      discarded, rather than being added.               In addition, to
1989), is used. (See Sun 2002 for further details.)                        avoid the proliferation of useless rules, a RER density
   In the top level, explicit knowledge is captured in a                   measure is in place. A density of 1/x means that a rule
simple prepositional rule form. We devised an algorithm                    must be invoked once per x steps to avoid deletion due
for learning explicit knowledge (rules) using information                  to disuse. This corresponds to the agent’s memory for
from the bottom level (the Rule-Extraction-Refinement,                     rules, necessitating that a rule come up every once in a
or RER, algorithm). The basic idea is as follows: if an                    while in order to be retained.
action decided by the bottom level is successful then the                     To integrate results, levels are chosen stochastically,
agent extracts a rule (with its action corresponding to                    using a probability of selecting each level.
that selected by the bottom level and with its condi-                         When the outcome from the bottom level is chosen, a
tions corresponding to the current state), and adds the                    stochastic process based on the Boltzmann distribution
rule to the top level. Then, in subsequent interactions                    of Q values is used for selecting an action:
with the world, the agent refines the extracted rule by                                                      Q(x,a)/t
considering the outcome of applying the rule: if the out-                                         p(a|x) = Pe
                                                                                                              eQ(x,ai )/t
                                                                                                            i
come is successful, the agent may try to generalize the
conditions of the rule to make it more universal. If the                   where x is the current state, a is an action, and t controls
outcome is unsuccessful, the agent may try to specialize                   the degree of randomness (temperature) of the process.1
the rule, by narrowing its conditions down and making                         Below, we present three simulations involving the
them exclusive of the current state.                                       CLARION model. The first experiment uses the afore-
   The information gain (IG) measure of a rule is com-                     mentioned radar task (Carley et al 1998) but substitutes
puted (in this organizational decision task) based on the                  a different cognitive model. In CLARION, we can easily
immediate feedback at every step when the rule is ap-                      vary parameters and options that correspond to different
plied. The inequality, r > thresholdRER determines the                     cognitive capacities and test the resulting performance.
positivity/negativity of a step and the rule matching this                 The second simulation uses the same task, but extends
step (where r is the feedback received by an agent). The                   the duration of training given to the agents. Finally, in
positivity threshold (denoted thresholdRER above) cor-                     the third simulation, we vary a wide range of cognitive
responds to whether or not an action is perceived by the                   parameters of the model in a factorial design.
agent as being reasonably good. Based on the positiv-
ity of a step, PM (Positive Match) and NM (negative                           Simulation I: Matching Human Data
match) counts of the matching rules are updated. IG is
calculated based on PM and NM:                                             In this simulation, each agent (whatever its position in
                                                                           the organization) is implemented as a CLARION model.
                       P Ma (A) + c1                 P Ma (B) + c1         At the top level, RER is used to extract rules. At the
IG(A, B) = log2                          −log2
                 P Ma (A) + NMa (A) + c2       P Ma (B) + NMa (B) + c2     bottom level, each agent has a single Q-learning neural
                                                                           network that is trained, over time, to respond correctly.
where A and B are two different rule conditions that                       The network receives an external feedback of 0 or 1 after
lead to the same action a, and c1 and c2 are two con-                      each step, depending on whether the target was correctly
stants representing the prior (by default, c1 = 1, c2 =                       1
                                                                                This method is also known as Luce’s choice axiom
2). Essentially, the measure compares the percentages of                   (Watkins 1989). It is found to match psychological data in
positive matches under alternative conditions A and B.                     many domains.
                                                                       2100

classified by the network. Various parameter values were
chosen through trial-and-error optimization.
   The results of our simulation are shown in Table 2.
3,000 training cycles (each corresponding to a single in-
stance, followed by a decision by the organization) were
used for each organization in this simulation. The agents
of an organization were trained together within that or-
ganization. Other settings (such as organizational struc-
tures and information access) were the same as in Carley
et al (1998) as described earlier. As can be seen, our re-
sults closely accord with the patterns of the human data,
with teams outperforming hierarchal structures, and dis-
tributed access proving superior to blocked access. Also,
as in humans, performance is not grossly skewed towards          Figure 2: Training curve for team organization with dis-
one condition or the other, but is roughly comparable            tributed access
across all conditions (unlike some of the simulation re-
sults from Carley et al 1998). The match with the human
data is far better than in the simulations conducted in
the original study (Carley et al 1998). The better match
is due, at least in part, to a higher degree of cognitive
realism in our simulation.
  Agent/Org.   Team(B)  Team(D)   Hierarchy(B) Hierarchy(D)
     Human       50.0     56.7        46.7         55.0
   CLARION       53.2     59.3        45.0         49.4
Table 2: Simulation data for agents running for 3,000
cycles. The human data from Carley et al (1998) are re-
produced here. Performance of CLARION is computed
as percent correct over the last 1,000 cycles.
                                                                 Figure 3: Training curve for team organization with
   Because no further details of the human data and the          blocked access
models of Carley et al (1998) are available, no further
comparisons to the CLARION simulation can be made.
                                                                 chies using distributed access (Figure 4) now show not
   Simulation II: Extending the Previous                         only the best, but also the most stable (least variance)
                                                                 performance of any condition. Likewise, a hierarchy with
                       Simulation                                blocked access (Figure 5), previously a weak performer,
So far, we have considered agents trained for only 3,000         shows impressive gains in the long run. Thus, while hier-
cycles. The results were interesting, because they were          archies take longer to train, their performance is superior
analogous to those of humans. However, it is interesting         in the long run. In a hierarchy, a well-trained supervi-
to see what will happen if we extend the length of the           sor is capable of synthesizing multiple data points with
training. In particular, we are interested in knowing if         greater sensitivity than a simple voting process. Like-
the trends seen above will be preserved in the long run.         wise, the reduced individual variation in blocked access
It is important that before we draw any conclusion about         leads to less fluctuation in performance in the long run.
human performance, we understand the context and con-               There is a serious lesson here: limited data can al-
ditions under which data are obtained, and thereby avoid         low us to draw only limited conclusions—only with re-
over-generalizing possible conclusions (e.g., team vs. hi-       gard to the specific situation under which the data were
erarchy, blocked vs. distributed; Carley et al 1998).            obtained. There is a natural tendency for researchers
   Figures 2-5 show learning as it occurs over 20,000            to over-generalize their conclusions (Carley et al 1998),
(rather than 3,000) cycles.        Previously, the best-         which can only be remedied by more extensive inves-
performing condition was team organization with dis-             tigations. Given the high cost of human experiments,
tributed information access. As can be seen in Figure 2,         simulation has a large role to play in exploring alterna-
this condition continues to improve slowly after the first       tives and possibilities, especially social simulation with
3,000 cycles. However, it is overtaken by team organi-           cognitive architectures.
zation with blocked access (Figure 3). Thus, it seems
that while teams benefit from a diversified (distributed)              Simulation III: Varying Cognitive
knowledge base in the initial phase of learning, a well-
trained team with redundant (blocked) knowledge per-
                                                                                       Parameters
forms better in the long run.                                    In the two preceding simulations, agents were run under
   In the hierarchal conditions, too, we can see either a        a fixed set of cognitive parameters. Next let us see what
reversal or disappearance of the initial trends. Hierar-         happens when we vary these parameters (analogous to
                                                            2101

                                                               24) = 59.90, p < 0.001, MSE = 0.73 for organization;
                                                                F(1, 24) = 3.43, p < 0.05, MSE = 0.01 for information
                                                               access]. These interactions reflect the trends discussed
                                                               above: the superiority of teams and distributed infor-
                                                               mation access at the start of the learning process, and
                                                               either the disappearance or reversal of these trends to-
                                                               wards the end. This finding is important, because it
                                                               shows that these trends persist robustly across a wide
                                                               variety of settings of cognitive parameters, and do not
                                                               critically depend on any one setting of these parameters.
                                                                  The effect of probability of using the top vs. the bot-
                                                               tom level was likewise significant [F(2, 24) = 11.73, p
                                                               < 0.001, MSE = 0.02]. More interestingly, however, its
Figure 4: Training curve for hierarchal organization with      interaction with length of training was significant as well
distributed access                                             [F(2, 24) = 12.37, p < 0.001, MSE = 0.01]. Rule learning
                                                               is far more useful at the early stages of learning, when
                                                               increased reliance on them tends to boost performance,
                                                               than towards the end of the learning process. This is
                                                               because rules are crisp guidelines that are based on past
                                                               success, and as such, they provide a useful anchor at the
                                                               uncertain early stages of learning. However, by the end
                                                               of the learning process, they become no more reliable
                                                               than highly-trained neural networks. This corresponds
                                                               to findings in human cognition, where there are indica-
                                                               tions that rule-based learning is widely used in the early
                                                               stages of learning, but is later increasingly supplanted
                                                               by similarity-based processes (Smith and Minda 1998)
                                                               and skilled performance (Anderson and Lebiere 1998).
                                                               Such trends may partially explain why hierarchies do
Figure 5: Training curve for hierarchal organization with      not perform well initially because a hierarchy’s supervi-
blocked access                                                 sor is burdened with a higher input dimensionality, and
                                                               therefore it takes a longer time to encode rules (which
                                                               are essential at the early stages of learning).
varying the training length earlier). This again allows
us to see the variability of results, and thus avoid over-        Predictably, the effect of learning rate was significant
generalization.      Because CLARION captures a wide           [F(2, 24) = 32.47, p < 0.001, MSE = 0.07]. Groups with
range of cognitive processes and phenomena, its param-         a higher learning rate (0.5) outperformed the groups
eters are generic rather than task-specific. Thus, we have     with the lower learning rate (0.25) by between 5-14%.
the opportunity of studying specific issues (such as or-       However, there was no significant interaction between
ganizational design), in the context of a general theory       learning rate and organization or information access.
of cognition.                                                  This suggests that quicker learners do not differentially
   Two sets of parameters of CLARION were separately           benefit from, say, a hierarchy versus a team. By the same
varied (in order to avoid the prohibitively high cost of       token, the poorer performance of slower learners cannot
varying all parameters simultaneously). The first set          be mitigated by recourse to a particular combination of
of parameters consisted of fundamental parameters of           organization and information access.
CLARION: (1) Reliance on the top versus the bottom                Let us now turn to the parameters related to RER rule
level, expressed as a fixed probability of selecting each      learning. Generalization threshold determines how read-
level. (2) Learning rate of the neural networks. (3) Tem-      ily an agent will generalize a successful rule. It is bet-
perature, or degree of randomness. The second set con-         ter to have a higher rule generalization threshold than a
sisted of parameters related to RER, including: (1) RER        lower one (up to a point). An ANOVA confirmed the sig-
positivity threshold, which must be exceeded for a rule        nificance of this effect [F(1, 24) = 15.91, p < 0.001, MSE
to be considered “successful.” (2) RER density measure,        = 0.01]. Thus, if one restricts the generalization of rules
which determined how often a rule must be invoked in           only to those rules that have proven relatively successful
order to be retained. (3) RER generalization threshold,        (by selecting a fairly high generalization threshold), the
which must be exceeded for a rule to be generalized.           result is a higher-quality rule set, which leads to better
(These parameters were also described earlier.)                performance in the long run.
   An ANOVA on the results confirmed the effects of or-           Relatedly, while the effect of rule density on perfor-
ganization [F(1, 24) = 30.28, p < 0.001, MSE = 0.05] and       mance was insignificant, the interaction between density
information access [F(1, 24) = 7.14, p < 0.05, MSE =           (i.e., “memory” for rules) and generalization threshold
0.01] to be significant. Moreover, the interaction of these    was significant [by an ANOVA; F(2, 24) = 2.93; p <
two factors with length of training was significant [F(1,      0.05; MSE = 0.01]. When rules are of relatively high
                                                           2102

quality (i.e., under a high generalization threshold) it          Carley, K. M., Prietula, M. J., and Lin, Z. (1998). De-
is advisable to have more of them available (which is               sign versus Cognition: The interaction of agent cogni-
achievable by lowering the density parameter). By con-              tion and organizational design on organizational per-
trast, when the average quality of rules is lower (i.e., un-        formance. Journal of Artificial Societies and Social
der a low generalization threshold) it is advantageous to           Simulation, 1(3).
have a quicker forgetting process in place, as embodied          Castelfranchi, C. (2001). “The Theory of Social Func-
by a high density parameter.                                        tions: Challenges for Computational Social Science
   Finally, the interaction between generalization thresh-          and Multi-Agent Learning.” In Sun R (ed.), Cog-
old and organization was significant at the start of the            nitive Systems Research, special issue on the multi-
learning process [by an ANOVA; F(1, 24) = 5.93, p <                 disciplinary studies of multi-agent learning, 2(1). pp.
0.05, MSE = 0.01], but not at the end. This result is               5-38.
more difficult to interpret, but probably reflects the fact
that hierarchies, at the start of the learning process, do       Gilbert, N. and Doran, J. (1994). Simulating Societies:
not encode very good rules to begin with (due to the                The Computer Simulation of Social Phenomena. Lon-
higher input dimensionality of the supervisor and the               don, UK: UCL Press, London.
resulting learning difficulty). Thus, generalizing these         Mandler, J. (1992). How to Build a Baby. Psychological
rules, even incorrectly, causes relatively little further           Review, 99(4), pp. 587-604.
harm.                                                            Moss, S. (1999). Relevance, Realism and Rigour: A
   This simulation confirmed an earlier observation—                Third Way for Social and Economic Research. CPM
namely, that which organizational structure (team vs.               Report No. 99-56. Manchester, UK: Center for Policy
hierarchy) or information access scheme (distributed vs.            Analysis, Manchester Metropolitan University.
blocked) is superior depends on the length of the train-         Reber, A. (1989). Implicit Learning and Tacit Knowl-
ing. It also showed that some cognitive parameters (e.g.,           edge. Journal of Experimental Psychology: General,
learning rate) have a monolithic, across-the-board effect           118(3). pp. 219-235.
under all conditions, whereas in other cases, complex in-
teractions of factors are at work. See Sun and Naveh             Rosenbloom, P., Laird, J., Newell, A., and McCarl, R.
(2004) for full details. This illustrates, once again, the          (1991). A Preliminary Analysis of the Soar Archi-
importance of limiting one’s conclusions to the specific            tecture as a Basis for General Intelligence. Artificial
cognitive context in which data were obtained.                      Intelligence, 47(1-3). pp. 289-325.
                                                                 Seger, C. (1994). Implicit Learning. Psychological Bul-
                       Discussion                                   letin, 115(2). pp. 163-196.
By using CLARION, we have been able to more accu-                Smith, J. D. and Minda, J. P. (1998). Prototypes in the
rately capture organizational performance data as well              Mist: The Early Epochs of Category Learning. Jour-
as to formulate deeper explanations for the results ob-             nal of Experimental Psychology: Learning, Memory,
served, due to cognitive realism. For instance, based               and Cognition, 24. pp. 1411-1436.
on our observations, one may formulate the following             Stadler, M. and Frensch, P. (1998). Handbook of Implicit
possible explanation: the poorer performance of hierar-             Learning. Thousand Oaks, CA: Sage Publications.
chies early on (see Simulation I) may be due, at least in        Stanley, W., Mathews, R., Buss, R., and Kotler-Cope,
part, to the longer training time needed to encode high-            S. (1989). Insight Without Awareness: On the Inter-
dimensional information by the supervisor, which leads              action of Verbalization, Instruction and Practice in a
to fewer useful rules being acquired at the top level. This         Simulated Process Control Task. Quarterly Journal of
in turn impacts performance, since rule learning is espe-           Experimental Psychology, 41A(3). pp. 553-577.
cially important in the early stages of learning (see Sim-
ulation III). Such explanations are only possible when           Sun, R. (2001). Cognitive Science Meets Multi-Agent
the model is cognitively realistic. Beside offering deeper          Systems: A Prolegomenon. Philosophical Psychology,
explanations, cognitive realism may also lead to greater            14(1). pp. 5-28.
predictive and prescriptive power for social simulations.        Sun, R. (2002). Duality of the Mind. Mahwah, NJ:
In CLARION, we can vary parameters and options that                 Lawrence Erlbaum Associates.
correspond to cognitive processes and test their effect          Sun, R., Merrill, E., and Peterson, T. (2001). From
on performance. In this way, CLARION can be used                    Implicit Skills to Explicit Knowledge: A Bottom-Up
to predict human performance in organizations, and fur-             Model of Skill Learning. Cognitive Science, 25(2). pp.
thermore to help performance by prescribing optimal or              203-244.
near-optimal cognitive abilities for specific tasks and or-
ganizational structures (see Sun and Naveh 2004).                Sun, R. and Naveh, I. (2004). Simulating Organizational
                                                                    Decision-Making Using a Cognitively Realistic Agent
                                                                    Model. Journal of Artificial Societies and Social Sim-
                       References                                   ulation, Vol.7, No.3.
Anderson, J. R. and Lebiere, C. (1998). The Atomic               Watkins, C. (1989). Learning with Delayed Rewards.
   Components of Thought. Mahwah, NJ: Lawrence Erl-                 PhD Thesis, Cambridge University, Cambridge, UK.
   baum Associates.
                                                             2103

