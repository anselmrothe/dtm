UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
An Evolutionary Computational Model of Prototype-Based Categorization: an Application on
Clinical Semeiotics
Permalink
https://escholarship.org/uc/item/1pk0k4zg
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 27(27)
Author
Gagliardi, Francesco
Publication Date
2005-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

        An Evolutionary Computational Model of Prototype-Based Categorization:
                                       an Application on Clinical Semeiotics
                                    Francesco Gagliardi (francesco.gagliardi@libero.it)
                              Department of Physical Sciences — University of Naples “Federico II”
                                                  Via Cintia — I-80126 Napoli, Italy
                             Abstract                                 (2002) for a comparison between models) overcome the
                                                                      constraints of these classical theories.
  The aim of this paper is to present a software artifact for            Notably the prototype theory (Rosch, E. 1975; Rosch, E.,
  machine supported understanding and modelling of prototype-         & Mervis, C.B. 1975) asserts that people categorize on the
  based categorization. This software system is able to perform
                                                                      basis of how close something is to the prototype or ideal
  discovery of syndromes (seen as prototypes) into a given data
  set of clinical observations. A new genetic algorithm is used       member of a category. For example, a robin is closer to the
  to achieve an unsupervised categorization of observation via        bird prototype than an ostrich is, but they are both closer to
  adaptive learning of number and features of prototypes. Its         it than they are to the prototype of a fish, so we call them
  evolutionary learning is oriented to maximize specificity and       both birds, only it takes longer to say an ostrich is a bird
  distinctiveness of categories. Experimental results show that       than it take to say a robin is a bird, because the ostrich is
  prototype-based categorization of clinical observation is           further from the prototype.
  suitable for syndrome-based categorization. The trichotomy             Besides, it is well established that people can categorize
  of categorizations (superordinate, basic and subordinate) is        the same objects at different levels of abstraction.
  explained by trade off between specificity and distinctiveness.
                                                                         Rosch et al.’s (1976) seminal research isolated three
  Moreover the natural basic level is also related to epistemic
  adequacy of found prototypes.                                       “natural” levels of object categorization: the superordinate
                                                                      (e.g. furniture), the basic (e.g. chair), and the subordinate
  Keywords: Categorization; Artificial Intelligence; Prototype        (e.g. Chippendale chair).
  Theory; Syndrome Discovery; Basic Level, Real-Life                     Of these, the basic level is known to have a privileged
  Problem.
                                                                      status that is often attributed to the organization of
                                                                      categories in memory.
In cognitive psychology the problem of the categorization                The origin of the bias to the basic level is still a matter of
has been formulated, according to prototypes theory, as a             debate. In categorization, researchers have proposed that
problem of similarity between a prototype and the members             categories at the basic level are more differentiated that is,
of the category that this represents.                                 “... have the most attributes common to members of the
  Cluster analysis is a machine learning task for partitioning        category and the least attributes shared with members of
a given data set of observations in groups of similar item.           other [contrasting] categories.” (Rosch et al., 1976, p. 435)
  We have realized an evolutionary machine learning                      The first component of this differentiation definition has
system for cluster analysis, which embeds prototype theory            been called the specificity (Murphy & Brownell, 1985), or
of categorization.                                                    the informativeness (Murphy, 1991) of a category, and the
  The aim is to use this software artifact for machine-               second component the distinctiveness of a category
supported explanations of adaptive categorization skill               (Murphy & Brownell, 1985; Murphy, 1991).
observed in human mind.                                                  The difference between distinct types of categorizations
  The multiple realizability of intelligent behaviour is one          would thus stem from distinct differentiations at these two
useful and objective explanatory strategy of cognitive                levels.
sciences.                                                                Both these aspects of categorization have been embedded
                                                                      in our model as explained in next sections.
       Categorization and Prototype Theory
Categorization is one of the most vexed unsolved issues in                    Cluster Analysis and Categorization
cognitive science field.                                              Cluster analysis (Jain et al. 1999) (Kaufman and Rousseeuw
  Traditional theories about categorization argue that people         1990) is the unsupervised classification (natural grouping)
categorize using the common features of the members.                  of observations (patterns, data items, or feature vectors) into
These features may be viewed as defining features because             groups (clusters) based on similarity. Intuitively, patterns
they are singly necessary and jointly sufficient to define the        within a valid cluster are more similar to each other than
category (Katz, J.J., 1972; Katz, J.J. & Fodor, 1963).                they are to a pattern belonging to a different cluster.
  In other words, each feature is an essential element of the            There is an obvious analogy between cluster analysis and
category; together, the properties uniquely define the                the cognitive problem of categorization. The usual
category.                                                             algorithms used in such field of the data analysis, (for
  The prototype theory as well as the exemplar-based                  instance statistical-iterative ones) are considered
models of categorization (see Minda, J.P. & Smith, J.D.
                                                                  732

unsupervised by contrast to classification ones because they                                          Prototypes and Sharp Clustering for Cluster
not require training set. However they require some                                                   Representation. We represent a cluster Ck through an item
knowledge that the human must supply to the system, as for                                            of S called prototype Pk which, in general, does not
instance number of categories. Moreover, there is often no                                            coincide with cluster centroid. A generic item Xi∈ DS is
explicit maximization of specificity and distinctiveness.
                                                                                                      assigned to the cluster Ck whose representative is the
   Then these algorithms cannot be considered, ipso facto, as
                                                                                                      closest. Formally:
computational models of the human mind categorization.
                                                                                                      if ∀X i ∃k : ∀k ≠ k d ( X i , Pk ) < d ( X i , Pk ) then X i ∈ Ck
Prototype-Based Cluster Analysis                                                                         A clustering can be denoted with the list of the
According to the prototype theory, an observation will be                                             representatives of its clusters: CL ≡ { P1 ,....., Pm }
categorized as an instance of a category if it is sufficiently                                        Homogeneity and Separability. Homogeneity for a cluster
similar to the prototype. Exactly what is meant by similarity                                         Ck is defined as:
to a prototype can be a complex issue, and there are actually
different theories of how this similarity should be measured                                                                                       ∑ d (X,P )           k
                                                                                                                            H ( Ck ) ≡ −
                                                                                                                                                ∀X ∈CK
(Smith, E.E. & Medin, 1981). Moreover, many researchers                                                                                                       wk
suggest that some features should be weighted more heavily                                            where Pk is the cluster prototype and wk is the cardinality of
as being more central to the prototype than are other                                                 k-th cluster. Hence we can define clustering homogeneity as
features (e.g. Komatsu, 1992).                                                                        weighted average of homogeneity of clusters, and
   In our computational model we have defining a simply                                               separability for a clustering as the weighted average of
sharp clustering criterion based on the number of similar                                             distances among clusters based on above metrics:
features shared between an observation and the prototype,                                                                             m
counted by range weighted city block distance.                                                                                      ∑ w ⋅ H (C )
                                                                                                                                           i    i              i
   We can define quantitatively similarity between item and                                                              H ( CL ) ≡    1
                                                                                                                                                m
prototype, thanks to the definition of a distance between                                                                                     ∑w    i   i
                                                                                                                                                1
observations. Moreover we can define quantitatively the                                                                             m −1      m
specificity and the distinctiveness of categories as                                                                                ∑∑   i
                                                                                                                                                    w ⋅ w j ⋅ d ( Ci , C j )
                                                                                                                                                  j i
homogeneity intra-cluster and separation inter-cluster,                                                                  S ( CL ) ≡  1       i +1
                                                                                                                                                m −1 m
respectively.                                                                                                                                   ∑∑    i      j
                                                                                                                                                               wi ⋅ w j
                                                                                                                                                  1     i +1
Formalization: Definitions and Notations                                                              Due      to     the      above                  definitions            of distance,
                                                                                                       −1 ≤ H ( CL ) ≤ 0, 0 ≤ S ( CL ) ≤ 1 .
Let us denote S as the universe of all possible elements, said
items, with L attributes. Each element is represented by an                                              A high value for H is usually related to a clustering with a
L-ple: X ≡ ( x(1) ,...., x( L ) ) , where X ∈ S and x (i ) denotes its i–                             quite high number of clusters, whereas a high value for S
                                                                                                      usually means that the clustering has a quite low number of
th attribute. Let us consider a data set with cardinality n, i.e.                                     clusters. So, maximization of both has contrasting needs.
the subset of S, which must be partitioned. Let us denote
this set with DS ≡ { X1 ,……, X n } , with, in general: DS ⊆ S .                                          Prototypes based categorization can be seen as a problem
Distance between Observations. Distance between                                                       of optimal location for prototypes in observation space
observations is defined based on the type of attributes. For                                          rather than finding partitioning for the given data set. Then
binary attributes we use the Hamming distance, while for                                              prototype localization can be performed driven by some
numerical attributes we use the linear distance:                                                      measure of goodness as the maximization of intra–cluster
δ ( i ) ( x (i ) , y ( i ) ) = x ( i ) − y ( i ) . We have chosen as definition of                    homogeneity, inter–cluster separability, or both so that
                                                                                                      members of the same cluster are as similar as possible one
distance between items a normalized city-block                                                (or     another and as different as possible from members in other
Manhattan) distance:                                                                                  clusters.
                                             1 L ⎛ 1                                        ⎞            Being concurrent maximization of homogeneity and
                        d ( X ,Y ) =            ∑
                                             L 1 ⎝R i⎜    (i )
                                                               ⋅ δ (i ) ( x (i ) , y (i ) ) ⎟
                                                                                            ⎠         separability impossible then categorization can be
                   (i )                                                                               performed maximizing only a trade off between intra–
where R                    is the range of i-th attribute:                                            cluster homogeneity and inter–cluster separability.
                                            ⎛                                             ⎞              This trade-off is embedded in our model by linear
                           R ( i ) ≡ δ (i ) ⎜ Max ⎡⎣ x (ji ) ⎤⎦ , Min ⎡⎣ x (ji ) ⎤⎦ ⎟
                                            ⎝ j∈{1,...,n}          j∈{ 1,..., n}          ⎠           dependency of fitness function from both components.
(xj(i) is the i–th attribute of the j–th observation).
 ∀ X , Y ∈ DS it results 0 ≤ d ( X , Y ) ≤ 1                                                             Syndrome Discovery in Clinical Semeiotics
Distance between Clusters. If Pi and Pk represent the                                                 Semeiotics is the medical discipline that studies signs and
prototypes for clusters Ci and Ck, distance between those                                             symptoms addressing towards diagnosis of pathologies. One
clusters is defined as distance between prototypes:                                                   of its goals is discovery of syndromes.
                                          d (Ci , Ck ) = d ( Pi , Pk )
                                                                                                  733

   The term syndrome is the association of several clinically                               Cluster Validation
recognizable features which often occur together, in similar
                                                                     Cluster validity analysis is the assessment of a clustering
way and without explicit reference to its pathological
                                                                     procedure’s output. Often this analysis uses a domain
factors.
                                                                     specific criterion of optimality (Jain et al. 1999).
   Therefore a syndrome can be considered as a prototype of
                                                                        We define a Pathology Addressing Index, PAI, which can
a clinical observations class.
                                                                     be useful to evaluate the degree of utility of discovered
   A syndrome can be expression of a specific pathology or
                                                                     syndromes in non–ambiguously addressing etiological
of totally different pathologies. Discovery of a syndrome is
                                                                     investigation of underlying physio–pathological causes of
important because it addresses next etiological phase, i.e.
                                                                     disease, which is supposed unknown.
investigation of underlying physio–pathological causes,
                                                                        For these reasons the index PAI can quantify the
with the aim to determine possible diseases causing
                                                                     epistemic adequacy of a considered group of prototypes in
syndrome appearance.
                                                                     clinical domain.
   For instance SARS (Severe Acute Respiratory Syndrome)
is a recent example of a syndrome discovery (a new                   Pathology Addressing Index
category among clinical observation) that was later
explained with the identification of a causative coronavirus.        Given a data set of clinical observations, each item belongs
   Hence discovery of a syndrome is the first phase of               to only one of the p pathologies (classes) based on medical
scientific discovery process in the medical domain.                  opinion contained in the data set. By sharp clustering, each
   Evidently syndrome based categorization are not                   item is assigned to only one of s syndromes (represented by
composed of classical immutabile categories, so they are, by         a cluster). Then a table between s syndromes and p
contrast, better represented from dynamically evolving               pathologies can be built that corresponds to a matrix A,
prototypes (cf. Smith, E. E., 1988; Smith, E. E., 1995).             where element aij is the number of elements in the data set
   Moreover syndrome detection is useful in facilitating             assigned to syndrome j which are affected by pathology i.
differential diagnosis and in outbreak surveillance.                    Based on this matrix we define a Pathology Addressing
                                                                     Index (PAI). Firstly, we define PAI for the generic j-th
The Considered Data Set                                              syndrome as follows:
                                                                                             p
                                                                                                 aij   where α ( j ) = Max aij
We have used the “dermatology” data set of UCI machine                          PAI ( j ) = ∑ i
                                                                                            1 α
                                                                                                  ( j)
                                                                                                                         i∈{1.. p}
learning repository (Blake & Merz 1998), which contains
clinical dermatological observations. It has been chosen                This index is 1 in the best case, in which the syndrome
because it also presents classification into pathologies. So         represents only one pathology, and is p in the worst case, in
we are able to compare syndromes discovered by our system            which the syndrome addresses uniformly towards all
with known pathologies. This concurs us to validate a                considered pathologies.
posteriori the etiological quality, i.e. epistemic adequacy, of         Then, we define PAI for the whole categorization as the
prototypes founds.                                                   weighted average for PAI(j):
   Pathologies present in the data set are six (see Table 1).                                   q                    q
                                                                                     PAI = ∑ j w( j ) ⋅ PAI ( j )  ∑   j
                                                                                                                         w( j )
        Table 1: Pathologies contained in the data set.                                         1                    1
                                                                     where wj denotes the weight of j–th syndrome, i.e. the
                                              No. of                 number of clinical cases assigned to j–th category. PAI can
          Id.        Pathologies                                     vary within 1 and p, and the higher its value the more
                                          Observation
           1   Psoriasis                       112                   ambiguously on average each syndrome addresses towards p
           2   Seboreic dermatitis              61                   pathologies rather than towards one. PAI does not depend
           3   Lichen planus                    72                   explicitly on s, and can tend easily to 1 as the number of
           4   Pityriasis rosea                 49                   syndromes increases. In fact, any clinical case could be
           5   Chronic dermatitis               52                   considered as a degenerate syndrome on its own.
           6   Pityriasis rubra pilaris        20                       For the considered data set we have: 1 ≤ PAI ≤ 6
                                                                        It can be observed that it is possible to compute this index
   This data set consists of 366 clinical cases, each with 34        only if pathologies are already known. Therefore it can be
attributes, 12 of which clinical and 22 hysto–pathological.          used for validation of clustering algorithm and it cannot is
Any attribute, apart from family anamnesis and age, is               be used from a clustering algorithm, and of course it was not
expressed by an integer value in [0, 3] where 0 means that           designed for this.
the attribute is not present, 3 states that it is present at its
maximum degree, 1 and 2 denote intermediate values.                           The Evolutionary Learning System
   Family anamnesis is a boolean attribute, has value 1 if                                      Implemented
any pathology has been observed in the family, 0 otherwise.          We have created an evolutionary learning system, called
Age is expressed by an absolute integer value.                       SDS (Syndrome Discovery System), based on a new Genetic
                                                                     Algorithms (GAs) (Holland 1975, Goldberg 1989), named
                                                                 734

Self–sizing Genome Genetic Algorithm (De Falco, Della                                          Mutation Operator. It acts on single prototype attributes of
Cioppa, Gagliardi, & Tarantino 2005) developed for                                             the offspring achieved by gene–pooling, and modifies them
unsupervised cluster analysis.                                                                 with a given probability. This probability is normalized with
   It is self–sizing with respect to the length of genotype                                    respect to whole genotype length to avoid that genotypes
encoding the prototypes. This algorithm finds the number of                                    with more chromosomes are more exposed to mutations.
categories and categories themselves through a direct search                                   Gene-pooling Operator. Selected two parents, this operator
of the prototypes and then segmentation of data set is                                         creates an offspring (prototypes list), with a variable number
achieved with sharp clustering concept.                                                        of chromosomes chosen from the set of chromosomes
   Genotype variable length is an essential feature of our                                     contained in both parents. Gene-pooling does not impose
algorithm which allows us not to require a priori knowledge                                    limits on maximal and minimal length of offspring. This
about number of prototypes, because this value is implicitly                                   operator is of the type: ( G (k1 ) , G (k 2 ) ) ⎯⎯      k
                                                                                                                                                         → G (k) with
encoded in the genotype itself, as its length, so it is a part of
the solution to find, together, of course, with prototypes                                      k ∈ [ 2, k1 + k2 ] . Probability   distribution     for     offspring
themselves. Our system can control on its own the length of                                    genotype length k is defined as uniform.
genotypes present in the population, thanks also to the                                           This operator is also based on deterministic internal
definition of a new crossover operator, named gene-pooling.                                    selection for the choice of chromosomes. Selection takes
   The adaptive prototypes search is driven by a fitness                                       into account weights (i.e. clusters cardinality), which
function that simultaneously maximize homogeneity and                                          chromosomes, seen as prototypes, have in parents
separability of the found categories by means of a suitable                                    genotypes.
parameter called scale factor.                                                                    Given two genotypes G1(k1 ) and G (k2 2 ) , Algorithm 1 shows
                                                                                               gene–pooling procedure.
Genotype
   It is constituted by more chromosomes, each encoding a                                                     Algorithm 1: Gene–Pooling Algorithm
prototype. A prototype belongs to the environment set S of
the given data set DS, so it is a list of attributes according to                              1.    Create genetic soup GS(k1 +k 2 ) = G1(k1 ) ∪ G (k2 2 )
same order used for objects of the data set. Formally, a                                       2.    Eliminate duplicate chromosomes in GS (k1 + k 2 ) thus
genotype is a list (with variable length) of chromosomes
defined as:                                                                                          obtaining GS (k )
    G ( k ) ≡ ( P1 ; … ; Pk ) ≡   (( x1
                                       (1)
                                           , … , x1( m ) ) ; …… ; ( xk(1) , … , xk( m) ) )     3.    Order chromosomes in GS (k ) based on weight
                                                                                               4.    Randomly choose number k of chromosomes for the
where Pj is the prototype of j–th cluster, xi(j) is the i–th
attribute of j–th cluster and k is not set a priori. Any                                             offspring, k ∈ [ 2, k1 + k2 ]
attribute is represented by values of same type as attribute in                                5.    Give the first k prototypes of GS (k ) to offspring
its own variation range.                                                                             genotype G (k)
Fitness Function
                                                                                                  Our system, thanks to the mechanism of selection internal
   After performing data set segmentation based on a                                           to crossover operator and thanks to the particular type of
genotype G, its quality must be evaluated by using a fitness                                   fitness, shows the emergent ability to control lengths of
function f. Choice of fitness function is critical especially as                               genotypes in the population and accordingly it finds the
concerns number of categories N.                                                               number of categories.
   Following Yip (2002) we have define it as linear function
of homogeneity and separation as follows:                                                                Experimental Results and Discussion
                          f fitness = H ( G ) + µ ⋅ S ( G )
                                                                                               Several tests has been implemented to changing operational
where µ represents a scaling factor (0 ≤ µ).                                                   parameters of the SDS, particularly we analyse here the test
   The following limit cases exist:                                                            suite performed to varying scale factor µ, that are
                  µ → 0 ⇒ f fitness = H ( G ) ⇒ N → +∞                                         responsible of the trade-off between homogeneity and
               µ → +∞ ⇒ f fitness = S ( G ) ⇒ N → 1                                            separation.
                                                                                                  We have taken into account for µ the set of values in the
   So categorization is formulated as a problem of direct
                                                                                               range [0.00; 1.00] with step 0.05. For each such value our
maximization of trade-off between H and S independently
                                                                                               genetic algorithm has been run twenty times.
of number of categories N, yet as µ varies a control on N
                                                                                                  In all experiments we have chosen population size
will be indirectly achieved. Given the above ranges for H
                                                                                               Ps = 100 and maximum number of generations Ng = 250.
(G) and S (G), and constrained µ ≤ 1, it turns out that:
                                                                                                  The aim is to study variation of the solutions found,
 -1 ≤ ffitness(G) ≤ +1.
                                                                                               homogeneity, separability and in particular the number of
Genetic Operators                                                                              categories with theirs epistemic adequacy as a function of
                                                                                               scale factor µ.
Selection. Selection is based on roulette wheel method.
                                                                                           735

  The results achieved are shown in Fig.1 and Fig.2. In the                                           low distinctiveness and high cardinality, so we can consider
former we show the dependence from parameter µ of                                                     them as subordinate categorizations.
homogeneity and separability of solutions found.                                                         In fact the achieved prototypes-syndromes have a good
  In the next one we show the dependence from parameter                                               effectiveness, they often individualize an only pathology:
µ of categorizations cardinality and PAI of solutions found.                                          the values of PAI are close to 1, the ideal value.
                                                                                                         Otherwise in second region (µ ≥ 0.60) our system
                                        Homogeneity        Separation
                                                                                                      maximizes the separation mainly. Obtained categorizations
                  -0.08                                                     0.38
                                                                                                      have low specificity, high distinctiveness and low number of
                  -0.09                                                     0.36
                                                                                                      categories, and then they can be considered some
                   -0.1                                                     0.34                      categorizations of superordinate type. In fact the prototype-
                  -0.11                                                     0.32                      syndromes gotten have a not so good effectiveness, they
    Homogeneity
                                                                                                      individualize around two pathologies (PAI ≈ 1.9).
                                                                                   Separation
                  -0.12                                                     0.3
                  -0.13                                                     0.28                         Even if they can be considered some valid categories
                  -0.14                                                     0.26                      since they bring values of PAI well distant from the worse
                  -0.15                                                     0.24                      possible value (PAI = 6).
                  -0.16                                                     0.22                         In intermediate region we obtain a continuous range of
                  -0.17                                                     0.2
                                                                                                      possible categorizations from subordinate ones to
                  -0.18                                                     0.18
                           0      0.25           0.5           0.75     1
                                                                                                      superordinate ones, varying from those with great
                                           µ Scale Factor
                                                                                                      specificity and high number of categories to those with great
                                                                                                      distinctiveness and small number of categories.
                                                                                                         We can observe that the natural hierarchy of categories
                       Figure 1: Average values for H, S, as a function                               could in part arise from the trade off between specificity and
                                     of µ scale factor                                                distinctiveness.
                                                                                                         The basic categorizations would be fixed in the middle of
                                          Categories       PAI
                                                                                                      intermediary region where a right compromise can be
                                                                                                      individualized also in relationship to the etiological utility of
                  25
                                                                            2.2                       the found prototypes, quantified thanks to the Pathology
                  20                                                                                  Addressing Index.
                                                                            2
                                                                                                         In fact PAI has a almost constant behaviour for up to
 Cardinality
                  15                                                        1.8                       values of µ equal to 0.35 therefore it grows linear up to
                                                                            1.6      P.A.I.           values of µ equal to 0.65 for then to be almost constant
                  10                                                                                  again. This behaviour of PAI is not completely lined up with
                                                                            1.4
                                                                                                      the three regions individualized first.
                  5
                                                                            1.2                          Therefore we observe that for values of µ ≈ 0.35-0.40, the
                                                                                                      cut into halves of the number of categories to 9-12 (respect
                  0                                                         1
                       0         0.25           0.5           0.75      1
                                                                                                      the around 20 of the subordinate) produces a worsening of
                                          µ Scale Factor                                              prototype usefulness that are negligible or light: PAI
                                                                                                      increase from about 1.19 (for µ ≤ 0.25) to 1.27 – 1.40.
                                                                                                         We could identify for this middle values of µ (as above
                       Figure 2: Average values for cardinality and PAI                               recognized) of the intermediary region, the basic level
                                as a function of µ scale factor                                       observing that this natural level emerges either from the
                                                                                                      trade-off between homogeneity and separation either from
   From simple theoretical point of view we expect that as µ                                          maximizing the etiological usefulness (to minimize the PAI)
increases the homogeneity, separability and cardinality                                               in comparison to the cognitive effort of increasing the
change with continuity while in this data we can                                                      number of prototypes.
discriminate three regions.                                                                              In other terms for µ ≈ 0.35 our model exhibits a ‘critical’
   Two are the limit cases. In fact, we see that for µ ≤ 0.25                                         point beyond which a magnification of specificity and
obtained categorizations have about the same values for                                               consequently increase of number of categories is not
homogeneity, separation, number of categories and PAI,                                                rewarded by an increase of etiological usefulness of the
likewise for µ ≥ 0.60 but with reverse values.                                                        found prototypes.
   The third and most interesting region is the intermediate                                             This second aspect of basic level can be synthesized
one, i.e. that for 0.25 ≤ µ ≤ 0.60. In this region an almost                                          through the well known principle of parsimony “Pluralitas
monotonic behaviour can be observed, decreasing for                                                   non est ponenda sine necessitate” expressed by Ockham
homogeneity and number of categories and increasing for                                               which translates into English and into medical domain as
separation and PAI.                                                                                   syndromes should not be posited without necessity.
   We try to explicate this data. In first region (µ ≤ 0.25)                                             Here, we argue that the basic level would be also
obtained categorizations magnify more homogeneity rather                                              characterized by the methodological reductionism, over that
that separation. These categorizations have high specificity,
                                                                                                736

from the right compromise between specificity and                   Holland, J.H. (1975). Adaptation in natural and artificial
distinctiveness.                                                      systems, (2nd ed.). The University of Michigan Press.
                                                                    Hull, C.L. (1943). Principles of behaviour. NY: Appleton-
     Concluding Remarks and Future Works                              Century.
In this work we have adopted an ‘artifact approach’ for             Jain, A.K., Murty, M.N., & Flynn, P.J. (1999). Data
modelling and understanding a cognitive skill of human                clustering: a review. ACM Computing Surveys, 31(3),
mind, according to well rooted perspective of the ‘robot              266-323.
approach’ (Hull, C.L. 1943) and mechanistic explanation of          Katz, J.J. (1972). Semantic theory. New York: Harper &
intelligent behaviour (Cordeschi, R. 2002).                           Row.
   We have modeled the syndrome discovery process as an             Katz, J.J., & Fodor, J.A. (1963). The structure of semantic
adaptive prototype-based categorization.                              theory. Language, 39, 170-210.
   This assumption has been evaluated through the definition        Kaufman, L., & Rousseeuw, P.J. (1990). Finding groups in
of a domain specific index for cluster validation that                data. An introduction to cluster analysis. New York:
measures the effectiveness of the prototype-syndrome                  Wiley and Sons.
founds for addressing etiological investigations.                   Komatsu, L.K. (1992). Recent views on conceptual
   We have provided an evolutionary computational                     structure. Psychological Bulletin, 112(3), 500-526.
cognitive model of the prototypes theory of categorization          Minda, J.P. & Smith, J.D. (2002). Comparing prototype-
able to reproduce several levels of categorizations                   based and exemplar-based accounts of category learning
spontaneously.                                                        and attentional allocation. Journal of Experimental
   This model shows a possible explanation of the                     Psychology: Learning, Memory, and Cognition, 28, 275-
emergency of the categorization levels as trade-off between           292.
specificity and distinctiveness. Also, basic level is linked to     Murphy, G.L. (1991). Parts in objects concepts:
a point of equilibrium between the aetiological utility of the        Experiments with artificial categories. Memory &
prototypes and the principle of parsimony.                            Cognition, 19, 423-438.
   Moreover, our model, based on a genetic algorithm,               Murphy, G.L., & Brownell, H. H. (1985). Category
implements an evolutionary machine learning model for an              differentiation in object recognition: Typicality
adaptive human learning, so according to E. Mach and his              constraints on the basic category advantage. Journal of
biological view of knowledge (Pléh, C. 1997) we can argue a           Experimental Psychology: Learning, Memory and
possible algorithmic uniformity between adaptive individual           Cognition, 11, 70-84.
learning and evolutionary learning by species selection.            Pléh, C. (1997). Ernst Mach and Daniel Dennett: Two
   Future research will need conducted to deeply analyse and          Evolutionary Models of Cognition. Proceedings of the
model the interplay of trade-off between specificity and              nineteenth Annual Conference of the Cognitive Science
distinctiveness with epistemic adequacy of prototypes in              Society. Mahwah, NJ: Lawrence Erlbaum Associates.
clinical field and other ones.                                      Rosch, E. (1975). Cognitive Representations of Semantic
                                                                      Categories. Journal of Experimental Psychology, 104(3),
                    Acknowledgments                                   192-233.
                                                                    Rosch, E., & Mervis, C.B. (1975). Family resemblance:
I wish to thank Prof. V. Cordeschi for the improving                  Studies in the internal structure of categories. Cognitive
comments provided on an earlier version of this paper.                Psychology, 7, 573-605.
                                                                    Rosch, E., Mervis, C. B., Gray, W. D., Johnson, D. M., &
                         References                                   Boyes-Braem, P. (1976). Basic objects in natural
Blake, C.L., & Merz, C.J. (1998). UCI Repository of                   categories. Cognitive Psychology, 8, 382-452.
   machine                  learning                 databases      Smith, E. E. (1988). Concepts and thoughts. In R.J.
   [http://www.ics.uci.edu/~mlearn/MLRepository.html].                Sternberg & E. E. Smith (Eds.), The psychology of human
   Department of Information and Computer Science,                    thought (pp. 19-49). New York: Cambridge University
   University of California. Irvine, CA.                              Press.
Cordeschi R. (2002). The Discovery of the Artificial.               Smith, E. E., (1995). Concept and Categorization. In E.E.
   Behavior, Mind and Machines Before and Beyond                      Smith & D. N. Osherson (Eds.), An Invitation to cognitive
   Cybernetics. Dordrecht: Kluwer Academic Publishers.                science (Vol.3, Thinking) (2nd ed., pp. 3-33). Cambridge,
De Falco, I., Della Cioppa, A., Gagliardi, F., & Tarantino,           MA: MIT Press.
   E. (2005). A New Variable–Length Genome Genetic                  Smith, E. E., & Medin, D.L. (1981). Categories and
   Algorithm for Data Clustering in Semeiotics. Accepted              Concepts. Cambridge, MA: Harvard University Press.
   paper to The 20 th ACM Symposium on Applied                      Yip, A.M. (2002), A scale dependent data clustering model
   Computing. March 13-17, 2005, Santa Fe, New Mexico.                by direct maximization of homogeneity and separation.
Goldberg, D.E. (1989). Genetic algorithms in search                   Proc. Mathematical Challenges in Scientific Data Mining
   optimization and machine learning. Reading, MA:                    IPAM. January 14–18, 2002.
   Addison–Wesley.
                                                                737

