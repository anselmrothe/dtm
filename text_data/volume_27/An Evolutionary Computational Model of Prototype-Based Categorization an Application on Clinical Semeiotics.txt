UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
An Evolutionary Computational Model of Prototype-Based Categorization: an Application on
Clinical Semeiotics

Permalink
https://escholarship.org/uc/item/1pk0k4zg

Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 27(27)

Author
Gagliardi, Francesco

Publication Date
2005-01-01
Peer reviewed

eScholarship.org

Powered by the California Digital Library
University of California

An Evolutionary Computational Model of Prototype-Based Categorization:
an Application on Clinical Semeiotics
Francesco Gagliardi (francesco.gagliardi@libero.it)
Department of Physical Sciences — University of Naples “Federico II”
Via Cintia — I-80126 Napoli, Italy
(2002) for a comparison between models) overcome the
constraints of these classical theories.
Notably the prototype theory (Rosch, E. 1975; Rosch, E.,
& Mervis, C.B. 1975) asserts that people categorize on the
basis of how close something is to the prototype or ideal
member of a category. For example, a robin is closer to the
bird prototype than an ostrich is, but they are both closer to
it than they are to the prototype of a fish, so we call them
both birds, only it takes longer to say an ostrich is a bird
than it take to say a robin is a bird, because the ostrich is
further from the prototype.
Besides, it is well established that people can categorize
the same objects at different levels of abstraction.
Rosch et al.’s (1976) seminal research isolated three
“natural” levels of object categorization: the superordinate
(e.g. furniture), the basic (e.g. chair), and the subordinate
(e.g. Chippendale chair).
Of these, the basic level is known to have a privileged
status that is often attributed to the organization of
categories in memory.
The origin of the bias to the basic level is still a matter of
debate. In categorization, researchers have proposed that
categories at the basic level are more differentiated that is,
“... have the most attributes common to members of the
category and the least attributes shared with members of
other [contrasting] categories.” (Rosch et al., 1976, p. 435)
The first component of this differentiation definition has
been called the specificity (Murphy & Brownell, 1985), or
the informativeness (Murphy, 1991) of a category, and the
second component the distinctiveness of a category
(Murphy & Brownell, 1985; Murphy, 1991).
The difference between distinct types of categorizations
would thus stem from distinct differentiations at these two
levels.
Both these aspects of categorization have been embedded
in our model as explained in next sections.

Abstract
The aim of this paper is to present a software artifact for
machine supported understanding and modelling of prototypebased categorization. This software system is able to perform
discovery of syndromes (seen as prototypes) into a given data
set of clinical observations. A new genetic algorithm is used
to achieve an unsupervised categorization of observation via
adaptive learning of number and features of prototypes. Its
evolutionary learning is oriented to maximize specificity and
distinctiveness of categories. Experimental results show that
prototype-based categorization of clinical observation is
suitable for syndrome-based categorization. The trichotomy
of categorizations (superordinate, basic and subordinate) is
explained by trade off between specificity and distinctiveness.
Moreover the natural basic level is also related to epistemic
adequacy of found prototypes.
Keywords: Categorization; Artificial Intelligence; Prototype
Theory; Syndrome Discovery; Basic Level, Real-Life
Problem.

In cognitive psychology the problem of the categorization
has been formulated, according to prototypes theory, as a
problem of similarity between a prototype and the members
of the category that this represents.
Cluster analysis is a machine learning task for partitioning
a given data set of observations in groups of similar item.
We have realized an evolutionary machine learning
system for cluster analysis, which embeds prototype theory
of categorization.
The aim is to use this software artifact for machinesupported explanations of adaptive categorization skill
observed in human mind.
The multiple realizability of intelligent behaviour is one
useful and objective explanatory strategy of cognitive
sciences.

Categorization and Prototype Theory

Cluster Analysis and Categorization

Categorization is one of the most vexed unsolved issues in
cognitive science field.
Traditional theories about categorization argue that people
categorize using the common features of the members.
These features may be viewed as defining features because
they are singly necessary and jointly sufficient to define the
category (Katz, J.J., 1972; Katz, J.J. & Fodor, 1963).
In other words, each feature is an essential element of the
category; together, the properties uniquely define the
category.
The prototype theory as well as the exemplar-based
models of categorization (see Minda, J.P. & Smith, J.D.

Cluster analysis (Jain et al. 1999) (Kaufman and Rousseeuw
1990) is the unsupervised classification (natural grouping)
of observations (patterns, data items, or feature vectors) into
groups (clusters) based on similarity. Intuitively, patterns
within a valid cluster are more similar to each other than
they are to a pattern belonging to a different cluster.
There is an obvious analogy between cluster analysis and
the cognitive problem of categorization. The usual
algorithms used in such field of the data analysis, (for
instance statistical-iterative ones) are considered

732

unsupervised by contrast to classification ones because they
not require training set. However they require some
knowledge that the human must supply to the system, as for
instance number of categories. Moreover, there is often no
explicit maximization of specificity and distinctiveness.
Then these algorithms cannot be considered, ipso facto, as
computational models of the human mind categorization.

Prototypes and Sharp Clustering for Cluster
Representation. We represent a cluster Ck through an item
of S called prototype Pk which, in general, does not
coincide with cluster centroid. A generic item Xi∈ DS is
assigned to the cluster Ck whose representative is the
closest. Formally:
if ∀X i ∃k : ∀k ≠ k d ( X i , Pk ) < d ( X i , Pk ) then X i ∈ Ck

Prototype-Based Cluster Analysis

A clustering can be denoted with the list of the
representatives of its clusters: CL ≡ { P1 ,....., Pm }

According to the prototype theory, an observation will be
categorized as an instance of a category if it is sufficiently
similar to the prototype. Exactly what is meant by similarity
to a prototype can be a complex issue, and there are actually
different theories of how this similarity should be measured
(Smith, E.E. & Medin, 1981). Moreover, many researchers
suggest that some features should be weighted more heavily
as being more central to the prototype than are other
features (e.g. Komatsu, 1992).
In our computational model we have defining a simply
sharp clustering criterion based on the number of similar
features shared between an observation and the prototype,
counted by range weighted city block distance.
We can define quantitatively similarity between item and
prototype, thanks to the definition of a distance between
observations. Moreover we can define quantitatively the
specificity and the distinctiveness of categories as
homogeneity intra-cluster and separation inter-cluster,
respectively.

Homogeneity and Separability. Homogeneity for a cluster
Ck is defined as:
H ( Ck ) ≡ −

∑ d (X,P )
wk

where Pk is the cluster prototype and wk is the cardinality of
k-th cluster. Hence we can define clustering homogeneity as
weighted average of homogeneity of clusters, and
separability for a clustering as the weighted average of
distances among clusters based on above metrics:
m

H ( CL ) ≡

∑ w ⋅ H (C )
i

1

i

i

m

∑w
1

m −1

S ( CL ) ≡

m

1

i

j i
i +1
m −1 m

∑∑

Let us denote S as the universe of all possible elements, said
items, with L attributes. Each element is represented by an
L-ple: X ≡ ( x(1) ,...., x( L ) ) , where X ∈ S and x (i ) denotes its i–
th attribute. Let us consider a data set with cardinality n, i.e.
the subset of S, which must be partitioned. Let us denote
this set with DS ≡ { X1 ,……, X n } , with, in general: DS ⊆ S .

j

wi ⋅ w j

i +1

Prototypes based categorization can be seen as a problem
of optimal location for prototypes in observation space
rather than finding partitioning for the given data set. Then
prototype localization can be performed driven by some
measure of goodness as the maximization of intra–cluster
homogeneity, inter–cluster separability, or both so that
members of the same cluster are as similar as possible one
another and as different as possible from members in other
clusters.
Being concurrent maximization of homogeneity and
separability impossible then categorization can be
performed maximizing only a trade off between intra–
cluster homogeneity and inter–cluster separability.
This trade-off is embedded in our model by linear
dependency of fitness function from both components.

Distance between Observations. Distance between
observations is defined based on the type of attributes. For
binary attributes we use the Hamming distance, while for
numerical attributes we use the linear distance:
δ ( i ) ( x (i ) , y ( i ) ) = x ( i ) − y ( i ) . We have chosen as definition of

where R

i

Due
to
the
above
definitions
of
distance,
−1 ≤ H ( CL ) ≤ 0, 0 ≤ S ( CL ) ≤ 1 .
A high value for H is usually related to a clustering with a
quite high number of clusters, whereas a high value for S
usually means that the clustering has a quite low number of
clusters. So, maximization of both has contrasting needs.

Formalization: Definitions and Notations

d ( X ,Y ) =

i

i

w ⋅ w j ⋅ d ( Ci , C j )

∑∑

1

distance between items a normalized city-block
Manhattan) distance:

k

∀X ∈CK

(or

1 L ⎛ 1
⎞
∑ ⎜ ⋅ δ (i ) ( x(i ) , y (i ) ) ⎟⎠
L 1 i ⎝ R (i )

(i )

is the range of i-th attribute:
⎛
⎞
R ( i ) ≡ δ (i ) ⎜ Max ⎡⎣ x (ji ) ⎤⎦ , Min ⎡⎣ x (ji ) ⎤⎦ ⎟
j∈{1,..., n}
1,...,
n
j
∈
{
}
⎝
⎠
(xj(i) is the i–th attribute of the j–th observation).
∀ X , Y ∈ DS it results 0 ≤ d ( X , Y ) ≤ 1
Distance between Clusters. If Pi and Pk represent the
prototypes for clusters Ci and Ck, distance between those
clusters is defined as distance between prototypes:
d (Ci , Ck ) = d ( Pi , Pk )

Syndrome Discovery in Clinical Semeiotics
Semeiotics is the medical discipline that studies signs and
symptoms addressing towards diagnosis of pathologies. One
of its goals is discovery of syndromes.

733

The term syndrome is the association of several clinically
recognizable features which often occur together, in similar
way and without explicit reference to its pathological
factors.
Therefore a syndrome can be considered as a prototype of
a clinical observations class.
A syndrome can be expression of a specific pathology or
of totally different pathologies. Discovery of a syndrome is
important because it addresses next etiological phase, i.e.
investigation of underlying physio–pathological causes,
with the aim to determine possible diseases causing
syndrome appearance.
For instance SARS (Severe Acute Respiratory Syndrome)
is a recent example of a syndrome discovery (a new
category among clinical observation) that was later
explained with the identification of a causative coronavirus.
Hence discovery of a syndrome is the first phase of
scientific discovery process in the medical domain.
Evidently syndrome based categorization are not
composed of classical immutabile categories, so they are, by
contrast, better represented from dynamically evolving
prototypes (cf. Smith, E. E., 1988; Smith, E. E., 1995).
Moreover syndrome detection is useful in facilitating
differential diagnosis and in outbreak surveillance.

Cluster Validation
Cluster validity analysis is the assessment of a clustering
procedure’s output. Often this analysis uses a domain
specific criterion of optimality (Jain et al. 1999).
We define a Pathology Addressing Index, PAI, which can
be useful to evaluate the degree of utility of discovered
syndromes in non–ambiguously addressing etiological
investigation of underlying physio–pathological causes of
disease, which is supposed unknown.
For these reasons the index PAI can quantify the
epistemic adequacy of a considered group of prototypes in
clinical domain.

Pathology Addressing Index
Given a data set of clinical observations, each item belongs
to only one of the p pathologies (classes) based on medical
opinion contained in the data set. By sharp clustering, each
item is assigned to only one of s syndromes (represented by
a cluster). Then a table between s syndromes and p
pathologies can be built that corresponds to a matrix A,
where element aij is the number of elements in the data set
assigned to syndrome j which are affected by pathology i.
Based on this matrix we define a Pathology Addressing
Index (PAI). Firstly, we define PAI for the generic j-th
syndrome as follows:

The Considered Data Set

aij where ( j )
α = Max aij
( j)
i∈{1.. p}
α
1
This index is 1 in the best case, in which the syndrome
represents only one pathology, and is p in the worst case, in
which the syndrome addresses uniformly towards all
considered pathologies.
Then, we define PAI for the whole categorization as the
weighted average for PAI(j):
p

PAI ( j ) = ∑ i

We have used the “dermatology” data set of UCI machine
learning repository (Blake & Merz 1998), which contains
clinical dermatological observations. It has been chosen
because it also presents classification into pathologies. So
we are able to compare syndromes discovered by our system
with known pathologies. This concurs us to validate a
posteriori the etiological quality, i.e. epistemic adequacy, of
prototypes founds.
Pathologies present in the data set are six (see Table 1).

q

PAI = ∑ j w( j ) ⋅ PAI ( j )

Table 1: Pathologies contained in the data set.
Id.

Pathologies

1
2
3
4
5
6

Psoriasis
Seboreic dermatitis
Lichen planus
Pityriasis rosea
Chronic dermatitis
Pityriasis rubra pilaris

1

q

∑

j

w( j )

1

where wj denotes the weight of j–th syndrome, i.e. the
number of clinical cases assigned to j–th category. PAI can
vary within 1 and p, and the higher its value the more
ambiguously on average each syndrome addresses towards p
pathologies rather than towards one. PAI does not depend
explicitly on s, and can tend easily to 1 as the number of
syndromes increases. In fact, any clinical case could be
considered as a degenerate syndrome on its own.
For the considered data set we have: 1 ≤ PAI ≤ 6
It can be observed that it is possible to compute this index
only if pathologies are already known. Therefore it can be
used for validation of clustering algorithm and it cannot is
be used from a clustering algorithm, and of course it was not
designed for this.

No. of
Observation
112
61
72
49
52
20

This data set consists of 366 clinical cases, each with 34
attributes, 12 of which clinical and 22 hysto–pathological.
Any attribute, apart from family anamnesis and age, is
expressed by an integer value in [0, 3] where 0 means that
the attribute is not present, 3 states that it is present at its
maximum degree, 1 and 2 denote intermediate values.
Family anamnesis is a boolean attribute, has value 1 if
any pathology has been observed in the family, 0 otherwise.
Age is expressed by an absolute integer value.

The Evolutionary Learning System
Implemented
We have created an evolutionary learning system, called
SDS (Syndrome Discovery System), based on a new Genetic
Algorithms (GAs) (Holland 1975, Goldberg 1989), named

734

Self–sizing Genome Genetic Algorithm (De Falco, Della
Cioppa, Gagliardi, & Tarantino 2005) developed for
unsupervised cluster analysis.
It is self–sizing with respect to the length of genotype
encoding the prototypes. This algorithm finds the number of
categories and categories themselves through a direct search
of the prototypes and then segmentation of data set is
achieved with sharp clustering concept.
Genotype variable length is an essential feature of our
algorithm which allows us not to require a priori knowledge
about number of prototypes, because this value is implicitly
encoded in the genotype itself, as its length, so it is a part of
the solution to find, together, of course, with prototypes
themselves. Our system can control on its own the length of
genotypes present in the population, thanks also to the
definition of a new crossover operator, named gene-pooling.
The adaptive prototypes search is driven by a fitness
function that simultaneously maximize homogeneity and
separability of the found categories by means of a suitable
parameter called scale factor.

Mutation Operator. It acts on single prototype attributes of
the offspring achieved by gene–pooling, and modifies them
with a given probability. This probability is normalized with
respect to whole genotype length to avoid that genotypes
with more chromosomes are more exposed to mutations.
Gene-pooling Operator. Selected two parents, this operator
creates an offspring (prototypes list), with a variable number
of chromosomes chosen from the set of chromosomes
contained in both parents. Gene-pooling does not impose
limits on maximal and minimal length of offspring. This
k
operator is of the type: ( G (k1 ) , G (k 2 ) ) ⎯⎯
→ G (k) with
k ∈ [ 2, k1 + k2 ] .

It is constituted by more chromosomes, each encoding a
prototype. A prototype belongs to the environment set S of
the given data set DS, so it is a list of attributes according to
same order used for objects of the data set. Formally, a
genotype is a list (with variable length) of chromosomes
defined as:

(( x

(1)
1

, … , x1( m ) ) ; …… ; ( xk(1) , … , xk( m) )

distribution

for

offspring

genotype length k is defined as uniform.
This operator is also based on deterministic internal
selection for the choice of chromosomes. Selection takes
into account weights (i.e. clusters cardinality), which
chromosomes, seen as prototypes, have in parents
genotypes.
Given two genotypes G1(k1 ) and G (k2 2 ) , Algorithm 1 shows
gene–pooling procedure.

Genotype

G ( k ) ≡ ( P1 ; … ; Pk ) ≡

Probability

Algorithm 1: Gene–Pooling Algorithm
1.
2.

)

3.
4.

where Pj is the prototype of j–th cluster, xi(j) is the i–th
attribute of j–th cluster and k is not set a priori. Any
attribute is represented by values of same type as attribute in
its own variation range.

5.

Fitness Function

Create genetic soup GS(k1 +k 2 ) = G1(k1 ) ∪ G (k2 2 )
Eliminate duplicate chromosomes in GS (k1 + k 2 ) thus
obtaining GS (k )
Order chromosomes in GS (k ) based on weight
Randomly choose number k of chromosomes for the
offspring, k ∈ [ 2, k1 + k2 ]
Give the first k prototypes of GS (k ) to offspring
genotype G (k)

Our system, thanks to the mechanism of selection internal
to crossover operator and thanks to the particular type of
fitness, shows the emergent ability to control lengths of
genotypes in the population and accordingly it finds the
number of categories.

After performing data set segmentation based on a
genotype G, its quality must be evaluated by using a fitness
function f. Choice of fitness function is critical especially as
concerns number of categories N.
Following Yip (2002) we have define it as linear function
of homogeneity and separation as follows:
f fitness = H ( G ) + µ ⋅ S ( G )
where µ represents a scaling factor (0 ≤ µ).
The following limit cases exist:
µ → 0 ⇒ f fitness = H ( G ) ⇒ N → +∞

Experimental Results and Discussion
Several tests has been implemented to changing operational
parameters of the SDS, particularly we analyse here the test
suite performed to varying scale factor µ, that are
responsible of the trade-off between homogeneity and
separation.
We have taken into account for µ the set of values in the
range [0.00; 1.00] with step 0.05. For each such value our
genetic algorithm has been run twenty times.
In all experiments we have chosen population size
Ps = 100 and maximum number of generations Ng = 250.
The aim is to study variation of the solutions found,
homogeneity, separability and in particular the number of
categories with theirs epistemic adequacy as a function of
scale factor µ.

µ → +∞ ⇒ f fitness = S ( G ) ⇒ N → 1
So categorization is formulated as a problem of direct
maximization of trade-off between H and S independently
of number of categories N, yet as µ varies a control on N
will be indirectly achieved. Given the above ranges for H
(G) and S (G), and constrained µ ≤ 1, it turns out that:
-1 ≤ ffitness(G) ≤ +1.

Genetic Operators
Selection. Selection is based on roulette wheel method.

735

The results achieved are shown in Fig.1 and Fig.2. In the
former we show the dependence from parameter µ of
homogeneity and separability of solutions found.
In the next one we show the dependence from parameter
µ of categorizations cardinality and PAI of solutions found.
Separation

-0.08

0.38

-0.09

0.36

-0.1

0.34

-0.11

0.32

-0.12

0.3

-0.13

0.28

-0.14

0.26

-0.15

0.24

-0.16

0.22

-0.17

0.2

-0.18

Separation

Homogeneity

Homogeneity

low distinctiveness and high cardinality, so we can consider
them as subordinate categorizations.
In fact the achieved prototypes-syndromes have a good
effectiveness, they often individualize an only pathology:
the values of PAI are close to 1, the ideal value.
Otherwise in second region (µ ≥ 0.60) our system
maximizes the separation mainly. Obtained categorizations
have low specificity, high distinctiveness and low number of
categories, and then they can be considered some
categorizations of superordinate type. In fact the prototypesyndromes gotten have a not so good effectiveness, they
individualize around two pathologies (PAI ≈ 1.9).
Even if they can be considered some valid categories
since they bring values of PAI well distant from the worse
possible value (PAI = 6).
In intermediate region we obtain a continuous range of
possible categorizations from subordinate ones to
superordinate ones, varying from those with great
specificity and high number of categories to those with great
distinctiveness and small number of categories.
We can observe that the natural hierarchy of categories
could in part arise from the trade off between specificity and
distinctiveness.
The basic categorizations would be fixed in the middle of
intermediary region where a right compromise can be
individualized also in relationship to the etiological utility of
the found prototypes, quantified thanks to the Pathology
Addressing Index.
In fact PAI has a almost constant behaviour for up to
values of µ equal to 0.35 therefore it grows linear up to
values of µ equal to 0.65 for then to be almost constant
again. This behaviour of PAI is not completely lined up with
the three regions individualized first.
Therefore we observe that for values of µ ≈ 0.35-0.40, the
cut into halves of the number of categories to 9-12 (respect
the around 20 of the subordinate) produces a worsening of
prototype usefulness that are negligible or light: PAI
increase from about 1.19 (for µ ≤ 0.25) to 1.27 – 1.40.
We could identify for this middle values of µ (as above
recognized) of the intermediary region, the basic level
observing that this natural level emerges either from the
trade-off between homogeneity and separation either from
maximizing the etiological usefulness (to minimize the PAI)
in comparison to the cognitive effort of increasing the
number of prototypes.
In other terms for µ ≈ 0.35 our model exhibits a ‘critical’
point beyond which a magnification of specificity and
consequently increase of number of categories is not
rewarded by an increase of etiological usefulness of the
found prototypes.
This second aspect of basic level can be synthesized
through the well known principle of parsimony “Pluralitas
non est ponenda sine necessitate” expressed by Ockham
which translates into English and into medical domain as
syndromes should not be posited without necessity.
Here, we argue that the basic level would be also
characterized by the methodological reductionism, over that

0.18
0

0.25

0.5

0.75

1

µ Scale Factor

Figure 1: Average values for H, S, as a function
of µ scale factor
Categories

PAI

25
2.2
2
1.8

15

1.6

10

P.A.I.

Cardinality

20

1.4
5

1.2

0

1
0

0.25

0.5

0.75

1

µ Scale Factor

Figure 2: Average values for cardinality and PAI
as a function of µ scale factor
From simple theoretical point of view we expect that as µ
increases the homogeneity, separability and cardinality
change with continuity while in this data we can
discriminate three regions.
Two are the limit cases. In fact, we see that for µ ≤ 0.25
obtained categorizations have about the same values for
homogeneity, separation, number of categories and PAI,
likewise for µ ≥ 0.60 but with reverse values.
The third and most interesting region is the intermediate
one, i.e. that for 0.25 ≤ µ ≤ 0.60. In this region an almost
monotonic behaviour can be observed, decreasing for
homogeneity and number of categories and increasing for
separation and PAI.
We try to explicate this data. In first region (µ ≤ 0.25)
obtained categorizations magnify more homogeneity rather
that separation. These categorizations have high specificity,

736

Holland, J.H. (1975). Adaptation in natural and artificial
systems, (2nd ed.). The University of Michigan Press.
Hull, C.L. (1943). Principles of behaviour. NY: AppletonCentury.
Jain, A.K., Murty, M.N., & Flynn, P.J. (1999). Data
clustering: a review. ACM Computing Surveys, 31(3),
266-323.
Katz, J.J. (1972). Semantic theory. New York: Harper &
Row.
Katz, J.J., & Fodor, J.A. (1963). The structure of semantic
theory. Language, 39, 170-210.
Kaufman, L., & Rousseeuw, P.J. (1990). Finding groups in
data. An introduction to cluster analysis. New York:
Wiley and Sons.
Komatsu, L.K. (1992). Recent views on conceptual
structure. Psychological Bulletin, 112(3), 500-526.
Minda, J.P. & Smith, J.D. (2002). Comparing prototypebased and exemplar-based accounts of category learning
and attentional allocation. Journal of Experimental
Psychology: Learning, Memory, and Cognition, 28, 275292.
Murphy, G.L. (1991). Parts in objects concepts:
Experiments with artificial categories. Memory &
Cognition, 19, 423-438.
Murphy, G.L., & Brownell, H. H. (1985). Category
differentiation in object recognition: Typicality
constraints on the basic category advantage. Journal of
Experimental Psychology: Learning, Memory and
Cognition, 11, 70-84.
Pléh, C. (1997). Ernst Mach and Daniel Dennett: Two
Evolutionary Models of Cognition. Proceedings of the
nineteenth Annual Conference of the Cognitive Science
Society. Mahwah, NJ: Lawrence Erlbaum Associates.
Rosch, E. (1975). Cognitive Representations of Semantic
Categories. Journal of Experimental Psychology, 104(3),
192-233.
Rosch, E., & Mervis, C.B. (1975). Family resemblance:
Studies in the internal structure of categories. Cognitive
Psychology, 7, 573-605.
Rosch, E., Mervis, C. B., Gray, W. D., Johnson, D. M., &
Boyes-Braem, P. (1976). Basic objects in natural
categories. Cognitive Psychology, 8, 382-452.
Smith, E. E. (1988). Concepts and thoughts. In R.J.
Sternberg & E. E. Smith (Eds.), The psychology of human
thought (pp. 19-49). New York: Cambridge University
Press.
Smith, E. E., (1995). Concept and Categorization. In E.E.
Smith & D. N. Osherson (Eds.), An Invitation to cognitive
science (Vol.3, Thinking) (2nd ed., pp. 3-33). Cambridge,
MA: MIT Press.
Smith, E. E., & Medin, D.L. (1981). Categories and
Concepts. Cambridge, MA: Harvard University Press.
Yip, A.M. (2002), A scale dependent data clustering model
by direct maximization of homogeneity and separation.
Proc. Mathematical Challenges in Scientific Data Mining
IPAM. January 14–18, 2002.

from the right compromise between specificity and
distinctiveness.

Concluding Remarks and Future Works
In this work we have adopted an ‘artifact approach’ for
modelling and understanding a cognitive skill of human
mind, according to well rooted perspective of the ‘robot
approach’ (Hull, C.L. 1943) and mechanistic explanation of
intelligent behaviour (Cordeschi, R. 2002).
We have modeled the syndrome discovery process as an
adaptive prototype-based categorization.
This assumption has been evaluated through the definition
of a domain specific index for cluster validation that
measures the effectiveness of the prototype-syndrome
founds for addressing etiological investigations.
We have provided an evolutionary computational
cognitive model of the prototypes theory of categorization
able to reproduce several levels of categorizations
spontaneously.
This model shows a possible explanation of the
emergency of the categorization levels as trade-off between
specificity and distinctiveness. Also, basic level is linked to
a point of equilibrium between the aetiological utility of the
prototypes and the principle of parsimony.
Moreover, our model, based on a genetic algorithm,
implements an evolutionary machine learning model for an
adaptive human learning, so according to E. Mach and his
biological view of knowledge (Pléh, C. 1997) we can argue a
possible algorithmic uniformity between adaptive individual
learning and evolutionary learning by species selection.
Future research will need conducted to deeply analyse and
model the interplay of trade-off between specificity and
distinctiveness with epistemic adequacy of prototypes in
clinical field and other ones.

Acknowledgments
I wish to thank Prof. V. Cordeschi for the improving
comments provided on an earlier version of this paper.

References
Blake, C.L., & Merz, C.J. (1998). UCI Repository of
machine
learning
databases
[http://www.ics.uci.edu/~mlearn/MLRepository.html].
Department of Information and Computer Science,
University of California. Irvine, CA.
Cordeschi R. (2002). The Discovery of the Artificial.
Behavior, Mind and Machines Before and Beyond
Cybernetics. Dordrecht: Kluwer Academic Publishers.
De Falco, I., Della Cioppa, A., Gagliardi, F., & Tarantino,
E. (2005). A New Variable–Length Genome Genetic
Algorithm for Data Clustering in Semeiotics. Accepted
paper to The 20 th ACM Symposium on Applied
Computing. March 13-17, 2005, Santa Fe, New Mexico.
Goldberg, D.E. (1989). Genetic algorithms in search
optimization and machine learning. Reading, MA:
Addison–Wesley.

737

