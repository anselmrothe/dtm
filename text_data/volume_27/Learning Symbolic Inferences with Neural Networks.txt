UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Learning Symbolic Inferences with Neural Networks
Permalink
https://escholarship.org/uc/item/5109x58n
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 27(27)
Authors
Gust, Helmar
Hagmayer, York
Kuhnberger, Kai-Uwe
et al.
Publication Date
2005-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

                             Learning Symbolic Inferences with Neural Networks
                                                         Helmar Gust (hgust@uos.de)
                                            Institute of Cognitive Science, University of Osnabrück
                                                 Katharinenstr. 24, 49069 Osnabrück, Germany
                                                Kai-Uwe Kühnberger (kkuehnbe@uos.de)
                                            Institute of Cognitive Science, University of Osnabrück
                                                 Katharinenstr. 24, 49069 Osnabrück, Germany
                                Abstract                                       well-known that classical logical connectives like conjunc-
                                                                               tion, disjunction, or negation can be represented by neural
   In this paper, we will present a theory of representing sym-                networks (Rojas, 1996). Furthermore it is known that every
   bolic inferences of first-order logic with neural networks. The             Boolean function can be learned by a neural network (Stein-
   approach transfers first-order logical formulas into a variable-
   free representation (in a topos) that can be used to generate               bach & Kohut, 2002). Although it is therefore straightfor-
   homogeneous equations functioning as input data for a neural                ward to represent propositional logic with neural networks,
   network. An evaluation of the results will be presented and                 this is not true for FOL. The corresponding problem, usually
   some cognitive implications will be discussed.                              called the variable-binding problem, is caused by the usage
   Keywords: Neural Networks; First-Order Inferences; Neural-                  of quantifiers ∀ and ∃, which are binding variables that occur
   Symbolic Logic.                                                             at different positions in one and the same formula. It is there-
                                                                               fore no surprise that there are a number of attempts to solve
                            Introduction                                       this problem of neural networks: Examples for such attempts
                                                                               are sign propagation (Lange & Dyer, 1989), dynamic localist
The syntactic structure of formulas of classical first-order                   representations (Barnden, 1989), tensor product representa-
logic (FOL) is recursively defined. Therefore it is possible                   tions (Smolensky, 1990), or holographic reduced representa-
to construct new formulas using given ones by applying a                       tions (Plate, 1994). Unfortunately these accounts have certain
recursion principle. Similarly semantic values of (complex)                    non-trivial side-effects. Whereas sign propagation as well as
formulas can be computed by the interpretation of the corre-                   dynamic localist representations lack the ability of learning,
sponding parts (Hodges, 1997). Consider, for example, the                      the tensor product representation results in an exponentially
following formula:                                                             increasing number of elements to represent variable bindings,
                    ∀x : human(x) → mortal(x)                                  only to mention some of the problems.
                                                                                  With respect to the inference problem of connectionist net-
The semantics of the complex formula is based on the seman-                    works the number of proposed solutions is rather small and
tics of the subexpressions human(x), mortal(x), and the impli-                 relatively new. An attempt is Hitzler, Hölldobler & Seda
cation → connecting these subexpressions. Clearly a problem                    (2004) in which a logical deduction operator is approximated
arises because of the presence of the quantifier ∀ and the vari-               by a neural network and the fixpoint of such an operator pro-
able x. Nevertheless it is assumed that a compositionality                     vides the semantics of a logical theory. Another approach
principle allows to compute the meaning of the complex for-                    is Healy & Caudell (2004) where category theoretic meth-
mula using the meaning of the corresponding subformulas. 1                     ods are assigned to neural constructions. In D’Avila Garcez,
   On the other side, it is assumed that neural networks are                   Broda & Gabbay (2002), tractable fragments of predicate
non-compositional on a principal basis making it difficult to                  logic are learned by connectionist networks. Finally in Gust
represent complex data structures like lists, trees, tables, for-              & Kühnberger (2004), a procedure is given how to trans-
mulas etc. Two aspects can be distinguished: The represen-                     late predicate logic into variable-free logic that can be used
tation problem (Barnden, 1989) and the inference problem                       as input for a neural network. To the knowledge of the au-
(Shastri & Ajjanagadde, 1990). The first problem states that,                  thors, the latter account is the only one that does not require
if at all, complex data structures can only implicitly be used                 hard-wired networks designed for modeling a particular the-
and the representation of structured objects is a non-trivial                  ory. Rather one network topology can be used for arbitrary
challenge for connectionist networks. The second problem                       first-order theories. We will apply the account presented in
tries to model inferences of logical systems with neural ac-                   Gust & Kühnberger (2004) to model first-order inferences of
counts. In this paper, our primary aim is the second problem.                  neural networks and to discuss issues relevant for cognitive
   A certain endeavor has been invested to solve the repre-                    science.
sentation problem as well as the inference problem. It is
                                                                                  The paper has the following structure: First, we will sketch
    1
      In classical logic, variables are not only used to express quantifi-     the basic ideas of variable-free first-order logic using a rep-
cation but also to syntactically mark multiple occurrences of terms.           resentation of FOL induced by category-theoretic means in a
Variable management is usually considered as a problematic issue in
logic. In particular, the problem arises that algorithms have certain          topos. Second, we will present the general architecture of the
difficulties with quantified variables: The non-decidability of FOL            system, the structure of the neural network to code variable-
is a direct consequence of this fact.                                          free logic, and the neural modeling of inferences processes.
                                                                           875

Fourth, we will discuss and evaluate in-depth an example of                 LOGIC                 TOPOS            f ◦g = h                NNs
how logical inferences can be learned by a neural network.
                                                                            Input:                The              Equations               Learning:
Last but not least, we will relate this to general issues in cog-           A set of                               in normal
                                                                                                  input is                                 achieved
nitive science and we will add some concluding remarks.                     logical
                                                                            formulas         ⇒ translated
                                                                                                  into a set
                                                                                                                ⇒ form
                                                                                                                   identi-
                                                                                                                           are
                                                                                                                                    ⇒ by   imizing
                                                                                                                                                   min-
                   Logic Without Variables                                  given in          6 of objects       6 fying              6 distances
                                                                            a logical             and              arrows in               between
In order to circumvent problems having to do with the un-                   language              arrows           the topos               arrows
decidability of FOL a certain endeavor was invested to de-
velop a theory of variable-free logic. 2 A prominent approach                     Is done by hand but
                                                                                                            PROLOG pro-
                                                                                                                           The equations generated by the
                                                                                  could easily be done                     PROLOG program are used as
to achieve this is the usage of a topos (Goldblatt, 1984). In-                    by a program
                                                                                                            gram
                                                                                                                           input for the neural network
tuitively a topos is a category that has all limits (for exam-
ple products), that allows exponents, and that has a subobject                    Figure 1: The general architecture of the account.
classifier. The properties of a topos induce a semantics on
the logical constructions. This semantics is equivalent to the
standard semantics of FOL (Goldblatt, 1984; Gust, 2000), i.e.             work, we developed a simple topos language L T to code the
the full expressive power of FOL is available in a topos. It is           definitions of objects and arrows in a way such that they can
straightforward to translate FOL formulas into objects and ar-            be processed by the program components. The idea is that
rows in a topos.                                                          a given arrow f can be used to generate new equations like
   We give a prototypical example of a category that satisfies            id ◦ f = f , f ◦ id = f and so on. Last but not least, these
the properties of a topos, namely the category SET. The                   equations are used to train a neural network. The structure of
objects of SET are sets, connected by set theoretic functions             the used neural network will be described below.
(called arrows). A product a × b can simply be identified                     The motivation of the proposed solution is based on the
with the Cartesian products of sets a and b, and an exponent              idea that we need to transform an interpretation function I
ab with the set of functions f : b → a.                                   of classical logic into a function I  : Rm → Rn in order to
   In category theory, constructions like products allow the              make it appropriate as input for a neural network. The first
construction of new arrows. For example, in the case of a                 step to achieve this can loosely be associated with reification
Cartesian product a × b the following condition holds: if ar-             and elimination of variables, both standard techniques com-
rows f : c → a and g : c → b are given, then there exists a               monly used in AI: Formulas of first-order predicate logic are
unique arrow h : c → a × b such that the corresponding di-                interpreted as objects and arrows in a topos. The second step
agram commutes. We will use the possibility of constructing               is motivated by the challenge to represent logical formulas as
new arrows – provided some other arrows are given – in the                equations and finally to represent formulas as equations in a
account presented in this paper. The object ! in SET (called              real-valued vector space. In the last step, a necessary issue is
the terminal object) is the one-element set {0} with the prop-            to hard-wire certain principles like the one that true and false
erty that for all sets a there is exactly one arrow from a into           is maximally distinct.
{0}. The truth value object Ω = {0, 1} and the subobject                      Figure 2 depicts the structure of a neural network that is
classifier true: ! → Ω mapping 0 to 1 generalizes characteris-            used in order to model the composition process of evaluating
tic functions and therefore interpretations of predicates. Log-           terms and formulas. Each object of the topos is represented
ical terms can be interpreted as mappings from the terminal               as a point in an n-dimensional real-valued unit cube. In the
object into the universe U , and logical 2-ary connectives as             example used in this paper 3 , we chose n = 5. Each arrow in
mappings Ω × Ω → Ω. Quantified formulas correspond to an                  the topos is again represented as a point in the n-dimensional
operation mapping (complex) predicates to (complex) predi-                real-valued unit cube together with pointers to the respective
cates. The topos account allows the reduction of all logical              domain and codomain. The input of the network is repre-
connectives to one uniform operation, namely concatenation                sented by weights from the initial node with activation 1. This
of arrows.                                                                allows the backpropagation of the errors into the representa-
                                                                          tion of the inputs of the network. The input of the network
               Neural Learning of Formulas                                represents the two arrows to be composed by the following
The Architecture of the System                                            parts:
Figure 1 depicts the general architecture of the presented ac-            • The domain of the first arrow
count in four steps: First, input data is given by a set of logi-         • The representation of the first arrow
cal formulas (determining a partial theory) relative to a given
logical language L. The language L is considered to be a                  • The codomain of the first arrow which must be equal to the
classical first-order language. Second, this set of formulas is               domain of the second arrow
translated into objects and arrows of a topos based on the fact           • The representation of the second arrow
that FOL can be represented by a topos. Third, a PROLOG
program is generating equations in normal form f ◦ g = h                  • The codomain of the second arrow
identifying new arrows in the topos. It order to make this                     3
                                                                                 The choice of n depends on the number of objects and arrows
                                                                          which need to be represented. If n is too large, then overfitting of the
    2
      In this section, we only sketch the idea of representing FOL in     network can occur. If n is too small, the network may not converge.
a variable-free logic. A detailed development can be found in Gust        Currently we do not know the precise correlation between the choice
& Kühnberger, 2004.                                                      of n and the relative size of the logical theory.
                                                                      876

                first layer: 5*n hidden layer: 2*n output layer: n
                                                                                        Table 2: The specification of L T
                                                                                    LT                    Intended Interpretation
             dom1                                                                    !                       Terminal object !
                                                                                     @                     Truth value objects Ω
                a1                                                                    u                       The universe U
                                                         a2 ◦ a1
      1    cod1=dom2                                                                 t                        Truth value true
                a2
                                                                                     f                       Truth value f alse
                                                                                  Y x Z                 Product object of Y and Z
            cod2                                                                  y x z                 Product arrow of y and z
                                                                                   ! X                     Terminal arrow of X
Figure 2: The structure of the neural network that learns com-                x: Y --> Z                  Definition of an arrow
position of first-order formulas.                                                 y o z               Composition of arrows x and y
   These requirements lead to a net with 5 · n many input
values (first layer in Figure 2). The output of the network is         constants not only Socrates but also robot and something are
the representation of the composed arrow. In the example, we           introduced with the additional information that robot is not
use 2 · n many nodes for the hidden layer.                             human. There is no knowledge about something available.
   In order to enable the system to learn logical inferences,          These types of information are represented by equations. For
some basic arrows have static (fixed) representations. These           example, the composition of the arrow human and socrates
representations correspond directly to truth values.                   is resulting in true representing that ”Socrates is human”.
                                                                       Slightly more difficult is the representation of universally
• The truth value true : (1.0, 1.0, 1.0, 1.0, 1.0)                     quantified expressions like ∀x : human(x) → mortal(x).
• The truth value false : (0.0, 0.0, 0.0, 0.0, 0.0)                    The equation is constructed as follows: 4
   Notice that the truth value true and the truth value false are
maximally distinct. All other objects and arrows are initial-                       ∀(⇒ ◦ human × mortal ◦ d) = true
ized with random values. The defining equations of the theory             This is equivalent to
and the equations generated by categorical constructions (like
products) are used to train the neural network.                                     ⇒ ◦ human × mortal ◦ d = true ◦ !
   The following table is an example how certain logical prop-
erties of objects and arrows of the topos can be coded in L T .           The diagonal arrow d : U → U × U is composed with
                                                                       the arrows for predicates human : Ω → Ω and mortal :
                                                                       Ω → Ω and the arrow for the implication ⇒: Ω × Ω → Ω.
        Table 1: Example code of the objects and arrows                Notice further that the construction is only possible because a
      !.                         # the terminal object                 topos guarantees the existence of the relevant arrows. Finally,
      @.
      ! x ! = !.
                                 # the truthvalue object               test equations are represented in Table 3. They correspond to
      u.
      static t: ! --> @,
                                 #
                                 #
                                    the universe
                                    true
                                                                       the logically possible combinations of mortal, human, and
      static f: ! --> @.
      not: @          --> @,
                                 #
                                 #
                                    false
                                    negation
                                                                       angel on the one hand and Socrates, robot, and something
      ->: @ x @ --> @.           #  implication                        on the other. All combinations can be either true or false.
      not o t = f,
      not o f = t,
      -> o t x t = t,
      -> o t x f = f,                                                  The Results
      -> o f x t = t,
      -> o f x f = t.                                                  The input generated by the Prolog program is feeded into the
                                                                       neural network. The result of an example run is then given
   In Table 1, elementary logical operators and equations are          by the errors of the test equations. These test equations query
defined specifying the behavior of classical connectives. The          whether the composition of angel and robot is false, whether
coding of topos entities in L T is straightforward: In the first       the composition of angel and robot is true, whether the com-
part we define objects and arrows and in the second part               position of angel and socrates is false etc. The results of test
we specify the defining equations. Table 2 summarizes the              run of our example is depicted below:
important constructions. Furthermore L T provides a macro
                                                                          Tests:
mechanism to allow a compact coding for complex equations                 angel o robot           = f           0.636045
                                                                          angel o robot           = t           0.886353
(cf. the definition of ==> in Table 3). All derived objects and           angel o socrates        = f           2.197811
                                                                          angel o socrates        = t           0.011343
arrows, e.g. identities and products, are recognized by the               angel o something       = f           0.053576
PROLOG program and the corresponding defining equations                   angel o something
                                                                          heaven o robot
                                                                                                  =
                                                                                                  =
                                                                                                    t
                                                                                                    f
                                                                                                                2.017303
                                                                                                                0.454501
are automatically generated.                                              heaven o robot
                                                                          heaven o socrates
                                                                                                  =
                                                                                                  =
                                                                                                    t
                                                                                                    f
                                                                                                                1.080864
                                                                                                                2.153396
                                                                          heaven o socrates       = t           0.013502
Example                                                                   heaven o something      = f           0.034890
                                                                          heaven o something      = t           2.111497
                                                                          mortal o socrates       = f           1.985289
In Table 3, an extension of the premises of the classical                 mortal o socrates       = t           0.030935
                                                                          mortal o robot          = f           0.195687
Socrates syllogism is modeled. The represented information
is not only that all humans are mortal but also that all mor-              4
                                                                             Notice that the following expressions are expressions in the
tals ascend to heaven and everyone in heaven is an angel. As           topos and not logical expressions.
                                                                   877

Table 3: Example code of an extension of the famous                   Socrates is classified as an angel using the transitivity of the
”Socrates inference”                                                  implication, the robot is classified as a non-angel. Clearly the
                                                                      input only specifies that the robot is not human. It does not
   # predicates of the theory
      human, mortal, heaven, angel: u ---> @ .                        follow logically that the robot cannot be an angel. The result
      X ==> Y: -> o X x Y o d u = t o ! u .
      human ==> mortal.                                               of the weight distribution of the neural network with respect
      mortal ==> heaven.
      heaven ==> angel.                                               to the robot can be interpreted as a support of something sim-
   #individuals                                                       ilar to a closed-world assumption. It is interesting that the
   distinctive
      socrates, robot, something: ! ---> u.                           interpretation of something (cf. Figure 5) differs from the in-
      human o socrates = t.
      human o robot      = f.                                         terpretation of robot, because with respect to something there
   # test the learned inferences
   tests
      mortal o something = t,                                         is no clear tendency how to classify this object.
      mortal o something = f,
      mortal o robot        = t,
                                                                         The models approximated by the network behave as ex-
      mortal o robot
      mortal o socrates = t,
                            = f,                                      pected: Test equations which are logically derivable true or
      mortal o socrates = f,
      heaven o something = t,
                                                                      false will be mapped in all models to t or f respectively. Those
      heaven o something = f,                                         equations for which no logical deduction of the truth value is
      heaven o socrates = t,
      heaven o socrates = f,                                          possible, are more or less arbitrarily distributed between f
      heaven o robot        = t,
      heaven o robot        = f,                                      and t in the set of models. Nevertheless, the models seem
      angel o something = t,
      angel o something = f,                                          to tend to realize a closed world interpretation, i.e. the truth
      angel o socrates = t,                                           value of mortal(robot), heaven(robot), and angel(robot)
      angel o socrates = f,
      angel o robot         = t,                                      tend to be false.
      angel o robot         = f.
   mortal o robot
   mortal o something
                          = t
                          = f
                                        1.488895
                                        0.025812
                                                                               Consequences for Cognitive Science
   mortal o something     = t           2.159551                      The translation of first-order formulas into training data of a
                                                                      neural network allows, in principal, to represent models of
   The system convincingly learned that Socrates is mortal,
                                                                      symbolic theories in artificial intelligence and cognitive sci-
ascended to heaven, and is an angel. Furthermore it learned
                                                                      ence (that are based on FOL) with neural networks. 5 In other
that the negations of these consequences are false. In other
                                                                      words the account provides a recipe – and not just a general
words, the system learned the transitivity of the implication
                                                                      statement of the possibility – of how to learn models of the-
in universally quantified formulas. With respect to robot the
                                                                      ories based on FOL with neural networks. Notice that the
system evaluates with a high certainty that robot is not mor-
                                                                      presented approach tries to combine the advantages of con-
tal. The other two properties are undecided. In the case of
                                                                      nectionist networks and logical systems: Instead of represent-
something relatively certain knowledge for the system is that
                                                                      ing symbols like constants or predicates using single neurons,
something is neither in heaven nor mortal nor an angel.
                                                                      the representation is rather distributed realizing the very idea
   We will have a closer look on how the neural network in-
                                                                      of distributed computation in neural networks. Furthermore
terprets queries. In the left diagram of Figure 3, the max-
                                                                      the neural network can be trained quite efficiently to learn a
imal error of the neural network of 10 runs with 1.6 · 10 6
                                                                      model without any hardcoded devices. The result is a distrib-
many iterations is depicted. The curves show four character-
                                                                      uted representation of a symbolic system.
istic phases: in the first phase (up to 50,000 iterations), the
                                                                         From a dual perspective, it would be desirable to find a
randomly chosen representations for the input arrows and ob-
                                                                      possibility to translate the distribution of weights in a trained
jects remains relatively stable. During the second phase (be-
                                                                      neural network back to the symbolic level. The symbol
tween 50,000 and approx. 200,000 iterations) the maximal
                                                                      grounding problem could then be analyzed in detail by trans-
error dramatically increases due to the rearrangement of the
                                                                      lating the representation levels into each other. Although we
input representations. In the third phase (between 200,000
                                                                      cannot give a detailed account for such a translation from the
and 600,000 iterations) the maximal error rapidly decreases
                                                                      neural level to the symbolic one in this paper, we think that
which is again connected with the reorganization of the input
                                                                      certain invariants on the neural correlate could be used for
representation. In most cases, the maximal error decreases in
                                                                      such a translation to the symbolic level. The development of
the fourth phase (between 600,000 and above iterations), but
                                                                      such a theory will be dedicated to another paper.
the input representations stay relatively stable.
   The right diagram of Figure 3 shows the stability behav-              A logical theory consists of axioms specifying facts and
ior of the neural network (again 10 runs with 1.6 · 10 6 many         rules about a certain domain together with a calculus deter-
iterations). In a first phase (up to 100,000 iterations), the in-     mining the “correct” inferences that can be drawn from these
stability of the weights increases dramatically. In a band be-        axioms. From a computational point of view this generates
tween approx. 100,000 and approx. 250,000 iterations the              quite often problems, because inferences can be rather re-
stability of the network increases and the network remains            source consuming. Modeling logical inferences with neural
(relatively) stable in the third phase between approx. 400,000        networks as presented in this paper allows a very efficient way
iterations and above. Interesting are certain fluctuations (of        of drawing inferences, simply because the interpretation of
certain runs) of the stability behavior between, for example,         possible expressions is “just there”, namely implicitly coded
around 800,000 iterations or for other runs between 1, 2 · 10 6       by the distribution of the weights of the network. Notice that
and 1, 6 · 106 many iterations.                                       the neural network does not only learn the input, but a model
   The two diagrams in Figure 4 show the behavior of                      5
                                                                            Notice that a large part of theories in artificial intelligence are
Socrates is an angel (left diagram) and The robot is an angel         formulated with tools taken from logic and are mostly based on FOL
(right diagram). The classification is as expected. Whereas           or subsystems of FOL.
                                                                  878

 2.5                                                                                      1.4
                                                         "z1.dat" using 1:3                                                                    "z1.dat" using 1:5
                                                         "z2.dat" using 1:3                                                                    "z2.dat" using 1:5
                                                         "z3.dat" using 1:3                                                                    "z3.dat" using 1:5
                                                         "z4.dat" using 1:3                                                                    "z4.dat" using 1:5
                                                         "z5.dat" using 1:3               1.2                                                  "z5.dat" using 1:5
                                                         "z6.dat" using 1:3                                                                    "z6.dat" using 1:5
   2                                                     "z7.dat" using 1:3                                                                    "z7.dat" using 1:5
                                                         "z8.dat" using 1:3                                                                    "z8.dat" using 1:5
                                                         "z9.dat" using 1:3                 1                                                  "z9.dat" using 1:5
                                                       "z10.dat" using 1:3                                                                   "z10.dat" using 1:5
 1.5
                                                                                          0.8
                                                                                          0.6
   1
                                                                                          0.4
 0.5
                                                                                          0.2
   0                                                                                        0
     0   200000  400000 600000 800000   1e+06 1.2e+06      1.4e+06    1.6e+06 1.8e+06         0   200000     400000 600000 800000 1e+06      1.2e+06       1.4e+06 1.6e+06
Figure 3: Stability and the maximal error of the neural network with respect to the extended Socrates example (10 runs with
1.6 · 106 iterations). The left diagram depicts the maximal error of the neural network. The right diagram shows the stability
behavior of the network with respect to the 10 runs.
   3                                                                                        3
                                                ’xrs.dat’ index 1 using 2:3                                                           ’xrs.dat’ index 0 using 2:3
                                                ’xrs.dat’ index 4 using 2:3                                                           ’xrs.dat’ index 3 using 2:3
                                                ’xrs.dat’ index 6 using 2:3                                                           ’xrs.dat’ index 7 using 2:3
 2.5                                                                                      2.5
   2                                                                                        2
 1.5                                                                                      1.5
   1                                                                                        1
 0.5                                                                                      0.5
   0                                                                                        0
       0        0.5         1         1.5          2               2.5           3              0        0.5         1        1.5        2               2.5          3
Figure 4: The distribution of the interpretation of Socrates is an angel (left diagram) and The robot is an angel (right diagram)
with respect to the extended Socrates example (10 runs with 1.6 · 10 6 iterations). The left diagram shows that Socrates is quite
stably classified as an angel, whereas the robot is quite stably classified as a non-angel.
   3
                                                ’xrs.dat’ index 2 using 2:3
                                                ’xrs.dat’ index 5 using 2:3
                                                ’xrs.dat’ index 8 using 2:3
 2.5
   2
 1.5
   1
 0.5
   0
       0        0.5         1         1.5          2               2.5           3
Figure 5: The distribution of the classification of Something is an angel with respect to the extended Socrates example (10 runs
with 1.6 · 106 iterations). The result shows that the distribution is quite equally distributed.
                                                                                      879

making the input true. In a certain sense these models are                                     References
overdetermined, i.e. they assign truth values even in those         Barnden, J.A. (1989). Neural net implementation of complex
cases which are not determined by the theory. Nevertheless            symbol processing in a mental model approach to syllo-
they are consistent with the theory. There is evidence from           gistic reasoning. In Proceedings of the International Joint
the famous Wason selection-task that human behavior is (in            Conference on Artificial Intelligence, 568-573.
our terminology) rather model-based than theory-based, i.e.
human behavior can be deductive without having an infer-            D’Avila Garcez, A., Broda, K. & Gabbay, D. (2002). Neural-
ence mechanism (cf. Gardner, 1989; Johnson-Laird, 1983).              Symbolic Learning Systems: Foundations and Applica-
We can give an explanation of this phenomenon: Humans act             tions. Springer-Verlag.
mostly according to a model they have (about, for example,          Gardner, H. (1989). Dem Denken auf der Spur. Kohlhammer.
a situation) and not according to a theory plus an inference        Goldblatt, R. (1984). Topoi, the categorial analysis of logic.
mechanism. The tendency of our models towards a closed-               Studies in Logic and the Foundations of Mathematics.
world assumption provide hints for an explanation of the em-          North-Holland, Amsterdam.
pirical observations, because – as can be seen in the robot
case – the property of the robot to be non-human propagates         Gust, H. (2000). Quantificational Operators and their Inter-
to the property to be a non-angel. This provides evidence for         pretation as Higher Order Operators. M. Böttner & W:
an equivalence between The robot is human and The robot is            Thümmel, Variable-free Semantics, 132-161, Osnabrück.
an angel in certain types of underdetermined situations.            Gust, H. & Kühnberger, K.-U. (2004). Cloning Compo-
   Taking into account time limitations, for example, in real-        sition and Logical Inferences in Neural Networks Using
world applications, the usage of a trained neural network in a        Variable-Free Logic, Papers from the 2004 AAAI Fall Sym-
complex system would significantly facilitate the application,        posium, Technical Report FS-04-03, pp. 25-30.
because there are no inference steps that need to be computed.      Healy, M. & Caudell, T. (2004). Neural Networks, Knowl-
Furthermore the module can deal with noisy and uncertain              edge and Cognition: A Mathematical Semantic Model
input data which are standardly considered as a problem for           Based upon Category Theory, University of New Mexico,
applications in a rapidly changing environment. Clearly un-           EECE-TR-04-020.
certain data cannot be assigned a definite truth value, but in
all cases the network will generate a certain value that can be     Hitzler, P., Hölldobler, S. & Seda, A. (2004). Logic programs
used by the system. A possibility to make the correspondence          and connectionist networks. Journal of Applied Logic,
between the neural correlate on the one side and symbolic ap-         2(3):245-272.
proaches under uncertainty on the other more obvious is the         Hodges, W. (1997). A Shorter Model Theory. Cambridge
introduction of a truth value n (neither true nor false) and a        University Press.
truth value b (both true and false). Such an introduction is        Johnson-Laird, P. (1983). Mental Modes: Towards a Cog-
straightforward on the neural network side and corresponds            nitive Science of Language, Inference, and Consciousness,
nicely to well-known accounts of many-valued logic in sym-            Cambridge, Mass.
bolic systems (Urquhart, 2001).
                                                                    Lange, T. & Dyer, M.G. (1989). High-level inferencing in a
                    Concluding Remarks                                connectionist network. Technical report UCLA-AI-89-12.
                                                                    Plate, T. (1994). Distributed Representations and Nested
In this paper, we presented a framework for modeling FOL,
                                                                      Compositional Structure.         PhD thesis, University of
in particular, its inference mechanisms with neural networks.
                                                                      Toronto.
We sketched a theory of uniformly translating axiomatic sys-
tems into a certain type of variable-free logic, which can be       Rojas, R. (1996). Neural Networks – A Systematic Introduc-
used for generating equations in normal form. These equa-             tion. Springer, Berlin, New York.
tions are further transformed into an input for a neural net        Shastri, L. & Ajjanagadde, V. (1990). From simple associ-
that learns the axioms together with logical consequences. A          ations to systematic reasoning: A connectionist represen-
detailed evaluation of the results were given and a discus-           tation of rules, variables and dynamic bindings using tem-
sion of consequences of this framework for cognitive science          poral synchrony. Behavioral and Brain Sciences 16: 417-
was presented. Besides the well-known advantages of stabil-           494.
ity and robustness concerning noisy data, the present account
                                                                    Smolenski, P. (1990). Tensor product variable binding and
allows to learn non-trivial inferences without deduction steps.
                                                                      the representation of symbolic structures in connectionist
   Future work will be concerned with the modeling of a               systems. Artificial Intelligence 46(1–2): 159–216.
three- and four-valued logic on the network. Because the net
topology is independent of the type of the underlying logic,        Steinbach, B. & Kohut, R. (2002). Neural Networks –
this step is straightforward. The resulting models need to            A Model of Boolean Functions. In Steinbach, B. (ed.):
be carefully evaluated. Moreover the integration of trained           Boolean Problems, Proceedings of the 5th International
neural nets into real-world robotic devices will be developed.        Workshop on Boolean Problems, 223-240, Freiberg.
In the case of robotics, it would be necessary to extend the        Urquhart, A. (2001). Basic many-valued logic. In D. Gabbay
framework to allow online learning. But this fits seamlessly          & F. Guenthner (Eds.), Handbook of Philosophical Logic,
into our approach since adding new axioms again does not              2nd ed., vol. 2, Kluwer Acad. Publ., Dordrecht, pp. 249-
change the network topology, contrary to most alternative ac-         295.
counts.
                                                                880

