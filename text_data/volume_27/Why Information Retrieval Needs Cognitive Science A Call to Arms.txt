UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Why Information Retrieval Needs Cognitive Science: A Call to Arms
Permalink
https://escholarship.org/uc/item/9893d3hq
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 27(27)
Authors
Hoenkamp, Eduardo
Hoffrage, Ulrich
Publication Date
2005-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

 Why Information Retrieval Needs Cognitive Science: A Call to Arms.
                                       Eduard Hoenkamp (hoenkamp@acm.org)
                        Nijmegen Institute for Cognition and Information (NICI); Montessorilaan 3
                                             6525 HR Nijmegen, the Netherlands
                           Abstract                               conclusion is perhaps premature. Whichever is the case,
                                                                  the alternative route is gaining attention as worthwhile
   Much of today’s success in Information Retrieval (IR)          to pursue.
   comes from a hard approach: employing blazingly fast           That cognitive science has an influence on IR is clear:
   machines, ever more refined statistics, and increas-
   ingly powerful classification schemes. In recent years,        User evaluation studies and studies in interface design
   however, the hard approach has entered a phase of              are legion. But note that such studies are more an
   diminishing returns.                                           application of the experimental paradigm than an in-
   This paper explores a softer alternative which, we             novative approach to improve the core of IR. Also the
   argue, is still in the phase of increasing returns. As         influence of IR on cognitive science is evident: Most
   the quality of an IR system is ultimately decided by
   its users, the approach starts from how these users            notably, a decade of latent semantic analysis (LSA)
   structure information. Interestingly, for this approach        shows how the data-driven approach within IR has in-
   many useful principles are readily available in the            spired studies of human cognition (see (Dumais, 2003)
   psychological literature.                                      for a comprehensive overview). In this paper we want
   We illustrate the approach with three examples. The
   first applies the cognitive status of ‘complex nominals’       to show an approach different from these two, one
   to improve search results by automatically constructing        where the use of known cognitive principles can lead
   specialized queries. The second shows how the con-             to more effective information retrieval. We think such
   nection between language and imagery at the ‘basic             research is direly needed to complement the hard ap-
   level’ can be used for multimedia retrieval on the World       proach, but the examples we found are very few. A
   Wide Web. The final example employs the notion
   of ’semantic space’ to make retrieval more effective           notable exception is the theory of a ‘cognitive space’
   especially for large scale corpora. In each example the        (Ingwersen, 1999; Gärdenfors, 2000) as alternative to
   results were substantial. The cases we studied illustrate      the document space model of IR (Salton, 1988). It is
   how an approach to information retrieval based on              an example where an old psychological theory, namely
   cognitive principles can lead to significant, immediate,
   and fundamental results. It shows how prolific the             Osgood’s (1957) semantic differential, is elaborated into
   application of cognitive science to the core of IR can be,     an innovative application for IR. It also illustrates the
   and we believe that both disciplines stand to benefit          main point of the present paper: no new cognitive the-
   from this approach.                                            ories need to be developed to innovate IR, as there are
   Keywords: Information retrieval; cognitive approach;
                                                                  enough cognitive principles readily available in the liter-
   increasing returns; case studies.                              ature.
                                                                     The sections that follow are self-contained presenta-
                                                                  tions for each case study. However, for a complete cov-
                       Introduction                               erage of the three cases, the space limitations were too
Browsing through proceedings of the major conferences             tight. As these details can be found in the work we pub-
on information retrieval1 one is confronted with a hard           lished elsewhere, we think we can argue our main point:
approach: ever faster machinery, increasingly refined sta-        how the straightforward application of cognitive science
tistical techniques, algorithms tweaked to specific do-           can lead to new insights and innovation in information
mains, applied linear algebra, and more and more pow-             retrieval.
erful classification methods. At the same time, one can-
not escape the impression that more and more effort is                    The case of Complex Nominals
needed to yield relatively small results, or in economic          Intuitively, words forming noun phrases (NP’s) are a
terms, diminishing returns. This could mean that the IR           richer and more precise representation of meaning than
problem is practically solved, or maybe that the field is         keywords: ‘horse’ and ‘race’ may be related, but ‘horse
ready for a paradigm shift away from the dominant hard            race’ and ‘race horse’ carry more circumscribed meanings
approach. The former will raise the eyebrows of anyone            than the words in isolation. Hence, several researchers
who looks up information on the Internet. The latter              have suggested that using noun phrases instead of key-
    1
      Notably the SIGIR Conference on Research and Devel-         words, may improve retrieval effectiveness. Yet, empiri-
opment in Information Retrieval, the TExt Retrieval Confer-       cal studies using noun phrases as queries did not confirm
ences (TREC), and the Digital Libraries Conference.               this intuition (Croft, 1995).
                                                              965

   The research we will present here isolates a gen-                 three steps (Hoenkamp & de Groot, 2000): (1) Objects
eral, ubiquitous, and productive sub-category of noun                in the Cyc representation are lexicalized via WordNet
phrases for which we found the intuition to hold: the                (Miller, 1995), (2) each Cyc relation is mapped to an
complex nominal (CN). Complex Nominals, such as                      RDP, which together with the previous step produces
nominalizations and noun compounds have been studied                 a deep structure, (3) the permitted transformations are
for centuries. What makes these constituents so special?             applied to the deep structure to yield all the CN ’s that
                                                                     can express the conceptual representation.
A. Findings from linguistics:
• CN s behave grammatically as single nouns,                         Proximity vs. Coherence
• occur (probably) universally over languages,                       Search engines often provide operators to confine the
• evoke a preferred reading,                                         matching of keywords to passages where the keywords
• are correctly used in the basic variety2 .                         are within a certain proximity, e.g. AltaVista’s near
B. Findings from Cognitive Science:                                  operator. We will compare this proximity matching to
• CN s are efficient in communication,                               what we will call coherence matching. Proximity Match-
• are very productive,                                               ing with N1 N2 is like AltaVista’s “N1 near N2 ”. For
• can act as nonce words (used only once).                           example, “picture near book” matches “She bought a
C. Findings from Information Filtering                               picture book of Italian Art”, but also “He had his pic-
• CN s can be generated from a semantic representation,              ture taken carrying a book”. Coherence Matching with
• reduce user interaction,                                           N1 N2 occurs if a passage contains one of: (1) an ex-
• attain a precision3 noun phrases generally don’t.                  act match with N1 N2 , or (2) a match with a syntactic
                                                                     variation, e.g. “picture book” matches “book with pic-
The latter findings come from our information fil-                   tures” as in “She had chosen the book with the rare
tering project Profile (Hoenkamp, Schomaker, van                     pictures” (3) a match which is a semantic variation,
Bommel, Koster, & van der Weide, 1996).                              e.g. “picture book” matches “volume of portraits” as
   The rationale behind using noun phrases as queries is             in “He published a volume of excellent portraits of con-
not only that they are more precise than the keywords in             temporary scientists”. (Examples from actual retrieval
isolation. Another reason is that people use NP’s all the            results.) The syntactic variations are formed through
time to identify referents in the information they want              linguistic transformations applied to the deep structure.
to convey. Since Profile (or any information filter)                 The semantic variations emerge through the variety of
is meant to find information on behalf of the user, the              words in WordNet that express the same word sense.
filter is on its own to construct the query. Hence, it may
have to express concepts for which is does not have a                Finding relevant passages
vocabulary. This is exactly the situation where people               In the example above, proximity matching with “pic-
use complex nominals pervasively. But how to construct               ture book” located “He had his picture taken carrying a
interpretable queries without human intervention?                    book”. Coherence matching removed the irrelevant re-
                                                                     sult for the reasons we discussed. The conjecture, then,
Generating Complex Nominals from a                                   is that coherence matching improves precision.
conceptual representation                                               To compare the two techniques, we had to select a doc-
                                                                     ument collection that would promote reliable and valid
Several researchers (Liddy & Myaeng, 1993; Gardiner,
                                                                     results. An ideal collection would be: (1) a corpus that
Riedl, & Slagle, 1994) have described systems that trans-
                                                                     is grammatically tagged, making the experiment inde-
late conceptual graphs into a query to search the TREC
                                                                     pendent of the power and correctness of a parser, (2)
corpus (using boolean search with proximity). Pro-
                                                                     which contains a substantial portion of spoken language,
file instead uses the Cyc formalism (Lenat, 1995) as
                                                                     so that novel CN ’s are likely to occur, (3) which contains
conceptual representation to generate CN s. Much is
                                                                     a variety of subject areas, to insure generality of the re-
known about the way people process novel CN ’s, see
                                                                     sults. The British National Corpus (BNC) is such a col-
e.g. (Ryder, 1994) and (Adams, 1973) from which we
                                                                     lection; a 100 million word collection of samples of writ-
selected our CN ’s. Our mechanism for generating CN ’s
                                                                     ten and spoken language from a wide range of sources.
from a conceptual representation is based on Levy’s
                                                                     The whole collection has been grammatically tagged. To
(1978) theory about complex nominals. It is a trans-
                                                                     be even more accurate, we used the ‘BNC sampler’, a
formational grammar enriched with so-called ’recover-
                                                                     10% subset of the corpus, where the grammatical tags
able deletable predicates’ (RDP), and a set of meaning
                                                                     have been manually checked, and which consists of about
preserving transformations. The algorithm proceeds in
                                                                     an equal amount of spoken and written texts.
    2
      The ’basic variety’ is a well-structured, remarkably effi-        Adams’ (1973) extensive overview of word formation
cient and simple form of language, that adult second language        distinguishes thirteen classes of CN ’s on the basis of
learners universally develop, when not under the influence of        the semantic relationship between the constituent nouns.
a particular teaching method (Klein & Perdue, 1997)                  We took an example from each class, and we produced
    3
      The dominant evaluation metrics in IR are precision and        an exhaustive set of syntactic and semantic variations.
recall. When documents are retrieved, precision is the pro-
portion of the retrieved documents that are actually relevant,          Next we simulated an existing WWW search engine:
and recall is the proportion of the relevant documents that          We first had the BNC sampler indexed by AltaVista.
are actually retrieved.                                              Then we reverse engineered our proximity matcher until
                                                                 966

                                                                          Given this positive result, we stepped up the scale of the
    Ontology
                                      word-forms
                                                                                                  relevant    irrelevant    average
                                                      word-senses
                                       WordNet                                                    passages     passages    precision
                                                                           proximity matching       633           843         .53
                                                                           coherence matching       541           126         .77
                                                   Noun                   Table 2: Comparison of proximity and coherence match-
                                                   pairs                  ing for the complete BNC. Coherence matching again
          coherence                  proximity
            match                      match                              showed a significant precision gain over proximity match-
                                                                          ing (Wilcoxon, N = 11, one-tailed, p < .01).
                                                     corpus
                                                                          corpus and repeated the experimental procedure for the
                                                                          complete BNC. The results are summarized in table 2.
                                                                          Again a significant increase in precision (p < .01).
Figure 1: Coherence matching for a compound con-
                                                                            This concludes the first case that demonstrates our
cept containing two sub-concepts and a relation between                   main point: that information access can be substantially
them. Sub-concepts are translated via WordNet to                          improved by applying cognitive principles readily avail-
pairs of keywords, thus losing their original relationship.               able in the literature. Onward to the next case.
Proximity matching locates potentially relevant passages
in the corpus. Coherence matching subsequently rein-                       The case of the ‘Basic Level Category’
states the original relationships (dashed arrow), remov-                  When you hear the word “chair”, you may imagine a
ing passages where the relationship does not hold.                        chair. You can mention its parts, indicate its height,
                                                                          perhaps mime how you would sit down in it. Hear
                                                                          the word “kitchen chair” instead, and not much will
it produced the same hits. Using this proximity matcher,                  change. But for the word “furniture” the situation
every CN exemplar we had constructed was matched                          changes radically: no simple image pops up, and there
against each document in the BNC sampler. All match-                      is little to mention in terms of parts or specific motor
ing passages were manually scored by two raters, and                      activity. The level in a hierarchy of concepts at which
marked as relevant if the raters agreed. Next precision                   such sudden proliferation of attributes occurs was
was calculated. This was repeated for all sets of CN ’s.                  investigated by Rosch (Rosch, Mervis, Gray, Johnson, &
The results are in the top row of Table 1. In a second                    Boyes-Braem, 1976), who called this the basic level. Sev-
                                                                          eral of the initial claims about the basic level have been
                         relevant   irrelevant      average               subject to debate (Tversky & Hemenway, 1984; Murphy
                         passages    passages      precision              & Smith, 1982). Uncontested, however, are Rosch’s
 proximity matching         36          152           .43                 observations about the striking connections at the basic
 coherence matching         27          38            .72                 level between imagery and language, to wit:
Table 1: Comparison of proximity and coherence match-                     A. Findings from Imagery:
                                                                          • The basic level appears the most abstract level
ing for the ‘BNC sampler’. Matching was performed
                                                                          for which an image can represent a class as a whole
with exhaustive semantic expansion, and limited stem-                     (Peterson & Graham, 1974),
ming (-ing, -er, etc.). Coherence matching shows a preci-                 • When just the name of an object is mentioned to a
sion gain from .43 to .72, which is significant (Wilcoxon,                subject, about the same attributes are listed as when
N = 13, one-tailed, p < .01).                                             that object is visually present (Rosch et al., 1976).
                                                                          B. Findings from Language studies:
pass, coherence matching was performed over the doc-                      • Many more attributes are listed for words at the basic
uments already found during proximity matching (see                       level than for the super-ordinate, and few additional for
Figure 1). The matching passages were scored by two                       the subordinate (Rosch et al., 1976),
raters, as before, and the results are summarized in the                  • For physical objects and organisms, parts notably pro-
bottom row of Table 1. The significance test: We don’t                    liferate at the basic level (Tversky & Hemenway, 1984),
know the distribution of relevant passage in the BNC                      • When people have to name a picture of an object
sampler per query, so we need a non-parametric test.                      at the subordinate level, they choose the word for the
Further, coherence matching occurs over the results of                    basic level (Rosch et al., 1976).
proximity matching (as it is a second pass), so the ob-
servations are correlated. Hence the Wilcoxon test, a                     As keywords have become a serious limitation when
non-parametric test for correlated paired observations.                   searching non-textual material on the internet, we
The considerable increase in precision is significant at                  hypothesized that the ‘basic level’ might be a way to
the level of p < .01.                                                     derive appropriate keywords for image retrieval.
                                                                    967

Content Retrieval from the Web via the                          a concept is a basic level category, than it has a pointer
basic level                                                     to a stereotypical image. (People tend to choose a fairly
                                                                standard viewpoint when illustrating material).
In the context of content delivery (text as well as im-            The image retrieval prototype proceeded in two stages:
ages), we formulated several conjectures that follow from       a retrieval stage and a selection stage. In the retrieval
the properties of the basic level (cf. the summary in the       stage a user sketches the outline of some aircraft. The
previous section):                                              outline is compared to the stereotype images stored with
• Documents about parts of a basic level category can           the ontology. The concept corresponding to the best
be retrieved by searching for the basic level word,             match is looked up, and its index into WordNet is fol-
• Images of a basic level category can be retrieved by          lowed. The word(s) expressing the word sense are then
searching for the basic level word,                             sent to several search engines. In the selection stage the
• The previous two cases should show notably higher             hits are retrieved, and the images are isolated (extensions
precision for unambiguous basic level words than for pol-       .jpg, .gif etc). These images are compared to the out-
ysemous basic level words.                                      line the user had originally drawn, and the pictures that
   As domain for our study we chose the aircraft do-            match are shown to the user for verification. In our ex-
main. First, its basic level words: airship, airplane,          periments the precision of the retrieved images was low.
balloon, and helicopter are unambiguous, which is con-          However, in contrast to textual documents, this does not
ducive to precision. Second, their underlying concepts          pose a problem to users: To find the relevant documents
can be clearly distinguished through two simple criteria        among e.g. 400 text documents is hard work, to spot the
(for example, ‘flying because lighter than air, not dirigi-     relevant pictures among 400 pictures (for example in a
ble’ distinguishes balloon from the other three). Let us        grid of 20 by 20) is easy.
take a closer look at each of these conjectures.
                                                                Conjecture about polysemy and precision. The
Conjecture about discovering parts of objects.                  third conjecture we made predicts that the good results
The first conjecture expresses the connection between           in the previous sections may not transfer in case of pol-
words and images by combining (1) the evidence                  ysemy, as the precision is inherently much lower. We
that objects are identified by recognizing their parts          compared the aircraft domain with its unambiguous ba-
(Biederman, 1987), and (2) the evidence that parts              sic level words to a domain with highly polysemous basic
are important in distinguishing the basic level per se          level words, namely furniture. We compared balloon and
(Tversky & Hemenway, 1984). To test the conjecture,             helicopter to chair and table as keywords, and retrieved
we set out to find parts of an airplane (the concept)           about 500 documents for each. We isolated the pic-
on WWW via the basic level word ‘airplane’. We in-              tures from the documents, tallied the relevant pictures
formally collected a layman’s aircraft ontology, which          for each and calculated precision (see Table 3). When
contains words such as wing, tail, gear, rudder etc. The
automated search proceeded as follows. A search engine             Basic level      number      number        precision
was queried until about ten thousand documents were                word             of docs    of images         (%)
retrieved that gave a hit on ’airplane’ (the basic level
                                                                   Balloon            436          153            13
word). These documents were indexed, followed by di-
mension reduction (SVD). Then words that loaded most               Helicopter         432          438            28
heavily on the principal axes were clustered. Words that           Chair              500           33             2
loaded higher than a threshold were retained. The algo-            Table              500            7             1
rithm set this threshold such that the words expressing
parts already in the ontology were retained. This way,          Table 3: Precision decreases dramatically with increas-
sixty words remained. We inspected these words by hand          ing polysemy (aircraft vs furniture).
and found that words indicating parts of an airplane were
prominently present. Especially noteworthy is that sev-         we compared the aircraft domain to that of furniture,
eral new parts were discovered that were not present            the average precision fell from about 20% to a meager
in the original ontology, such as ‘aileron’ and ‘elevator’.     2%. This may look disastrous, but the basic level itself
This confirms the first conjecture, namely that parts of        provides a remedy for this lack of precision: Since the
an object can be found by searching for the basic-level         basic level is the highest level for which an image can
word. In addition, it shows an example where a lacuna           be construed, an image to illustrate the super-ordinate
in the ontology was discovered by a fully automated pro-        level must be at the basic level. So instead of the highly
cedure (Hoenkamp, 1998).                                        polysemous words ’chair’ or ’table’ we can use ’furni-
Conjecture about retrieving images. To investi-                 ture’ with its low polysemy to retrieve images at the
gate the second conjecture, we used the observation by          lower level. Indeed, we found (Hoenkamp, Stegeman, &
Rosch et al. (1976) that the outlines of objects within         Schomaker, 1999) that the number of relevant documents
the basic level look alike. In the Profile representation       markedly increased this way.
(our ‘ontology’) concepts can have two special pointers:           This section demonstrated how exploiting cognitive
(1) if a concept occurs as a word sense in WordNet then         properties of the basic level, enabled us to automati-
it has a pointer to that word sense. This way, Word-            cally generate appropriate keywords for image retrieval.
Net was used to translate concepts into keywords. (2) If        Even for the fastest search engines it would have been
                                                            968

impossible to match our example images to the billions          as query model. (And we don’t need invent simplifying
of documents on WWW. So also in the case of multi-              assumptions to approximate the distribution.)
media retrieval the cognitive approach demonstrated an
improvement over the hard approach to IR.                       Where do the priors come from? In our experi-
                                                                ments the prior probabilities are derived from the ‘Hy-
         The case of ‘Semantic Space’                           perspace Analog to Language’ (HAL) representation for
                                                                a corpus (Burgess et al., 1998). The representation is
Our everyday search engines are based on the so-called          computed by sliding a window over the documents and
’vector space model’ (Salton, 1988), in which documents         assigning weights to word pairs, inversely to the distance
are seen as points in a metric space spanned by the terms       from each word to every other in the window. We nor-
(words) in the corpus. In recent years, probabilistic lan-      malize the distances to use as transition probabilities for
guage modeling is gaining recognition as a viable alterna-      Markov chain. Note how the two special properties of
tive. In this model a document is seen as a sample from a       texts have been encoded in the Markov chain: From the
word sequence generated according to some distribution.         underlying semantic dependencies follow the transition
These distributions would assign different probabilities        probabilities, and from the surface structure follows the
to a sequence of query terms. And for a given query, the        ergodicity.
documents can be ranked according to the probabilities
their distributions would assign. So a language model           Validation on a large scale corpus
for IR needs to describe (1) the document model, i.e.           The experiment we did to validate the Markov approach
the distribution over terms, and (2) the query model,           is rather technical, and perhaps requires more than a
i.e. how a probability is assigned given the query terms.       passing familiarity with IR. Yet we would like to show
In the hard approach to IR, a variety of proposals have         how the cognitive science orientation stands up to a cor-
been published regarding the choice for (1) and (2). But        pus the size that has so far only been approached by
we will look at these points from a cognitive standpoint.       the hard branch of IR (about 50 million words). We
Deriving a query model
                                                                  AP89       Baseline   Robertson’s       Stationary
The strength of language models lie in their well-
                                                                  topics                TREC-3            kernel
understood formalisms and mathematical rigor. Their
weakness, however, lies in the additional assumptions             101-150    0.1806     0.2298 (+27%)     0.2594 (+44%)
required to make the formalisms tractable in large scale          151-200    0.2244     0.2386 (+6%)      0.2726 (+22%)
applications. Why some assumptions work well and oth-
ers don’t is not always clear. In our cognitive approach,       Table 4: Average precision for the two re-weighting
we capitalize on two properties of the documents to be
                                                                schemes compared to the baseline
retrieved. One is that texts are samples from a natural
language corpus, hence have surface constraints. The
                                                                conducted an experiment on relevance feedback with re-
other is that texts represent content, hence have under-
                                                                weighting. In this paradigm, a rather crude method is
lying semantic dependencies. To our surprise, we dis-
                                                                used to collect a set of so-called pseudo-relevant docu-
covered that the language models proposed to date have
                                                                ments. Then the weights used for ranking the documents
overlooked either or both properties of the very material
                                                                are re-weighted. We repeated a well-known TREC ex-
they are trying to model.
                                                                periment except that we used our cognitively based cal-
The document model as an ergodic chain. Many                    culation for re-weighting.
cognitive phenomena can be well understood in terms             Material : TREC corpus AP89, and two sets of topics
of word-pairs, cf. the research on memory (Shiffrin &           (101-150, 151-200).
Steyvers, 1998) and on the ’semantic space’ (Burgess,           Procedure: We applied our method to Robertson’s
Livesay, & Lund, 1998). If the probability of a term            (1992) query expansion approach in TREC-3 as fol-
depends only on the preceding term, then one can de-            lows. We took BM25 as baseline, and the 20 top-ranked
fine the distribution as a Markov chain with the terms          documents as the relevant set. First, Robertson’s re-
as states. The subsequent states, then, are sequences of        weighting scheme for Okapi was applied, i.e. queries
terms which form documents. Two observations about              were expanded with a (varying) number of terms chosen
natural language seem so obvious that they are easily           by his term selection value (TSV). Then precision/recall
overlooked: (1) Words can be separated by any number            statistics were obtained. Next, the HAL matrix for the
of intermediate words. Therefore, the Markov chain pro-         relevant set was computed. For each query (1) the sub-
ducing the words is aperiodic, and (2) You can always           matrix was taken containing only the terms of the ex-
get from one word to another by continuing to produce           panded query, (2) this matrix was normalized to get a
text. Consequently, the Markov chain is irreducible. A          probability distribution, and (3) the stationary distri-
Markov chain with both properties is called ergodic, and        bution was computed. This vector represented the ex-
it has the property that in the long run it reaches a sta-      panded query. Finally, the cosine distance to the docu-
tionary distribution, irrespective of the initial state. It     ments determined the new ranking. The procedure was
follows from this observation that if we could know the         repeated for query extension from 10 to 100 terms in
transition probabilities of terms (in a document or cor-        steps of 10.
pus) we could compute the stationary distribution to use        Results. The average precision for the stationary kernel
                                                            969

was a substantial improvement for all query extensions.           Development in Information Retrieval (pp. 385–387).
The overall averages are summarized in Table 1.                   New York: ACM.
                                                                Hoenkamp, E., Schomaker, L., van Bommel, P., Koster,
                       Conclusion                                 C., & van der Weide, T. (1996). Profile - A Proactive
                                                                  Information Filter (Tech. Rep. No. 9602). Computer
We started with the observation that in today’s hard ap-          Science Institute, University of Nijmegen, the Nether-
proach to information retrieval, more and more effort is          lands.
needed to achieve even small improvements. Or, in eco-          Hoenkamp, E., Stegeman, O., & Schomaker, L. (1999).
nomic terms, the approach seems to have reached a point           Supporting content retrieval from WWW via ’basic
of diminishing returns. As a complement to this ap-               level categories’. In M. Hearst, F. Gey, & R. Tong
proach we advocate a cognitive science approach, which            (Eds.), SIGIR ’99: 22nd annual international ACM
is in the phase of increasing returns.                            SIGIR Conference (pp. 311–312). ACM Press, New
   We described three cases in different areas of cognitive       York.
science, and showed that the principles that have been          Ingwersen, P. (1999).Cognitive information retrieval.An-
around in the field for a long time could be applied to           nual Review of Information Science and Technology,
compete with the prevalent hard approach of IR.                   34, 3–52.
   As may have transpired from the case descriptions,           Klein, W., & Perdue, C. (1997). The basic variety or:
even the softer approach requires a substantial program-          Couldn’t natural languages be much simpler? Second
ming effort. The need to combine a thorough knowl-                Language Research, 13 (3), 301–348.
edge of psychology with good programming skills should          Lenat, D. (1995).Cyc: A large-scale investment in knowl-
make this approach particularly suited, attractive, and           edge infrastructure. Communications of the ACM,
opportune for cognitive scientists. This kind of research         38 (11), 33–38.
is much needed in IR. And as information retrieval is           Levi, J. N. (1978).The Syntax and Semantics of Complex
an important pillar of the information society, we intend         Nominals. New York: Academic Press.
this paper also as a call to arms.                              Liddy, E., & Myaeng, S. (1993). Dr-link: A system
   If that call is answered by cognitive scientists, we ex-       update for TREC-2. In Proceedings of the Second
pect the results for information retrieval to be substan-         Text REtrieval Conference (TREC-2) (pp. 85–100).
tial, immediate, and fundamental.                                 Gaithersburg: NIST.
                                                                Miller, G. (1995). Wordnet: A lexical database for en-
                        References                                glish. Communications of the ACM, 38 (11), 39–41.
                                                                Murphy, G., & Smith, E. (1982). Basic-level superiority
Adams, V. (1973). An introduction to Moderm English               in picture categorization. Journal of verbal learning
   word formation: London: Longman.                               and verbal behavior, 21, 1–20.
Biederman, J. (1987).Recognition-by-components. a the-          Osgood, C., Suci, G., & Tannenbaum, P. (1957). The
   ory of image understanding. Psychological Review, 94,          Measurement of Meaning: Urbana: The University of
   115–147.                                                       Illinois Press.
Burgess, C., Livesay, K., & Lund, K. (1998). Explo-             Peterson, M., & Graham, S. (1974).Visual detection and
   rations in context space: Words, sentences, discourse.         visual imagery. Journal of Experimental Psychology,
   Discourse Processes, 25, 211 – 257.                            103, 509–514.
Croft, W. (1995). Effective text retrieval based on com-        Robertson, S. E., Walker, S., Hancock-Beaulieu, M.,
   bining evidence from the corpus and users. IEEE Ex-            Gull, A., & Lau, M. (1992). Okapi at TREC. In Text
   pert, 10 (6), 59–63.                                           REtrieval Conference (pp. 21–30.
Dumais, S. (2003). Data-driven approaches to informa-           Rosch, E., Mervis, C. B., Gray, W. E., Johnson, E. M.,
   tion access. Cognitive Science, 27, 491–524.                   & Boyes-Braem, P. (1976). Basic objects in natural
Gärdenfors, P. (2000).Conceptual Spaces: The Geometry            categories. Cognitive Psychology, 8, 382–439.
   of Thought: Cambridge: MIT Press.                            Ryder, M. E. (1994). Ordered chaos: A Cognitive Model
Gardiner, D., Riedl, J., & Slagle, J. (1994). TREC-3:             for the Interpretation of English Noun-Noun Com-
   Experience with conceptual relations in information            pounds. Ph.D. thesis, University of California, San
   retrieval. In Proceedings of the Third Text REtrieval          Diego.
   Conference (TREC-3) (pp. 333–352). Gaithersburg:             Salton, G. (1988). Automatic text processing: the trans-
   NIST.                                                          formation, analysis and retrieval of information by
Hoenkamp, E. (1998). Spotting ontological lacunae                 computer. Reading, Mass.: Addison-Wesley.
   through spectrum analysis of retrieved documents. In         Shiffrin, R. M., & Steyvers, M. (1998). The effectiveness
   Proceedings of the 15th European Conference on Ar-             of retrieval from memory.In M. Oaksford & N. Chater
   tificial Intelligence ECAI-98. Workshop on Applica-            (Eds.), Rational models of cognition (pp. 73–9–5).
   tions of Ontologies and PSMs (pp. 73–77). Chicester:           Oxford University Press.
   Wiley.                                                       Tversky, B., & Hemenway, K. (1984).Objects, parts and
Hoenkamp, E., & de Groot, R. (2000). Finding relevant             categories. Journal of Experimental Psychology, 113,
   passages using noun-noun components. In M. Hearst,             169–193.
   F. Gey, & R. Tong (Eds.), Proceedings of the 23rd In-
   ternational ACM SIGIR Conference on Research and
                                                            970

