UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Function Learning with an Ensemble of Linear Experts and Off-The-Shelf Category-Learning
Models
Permalink
https://escholarship.org/uc/item/6b5992vr
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 27(27)
Authors
Harris, Harlan D.
Hasson, Uri
Johnson-Laird, P.N.
et al.
Publication Date
2005-01-01
Peer reviewed
  eScholarship.org                                Powered by the California Digital Library
                                                                    University of California

            Function Learning with an Ensemble of Linear Experts and
                              Off-The-Shelf Category-Learning Models
                                   Harlan D. Harris (harlan.harris@uconn.edu)
                                             University of Connecticut at Storrs
                                  Department of Psychology; 406 Babbidge Road, Unit 1020
                                                     Storrs, CT 06269 USA
                                        John Paul Minda (jpminda@uwo.edu)
                                                 University of Western Ontario
                                                   Department of Psychology
                                                 London, ON N6A 5C2 Canada
                            Abstract                                  In experimental investigations of function learning,
                                                                   several properties of the human capacity for function
   The relationship between function learning and other            learning have been discovered. For example, functions
   types of concept acquisition is far from well understood.       that are linear (in an appropriate psychological space)
   Some models of function learning have used approaches
   that are very different from current models of categoriza-      are easier to learn than functions that have curvature,
   tion, while more recent function learning models have           functions that increase are easier to learn than functions
   used exemplar representations, following the categoriza-        that decrease, and extrapolation is less accurate than
   tion literature. This paper describes two new models            interpolation (Busemeyer, Byun, Delosh, and McDaniel
   of function learning that combine well-studied “off-the-        1997).
   shelf” approaches to category learning (ALCOVE and
   SUSTAIN) with recent work in knowledge partitioning.               Several different categories of models have been used
   These models are shown to perform basic function learn-         to explore function learning. Briefly, rule-based models
   ing tasks, to partition knowledge of functions, and to be       perform mathematical regression given the stimuli (Koh
   capable of addressing some individual differences in at-
   tention and generalization.                                     and Meyer 1991), exemplar models use interpolation and
                                                                   extrapolation techniques to generalize over stored exam-
                                                                   ples (Busemeyer, Byun, Delosh, and McDaniel 1997; De-
                       Introduction                                Losh, Busemeyer, and McDaniel 1997; Guigon 2004),
                                                                   and gating models learn a piecewise-linear approxima-
Although most research in concept learning focuses on
                                                                   tion to the function using simple experts and a gating
learning of discrete categories, people also learn func-
                                                                   module (Kalish, Lewandowsky, and Kruschke 2004).
tions of continuous variables. Cognitive tasks such as
estimating how long you can stay in the sun before                    Experimental results have generally not been consis-
you burn, or how much a used car might be worth, re-               tent with the predictions of rule-based models, and they
quire prediction of a quantitative value, given a com-             have generally been left behind. Exemplar models, such
bination of qualitative and quantitative cues. Labora-             as ALM (Busemeyer et al. 1997) and EXAM (DeLosh
tory tasks in the literature include prediction of the rate        et al. 1997), have been more promising, but cannot ac-
of spread of wildfires, given windspeed and slope, and             count for multi-modal patterns of extrapolation results
prediction of the amount of food aliens might require,             seen when multiple functions are learned simultaneously
given size and physical attributes. Formally, function             (Lewandowsky et al. 2002). The POLE (Population
learning is the task of learning a mapping from a multi-           Of Linear Experts) model of function learning (Kalish,
dimensional domain space to a continuous value. For                Lewandowsky, and Kruschke 2004) is an attempt to ad-
the purposes of this paper, we are concerned with the              dress this. POLE is a complex model which uses a large
mapping (C1 , . . . , Cn−1 , Rd ) ⇒ Rr , where the Ci are bi-      number of fixed linear experts, controlled by a gating
nary cue features, and Rd and Rr are the domain and                network. The gating network learns which experts are
range respectively of the function to be learned1 . This           most accurate for particular input domains, then gates
framework can occur when the function to be learned is             the experts in a probabilistic manner. POLE can repli-
partitioned, or separated into a number of subfunctions,           cate the multi-modal patterns, but has significant flaws
each of which is learned separately. For example, one              (summarized in the discussion) that limit the extent to
type of animal may require a lot more food as its size             which its results support its laudable framework.
increases, while another type may require only a little               The work presented here is an effort to improve on
more food. Recent work has shown that both category                some of the basic assumptions of POLE, by using well-
and function learning tasks often involve partitioning of          understood computational models of category learning
knowledge to particular contexts (Lewandowsky, Kalish,             as major components of the model. These models ac-
and Ngang 2002; Lewandowsky and Kirsner 2000).                     count for a wide variety of categorization phenomena,
                                                                   and their use as components of this new work allows it
    1
      Experimental research into cognitive function learning       to be better tied to research regarding attention alloca-
has not yet carefully investigated how and when people learn       tion, exemplar and cluster formation, and other impor-
functions of two or more continuous variables. Extensions of
the models presented here could potentially make interesting       tant aspects of concept and skill acquisition.
predictions about performance on these tasks.                         The remainder of this paper describes two new,
                                                               905

                                                                                               X
                            Output Selection                                    ahid
                                                                                 j   = exp[−c(        αi |hji − Ii |r )q/r ]  (1)
                                                                                                 i
                                                                                                X
                                                                                         Gk =         gkj ahid
                                                                                                           j                  (2)
                                              Gating                                             j
          Expert1     Expert2        Expertk
                                             Network
                               ..                                     Each input Ii is compared to the exemplars, hj i,
                                                                   weighted by each attribute’s attention value, αi , and
                                                                   transformed by an exponential function with parame-
                                                                   ters c, q, andr, giving ahid
                                                                                            j , the activation of each hidden
                                                                   (exemplar) unit. These activations are then transformed
                Input1      Input2 .. Inputn
                                                                   through a weight matrix (gk j) to get G, the activation of
                                                                   the k gating nodes (equivalent to aout     K in ALCOVE). G
                                                                   is then used to compute the ensemble output probability
                                                                   distribution:
Figure 1: AEGLE and SEGLE function learning sys-
tems, block diagram. For AEGLE, the gating network                                                 exp(φGk )
is ALCOVE; for SEGLE, it is SUSTAIN.                                            P (O = Ok ) = P                     = G0k     (3)
                                                                                                    z exp(φG    z )
                                                                      O is the overall ensemble output, and Ok is the output
                                                                   of each of the simple linear experts, computed as:
closely-related algorithms for modeling function learn-                                   Ok = wk I1 − bk                     (4)
ing, based on the concepts underlying POLE and the
mixture-of-experts algorithm from the neural network                  where wk and bk are each expert’s weight and bias.
literature (Jacobs, Jordan, Nowlan, and Hinton 1991)                  When the ensemble is learning, a teaching value is
but combined with well-understood psychological mod-               then used to update the model’s weights. Each expert’s
els of category learning as the gating component. These            error is minimized using a variation on the usual LMS
models account for a number of the effects observed in             rule, where the error is the normal sum-squared error.
function learning experiments, and suggest new ways to             The weight and bias update rules are modulated by the
explore this rather complicated set of behaviors. The              gating values (G0k ), so that an expert that made a large
next two sections introduce these new models, followed             error would be updated only minimally if it was unlikely
by an overview of some basic simulations performed us-             to have been selected. Additionally, the use of momen-
ing the models, and some concluding analysis.                      tum (m) speeds up learning, and an adjustment to the
                                                                   bias update rule slows down learning so as not to over-
                                                                   whelm the weight updates when learning rates are large.
                              AEGLE
                                                                        ∆wk = (∆wk · m) + ηw (T − Ok )G0k I1                  (5)
The first new model is called AEGLE, for ALCOVE-                         ∆bk = (∆bk · m) − ηw (T −        Ok )G0k   mean(I1 ) (6)
based Ensemble of Gated Linear Experts. It uses, as
a core component, the ALCOVE (Attention Learning                      where ηw is the learning rate, T is the training signal,
COVEring map) model of classification (Kruschke 1992).             and mean(I1 ) is the average value of the inputs to the
ALCOVE learns to classify using an exemplar-based rep-             experts.
resentation, with error-driven changes to weights and                 Then, the teaching signal (target vector) for the gating
attention. AEGLE is thus a model of function learn-                network is computed as follows:
ing that uses a well-studied model of classification as its
                                                                                            (|T − Ok | + )θ
gating network. Figure 1 shows its overall architecture.                           Tk0 =                               ,      (7)
                                                                                         maxz [(|T − Oz | + )θ ]
   The experts are simple Least-Mean-Square (LMS) lin-
                                                                      where  is a very small number to prevent division by
ear nodes, receiving only a single real value as input.
                                                                   zero, and θ is a parameter. That is, the target value
The gating network gets all input attributes, and learns
                                                                   is near 1 when the expert made only a small prediction
to predict which experts will perform well for each ex-
                                                                   error, relative to the other experts, but near 0 when the
ample. Both the experts and the gating network learn
                                                                   expert made a large prediction error, relative to the other
in an error-driven manner.
                                                                   experts. The updates to weights and attention are the
   The input is vector I1 , . . . , In , where I1 is a distin-     normal ALCOVE update rules:
guished real-valued feature in [0, 1], used as the input to
the experts, and I2 , . . . , In are boolean context features,
used only as inputs to the gating network.                                            ∆gkj = ηg (Tk0 − Gk )ahid j             (8)
                                                                                    XX
   When a stimulus arrives at the gating network, it is                 ∆αi = ηα       [ (Tk0 − Gk )gkj ]ahid  j c|hji − Ii | (9)
processed exactly as in ALCOVE:                                                      j   k
                                                               906

                                                                                              X
   Note that αi is constrained to be non-negative.                                    Ckout =       wkj Hjout                  (16)
   The initial values of the expert weights are selected                                        j
from a normal distribution. As positive-sloped functions                                       exp(d Ckout )
are easier to learn, that distribution has mean 1 and                        P (O = Ok ) = P               out )
                                                                                                                  = G0k        (17)
standard deviation 2. The biases are initially set to 0.5.                                     z  exp(d  C z
The initial values of the gating network’s weights are as
follows: αi = 1, gkj = 0. Following ALCOVE, we use                   These equations are the same as those shown in (Love
the training examples to set the exemplar nodes, hji .            et al. 2004), except that µ is computed for scalar rather
   Table 1 summarizes the parameters for AEGLE.                   than nominal inputs, and then a running average is com-
   Summary. AEGLE is a mixture-of-experts learning                puted. SUSTAIN’s rule for updating λ, the tuning of
model, using the standard ALCOVE classification model             cluster receptive fields, does not converge for scalar in-
as a gating module, and commonly-used linear LMS                  puts that get arbitrarily close to the cluster prototype
nodes as experts. Like POLE, it does gated piecewise-             (Brad Love, personal communication), and this averag-
linear approximation based on an exemplar representa-             ing allows convergence.
tion of the stimulus space. Unlike POLE, it uses rel-                The teaching signal for the SUSTAIN-based gating
atively standard computation and update rules for the             module is the same vector Ti0 as used for ALCOVE in
experts and for the gating module.                                AEGLE. After computing the teaching signal, the expert
                                                                  weights are updated (as with AEGLE), then the follow-
                          SEGLE                                   ing steps occur to update the gating network: update
                                                                                                                   pos
The second new model is called SEGLE, for SUSTAIN-                the prototype for the winning cluster, Hwinner           ; update
based Ensemble of Gated Linear Experts. It uses                   the attention tuning vector, λ; and update the weights,
the SUSTAIN (Supervised and Unsupervised STratified               wjk . Those steps are notated as follows:
Adaptive Incremental Network) model of classification
(Love, Medin, and Gureckis 2004) in essentially the same                            pos                    pos
framework as AEGLE. SUSTAIN has a similar architec-                             ∆Hi,winner  = ηg (Ii − Hi,winner     )         (18)
ture to ALCOVE, but uses incrementally-created clus-                              ∆λi = ηg e−λi µ̄ij (1 − λi µ̄ij )            (19)
ters as the internal representation of the input, rather
than exemplars. The allocation of attention also differs,                         ∆wjk =   ηg Hjout (Tk −  Ckout )             (20)
as noted below. SEGLE thus extends AEGLE by us-
ing a newer model of categorization, one that builds a               SUSTAIN has two modes for adding new clusters. Ei-
multiple-prototype model of the stimuli rather than us-           ther new clusters can be added when an exemplar is too
ing a large number of arguably implausible exemplars.             dissimilar to existing clusters, or they can be added when
   The SUSTAIN gating network is initialized with a sin-          the error on an exemplar is too high. Here, we use the
gle cluster prototype, the location of the first training ex-     first method, an unsupervised approach. (The second,
emplar. The position of a cluster in the instance space is        supervised method, is too sensitive to instability in the
                                                                                                                       act
given by the vector Hjpos . The weights wkj for the first         gating network’s training signal.) When Hwinner              < τ,
                                                                  a new cluster is added, with center equal to the offend-
cluster are initialized to n1 .                                   ing example, and with gating weights set so that it will
   When making a prediction, the following steps occur:           initially prefer to use experts that are rarely used:
update µ̄j , a vector of the average distances (one per in-
put dimension) of the exemplars to each cluster; compute                                        ( j wjk )−1
                                                                                                  P
Hjact , the activation of each cluster; do winner-take-all                         wnew,k =   P     P         −1
                                                                                                                               (21)
                                                                                                  z(  j wjz )
and compute Hjout , the output of each cluster; compute
Ckout , the activation of each gating unit. Then, as with            The only significant changes to SUSTAIN for its ap-
AEGLE, compute P (O = Oi ) = G0 and select an ex-                 plication to SEGLE are the averaging of µ to allow con-
pert to make a prediction. These steps are expressed              vergence, the weights for new clusters, and the method
algebraically as follows:                                         used to generate a training signal. All other details of
                                                                  processing and learning are identical. See Table 1 for a
                                                                  summary of SEGLE’s parameters.
                                       pos
                      µij = |Ij − Hij      |             (10)        Summary. Like AEGLE, SEGLE is a mixture-of-
                        1 X                                       experts learning model. It uses a variant of the SUS-
               µ̄j = γ(        µij ) + (1 − γ)µ̄j        (11)     TAIN classification model as its gating module, and
                        j i                                       shares the same LMS experts as AEGLE. Unlike AE-
                           Pn
                                 (λ )r e−λi µj                    GLE and POLE, its gating module uses SUSTAIN’s
                    act
                  Hj = i=1     Pn i        r
                                                         (12)     dynamically-created clusters as its internal representa-
                                  i=1 (λi )                       tion instead of exemplars.
                   winner = argmaxHjact                  (13)
                                (H act       )β
                                                                                        Simulations
                     out
                   Hwinner    = P winneract   β
                                                         (14)     Four simulations show that AEGLE and SEGLE can ac-
                                    j (Hj )                       count for a number of properties of function learning
                          out
                        Hnotwinner     =0                (15)     seen in experiments.
                                                              907

                                       Table 1: Parameters for AEGLE and SEGLE
                               AEGLE                                                     SEGLE
                    Param.     Description               Value      Param.               Description              Value
                    ηw         expert learning rate       free      ηw                   expert learning rate      free
                    ηg         gating learning rate       free      ηw                   gating learning rate      free
                    ηα         attention learning rate    free
                    c          specifity                  free      r                    attentional focus        free
                    φ          decision consistency       free      d                    decision consistency     free
                                                                    β                    cluster competition      free
                                                                    τ                    new cluster threshold    free
                    θ          gating target exponent       2       θ                    gating target exponent     2
                    r          distance metric              1       γ                    µ decay constant          0.1
                    q          similarity gradient          1
Simulation 1
Following (Kalish et al. 2004), the parameter space
of SEGLE was explored to confirm that it finds the                                  1       Training
                                                                                            Testing
same functions difficult as people do. Over 20,000
models with parameters selected randomly were taught
seven functions. The free parameters were selected
                                                                        Response
from the following ranges: r ∈ [.1, 4], β ∈ [1, 10], d ∈
[1, 10], ηg ∈ [.01, 1], τ ∈ [.1, 1], ηw ∈ [.01, 1], m ∈ [0, 1].
5 experts were used in all cases. The seven functions,                             0.5
all with domain and range in [0, 1], were (a) random,
y = random; (b) positive linear, y = x; (c) negative lin-
ear, y = 1 − x; (d) monotonic increasing, y = x2 ; (e)
monotonic non-decreasing, y = 12 + 8(x − 21 )3 ; (f) non-
monotonic quadratic, y = 1 − 4( 12 − x)2 ; and (g) cyclic,                          0
y = 12 + .2 sin(20x).                                                                0                                            1
                                                                                                              0.5
   After training on 5 blocks of 60 examples each, the                                                     Stimulus
mean-squared error on the training examples were com-
puted, and used to rank the difficulty of each function.
Experimental work has shown a ranking of b < c < d ∼                    Figure 2: Performance of SEGLE on a replication of the
e ∼ f < g ∼ a. Over the very wide range of param-                       effect seen in Kalish et al. (2004), Exp. 1. Small dark
eters explored, over 21% matched this ordering. All of                  circles are the training examples, while large dark circles
the first six of the principles described by (Busemeyer                 are SEGLE’s responses on the final training block. Small
et al. 1997) were confirmed: 90.6% of the models found                  light circles are the predicted ideal responses to the test
cyclic and random functions most difficult, 94.5% found                 stimuli, and the light diamonds are SEGLE’s responses
positive linear functions easier than negative linear func-             to those stimuli.
tions, 98.6% found monotonic increasing functions eas-
ier than non-monotonic functions, and the linear func-
tion was easier than the monotonic increasing function                  learned the slope of the line better than did the expert
77.0% of the time. All of these patterns are similar to                 used for x < 0.5.
those observed in POLE, although we explored a sig-
nificantly larger area of the parameter space, reducing                 Simulation 3
overall accuracy.                                                       In addition to partitioning based on sub-ranges of the
   These results show that over a fairly wide range of                  input attribute used to make the function prediction,
parameters, SEGLE finds similar sorts of functions dif-                 SEGLE can partition based on an external cue. POLE
ficult, and easy, as do humans learning functions in the                was not explicitly tested on this sort of function, al-
lab.                                                                    though it should be able to learn it, so new stimuli were
                                                                        created to illustrate this ability.
Simulation 2                                                              The stimuli consisted of a binary cue, c ∈ {0, 1}, and a
To account for the sort of knowledge partitioning de-                   domain variable that ranged from 0 to 1. The function to
scribed by (Lewandowsky et al. 2002), POLE was shown                    be learned was y = 12 (1+x2 ) if c = 0, and y = 12 (1−x2 ) if
to extrapolate in a discontinuous manner when there                     c = 1. 33 examples repeated in 10 blocks were presented
was a gap between qualitatively different segments of the               to SEGLE. The parameters, which required almost no
training examples (Kalish et al. 2004, Exp. 1). SEGLE                   tuning from an initial guess, were as follows: r = 1, β =
also easily shows this effect, as shown in Figure 2. Note               10, d = 10, ηw = .15, ηg = .8, m = .8, experts = 8, τ = .5.
the discontinuity at x = 0.5. The expert used for x > 0.5                 The results from a typical run of the simulation are
                                                                  908

           1.2
                                                                                                            Training Error                 CA/FR Ratio 6
                   cue=0, target
             1                                                                                              Criterial Attribute
                   cue=0, SEGLE
                                                                      "Error" from Ideal Responses
                                                                                                     0.25   Family Resemblance
                   cue=1, target                                                                                                                          5
           0.8     cue=1, SEGLE
                                                                                                                                                              CA/FR Attention Ratio
                                                                                                     0.2
                                                                                                                                                          4
Response
           0.6
                                                                                                     0.15
                                                                                                                                                          3
           0.4
                                                                                                      0.1                                                 2
           0.2
                                                                                                     0.05                                                 1
             0
           -0.2                                                                                        0                                                  0
               0    0.2      0.4       0.6     0.8        1                                             0        0.5           1         1.5          2
                                Stimulus                                                                     c (Specificity of Exemplar Activation)
Figure 3: Performance of SEGLE on a cue-partitioned                   Figure 4: Results showing performance of AEGLE on
function. Lines are the two target functions, and the per-            Minda and Ross (2004) cued function-prediction task.
formance on the last of 10 blocks of training are shown.              Lines with symbols and error bars show similarity to
                                                                      “ideal” responses, if only CA or only FR information was
                                                                      used. The CA/FR attention ratio (axis on the right) is
shown in Figure 3. On the last block of training, SEGLE               high if most of the attention is on the CA attribute.
showed close fidelity to the target function, correctly us-
ing the cue to partition the experts (with the exception
of one gating error at x = 0.59). Note that performance
on the downward-sloping part of the c = 1 function was
worse than elsewhere, consistent with the general princi-                The parameters for AEGLE were roughly tuned to
ple that functions with positive slope are easier to learn.           get good performance on the training task, as follows:
                                                                      φ = 4, ηw = .5, m = .1, ηg = .1, ηα = .9, experts = 2, τ =
Simulation 4                                                          2. To illustrate individual differences, c, the specificity
Minda and Ross recently investigated the interactions                 of activation of the stored exemplars in the gating net-
between two simultaneous concept-learning tasks, a cate-              work, was varied from 0.2 to 2. 10 replications of the
gorization task and a function-learning task, sharing the             model (for each value of c) were trained for 10 blocks,
same stimuli (Minda and Ross 2004). For the present                   and their responses on the conflict items were compared
simulation, we consider just the results from a condition             to “ideal” responses based solely on the CA and FR in-
where only the function learning task was performed.                  formation. The results are shown in Figure 4, along with
   The stimuli contained both a criterial attribute (CA)              the (low and relatively constant) training error and a
and a family resemblance (FR) structure of 5 attributes               measure of differential attention. With c near 1.1, re-
(see Table 1 of Minda and Ross, 2004), plus a scalar at-              sponses on the conflict items tended to be most similar
tribute. The function target could be predicted given                 to the responses expected if attention were focused on
the scalar attribute and either the CA or FR informa-                 the criterial attribute. Indeed, the ratio of attention to
tion. They found significant individual differences in at-            the CA attribute to the mean attention to the FR at-
tention and generalization. After learning to a criterion,            tributes reaches its peak at c = 1.4. With c near 0 or
subjects saw conflict items where the CA and FR in-                   2, attention is more evenly distributed, and responses to
formation conflicted. 58% of responses to those conflict              the conflict items reflect that.
items were consistent with the use of the single criterial
attribute, while 31% of responses were consistent with                  These results are consistent with Minda and Ross’ ar-
the use of the broad family resemblance structure.                    gument that individual differences in generalization on
   This task can be modeled using AEGLE2 , treating the               their task are due to differential weighting of attention
CA and FR attributes as cues and the scalar attribute                 to the CA and FR attributes. Further work is neces-
as the domain of the function. That is, AEGLE can                     sary to confirm whether the parameter c, representing
learn to partition its knowledge based on the cues, gating            the extent to which exemplars in the ALCOVE gating
different functions based on those cues.                              network are activated by distant inputs, is a good ex-
    2
                                                                      planation for those individual differences. It should be
      Unfortunately, the scheme used in SUSTAIN to tune at-           noted that Nosofsky and Zaki (1998) used variations in
tention is based on maximizing coverage of examples, not
minimizing error. As a result, attention to attributes is not         this same parameter to explain the differences between
differentially affected by error, and SEGLE is inadequate for         controls and amnesiac patients in recognition and cate-
exploring this data set.                                              gorization tasks.
                                                                909

                      Discussion                                                Acknowledgments
                                                               This work was supported by National Institute on Deaf-
                                                               ness and Other Communication Disorders Grant DC-
Of the myriad possible models of function learning, AE-        005765 to James S. Magnuson. Thanks to Jim Magnuson
GLE and SEGLE illustrate two variants on the mixture-          and the reviewers for comments on drafts of this paper,
of-experts approach pioneered by POLE (Kalish et al.           and to Lewis Bott, Michael Kalish, and Brad Love for
2004). Like POLE, AEGLE and SEGLE conceptualize                their suggestions regarding this project.
function learning as a process of learning how to select
simple experts. SEGLE finds the learning of different                                References
functions roughly as difficult as do experimental sub-           Busemeyer, J. R., E. Byun, E. L. Delosh, and M. A.
jects, can partition stimuli based on regions of the func-           McDaniel (1997). Learning functional relations
tion domain, and can partition stimuli based on external             based on experience with input-output pairs by
cue variables. Work with AEGLE suggests further ap-                  humans and artificial neural networks. In K. Lam-
proaches for modeling individual differences in learning             berts and D. Shanks (Eds.), Knowledge, concepts,
with many potential cues.                                            and categories, Chapter 11, pp. 405–437. Cam-
                                                                     bridge, MA: MIT Press.
   Although the general framework used by POLE is
                                                                 DeLosh, E. L., J. R. Busemeyer, and M. A. McDaniel
shared by AEGLE and SEGLE, both the experts and the
                                                                     (1997). Extrapolation: The sine qua non for ab-
gating network are radically different, and in many ways,
                                                                     straction in function learning. Journal of Experi-
simpler and more suitable for analysis. Several of the
                                                                     mental Psychology: Learning, Memory and Cogni-
parameters used in POLE have subtle and non-obvious
                                                                     tion 23 (4), 968–986.
effects and interactions. The use of a multiplicative gain
for each expert makes POLE’s predictions extremely sen-          Guigon, E. (2004). Interpolation and extrapolation in
sitive to the initial conditions and to changes to that              human behavior and neural networks. Journal of
gain. As another example, if not for a threshold at 0, the           Cognitive Neuroscience 16 (3), 382–389.
three parameters ω, λw , and λb would have only two de-          Jacobs, R. A., M. I. Jordan, S. J. Nowlan, and G. E.
grees of freedom. These sorts of interactions, combined              Hinton (1991). Adaptive mixtures of local experts.
with a very novel sort of gating module, make analysis               Neural Computation 3, 79–87.
very difficult. In addition, the first author, despite the
                                                                 Kalish, M. L., S. Lewandowsky, and J. K. Kruschke
gracious assistance of Michael Kalish (including a por-
                                                                     (2004). Population of linear experts: Knowledge
tion of the original POLE source code) was unable to
                                                                     partitioning and function learning. Psychological
get a reimplementation of POLE to show the behaviors
                                                                     Review 111 (4), 1072–1099.
described in Kalish et al. (2004) without radical changes
from the reported parameters. Although POLE’s under-             Koh, K. and D. E. Meyer (1991). Function learning:
lying motivations are novel and compelling, our concerns             Induction of continuous stimulus-response rela-
about its replicability and transparency limit the extent            tions. Journal of Experimental Psychology: Learn-
to which the model can be successfully applied.                      ing, Memory & Cognition 17, 811–836.
                                                                 Kruschke, J. K. (1992). ALCOVE: An exemplar-based
   It should also be noted that although AEGLE and                   connectionist model of category learning. Psycho-
SEGLE should be more suitable to analysis than POLE,                 logical Review 99 (1), 22–44.
they both have significant limitations that will be ad-
dressed by future exploration. The simple LMS experts            Lewandowsky, S., M. Kalish, and S. K. Ngang (2002).
used in the model, although adequate for the simulations             Simplified learning in complex situations: Knowl-
described here, don’t learn very quickly or accurately. In           edge partitioning in function learning. Journal of
addition, LMS experts would predict that immediate and               Experimental Psychology: General 131 (2), 163–
accurate learning of a single repeated exemplar could not            193.
occur, while it certainly could. As for the gating mod-          Lewandowsky, S. and K. Kirsner (2000). Knowledge
ules, ALCOVE, by virtue of being an exemplar model,                  partitioning: Context-dependent use of expertise.
requires a large number of training examples to learn all            Memory & Cognition 28, 295–305.
of the gating weights, and does not efficiently represent        Love, B. C., D. L. Medin, and T. M. Gureckis (2004).
this knowledge in the manner that SUSTAIN’s clusters                 SUSTAIN: A network model of category learning.
do. Future models in the tradition of SUSTAIN may                    Psychological Review 111 (2), 309–332.
improve upon that approach’s ability to learn attention
weights in an error-driven manner (Brad Love, personal           Minda, J. P. and B. H. Ross (2004). Learning cat-
communication), and these advances will be very use-                 egories by making predictions: an investigation
ful in extensions to AEGLE and SEGLE. Clearly, the                   of indirect category learning. Memory and Cogni-
class of mixture-of-experts models of function learning              tion 32 (8), 1355–1368.
allow insight into the sorts of partitioning and example-        Nosofsky, R. M. and S. R. Zaki (1998). Dissociations
driven processes that underly human function learning,               between categorization and recognition in amnesic
but some details and a truly complete, comprehensive                 and normal individuals: An exemplar-based inter-
model remain for future work.                                        pretation. Psychological Science 9 (4), 247–255.
                                                           910

