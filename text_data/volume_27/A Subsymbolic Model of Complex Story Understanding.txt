UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
A Subsymbolic Model of Complex Story Understanding
Permalink
https://escholarship.org/uc/item/9t28c3fp
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 27(27)
Authors
Fidelman, Peggy
Hoffman, Ralph
Miikulainen, Risto
Publication Date
2005-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

                   A Subsymbolic Model of Complex Story Understanding
                                                Peggy Fidelman        Risto Miikkulainen
                               Department of Computer Sciences, The University of Texas at Austin
                                                   {peggy,risto}@cs.utexas.edu
                                                              Ralph Hoffman
                                                 Yale-New Haven Psychiatric Hospital
                                                       ralph.hoffman@yale.edu
                            Abstract                                   istic stories. In this section, psychological evidence for scripts
   A computational model of story understanding is presented           will be reviewed, and computational models based on script
   that is able to process stories consisting of multiple scripts.     theory will be outlined.
   This model is built from subsymbolic neural networks, but
   unlike previous such models, it can handle stories of variable      Script-Based Story Representations
   structure and length. The model can successfully parse and
   paraphrase script-based stories that share long sequences of        According to script theory (Schank & Abelson, 1977), peo-
   common events, with no confusion between the stories. It            ple organize knowledge of common routines into stereotypical
   also exhibits several aspects of human behavior, including          event sequences. These scripts are made up of sequences of
   robustness to small changes in the sequence of events and
   emotion priming effects in response to ambiguous cues. It           events with open slots, as well as requirements about what
   can therefore serve as a foundation for testing theories of         can fill those slots. Scripts make interaction efficient by pro-
   normal and impaired story processing in humans.                     viding everyone involved with a set of expectations about
                        Introduction                                   what will take place. For example, most people who have
                                                                       traveled by airplane know that first they must get a board-
Computational models are a valuable tool for understanding             ing pass, then wait in a security line, then pass through a
human behavior. They allow investigation of how various                metal detector, then wait at the gate, and so on. Without
theories about cognition may combine to produce observed               such a script, a person would have to put a lot more intel-
human function. They can also provide a way of studying im-            lectual effort into figuring out what was expected of him at
pairment in a controlled environment, where the behavioral             each point. If no one had such a script, airports could hardly
effects of various types of underlying damage can be inves-            serve the function they do.
tigated systematically by lesioning or otherwise disrupting               Scripts also serve to make natural language communication
parts of the model.                                                    efficient. There is no need to recount all the details of an
   In this paper, a computational model of human story un-             ordinary visit to the dentist, for example; the speaker can
derstanding is presented that can learn to read and para-              just make reference to such a visit, and the listener can fill
phrase script-based stories of arbitrary length. The model is          in the details herself.
built from subsymbolic neural networks, which mimic many
                                                                          The hypothesis that humans use such scripts in cognition
computational properties of the brain such as distributed rep-
                                                                       and language is well supported by experimental evidence. For
resentations and correlation-based learning and performance.
                                                                       example, the degree to which events in stories will be remem-
This subsymbolic foundation makes it possible to simulate
                                                                       bered can be predicted by whether those events are part of
phenomena that arise from these properties, which is difficult
                                                                       such a script (Graesser, Gordon & Sawyer, 1979; Graesser,
to do with symbolic models of story understanding. Unlike
                                                                       Woll, Kowalski & Smith, 1980). Similarly, the amount of
previous subsymbolic models, however, it is not restricted to
                                                                       time it takes for a human to understand a sentence can be
stories consisting of a rigid, fixed-length structure. This flex-
                                                                       predicted by whether it fits into a script (Den Uyl & van
ibility allows the processing of more realistic stories, consist-
                                                                       Oostendorp, 1980). Because scripts are a particularly well-
ing of multiple scripts, which in turn allows more meaningful
                                                                       established theory in psychology, they provide a good foun-
conclusions about human cognition to be drawn.
                                                                       dation for a computational model of story processing.
   Using a small corpus of hand-designed representative sto-
ries, the model is shown to successfully parse and paraphrase          Models of Script-Based Story Processing
stories that share long sequences of common events, with
no confusion between the stories. The model also exhibits              Scripts have been used as a basis for several symbolic models
several aspects of human behavior, including robustness to             of story processing. The first of these was SAM (Script Ap-
small changes in the sequence of events and emotion priming            plier Mechanism) (Cullingford, 1978), able to handle stories
effects in response to ambiguous cues.                                 with multiple simultaneously active and interacting scripts.
   The paper is organized as follows. Section 2 describes              FRUMP (Fast Reading Understanding and Memory Pro-
related work in script-based story processing, including the           gram) (DeJong, 1979), on the other hand, skimmed newspa-
DISCERN model on which the current model is based. Sec-                per stories about stereotypical episodes and filled in slots cor-
tion 3 details the architecture of the model, and Section 4            responding to the most important parts of the script. Scripts
examines its behavior under various experimental conditions.           have been used since then in numerous symbolic systems that
Section 5 discusses the results of the experiments and possi-          aim at understanding natural language stories.
ble directions for future work.                                           Although there has been a lot of work on subsymbolic pro-
                                                                       cessing of sentences in the past two decades (McClelland &
           Background and Related Work                                 Kawamoto, 1986; Jain, 1991; Rohde, 1999; Henderson, 1994;
Scripts are knowledge structures for stereotypical sequences           Mayberry, 2004), the approach has been much less successful
of events that allow efficient understanding of complex, real-         at the level of stories. Early on, several models were de-
                                                                   660

                                                                                                 Input text   Output text
                                                                               Sentence                  Lexicon            Sentence
                                                                                 Parser                                     Generator
                                                                                 Story          Memory            Episodic    Story
                                                                                Parser          Encoder           Memory    Generator
                                                                         Figure 2: Architecture of the extended DISCERN model (omit-
                                                                         ting the question answering modules). The modified parts of the
Figure 1: Architecture of the original DISCERN story processing          model (dashed-line box) make it possible for the model to process
system (Miikkulainen, 1993). All processing modules (light gray)         stories consisting of multiple sequential scripts.
and memory modules (dark gray) are implemented by artificial
neural networks. The system could parse, paraphrase, and answer
questions about stories that consist of single script instantiations.    modules. In the extended model, a memory encoder module
                                                                         is also included to compress the sequence of script repre-
                                                                         sentations. Because the experiments reported in this paper
veloped that addressed parts of this process, such as script             focus on paraphrasing, the question answering modules are
application, sequential inference, and anaphora (Dolan, 1989;            omitted for simplicity.
Harris & Elman, 1989; St. John, 1992).                                      The processing modules (including the memory encoder)
   In contrast, DISCERN (Miikkulainen, 1993) was an inte-                are either feedforward or simple recurrent networks, trained
grated subsymbolic model of script-based story processing,               with backpropagation. The lexicon module is implemented
consisting of several neural network modules of the various              as a set of two self-organizing maps, organized in an unsu-
script-processing subtasks as well as a lexicon and episodic             pervised learning process. In the original model, the episodic
memory (Figure 1). Processing modules in DISCERN were                    memory was also a self-organizing map. In order to keep
feedforward and simple recurrent networks trained with back-             memory effects separate from paraphrasing performance, in
propagation, and the lexicon and episodic memory modules                 the current implementation it is replaced with a simple array
were implemented as self-organizing maps.                                indexed by the memory encoder. (In future implementations,
   In its original form, DISCERN processed stories consist-              a map-based memory may be used as well.) All modules are
ing of a single script. Although this version of DISCERN                 trained separately and simultaneously. They learn to process
was trained with three different scripts and three different             each other’s output, and when connected, filter out noise and
versions of each of those scripts (Miikkulainen, 1993), stories          errors so that the system performance is stable.
were nevertheless restricted to one of these nine structures.               The modules communicate using distributed representa-
Except for variation in the role bindings within the script,             tions for semantic concepts, stored in a central lexicon. The
the system could not process stories that were not simple                representations are developed automatically by all process-
script instantiations.                                                   ing networks (including the memory encoder module) while
   Human stories, by contrast, are rarely this simple. Since             they are learning their processing tasks. With backward error
scripts are a way of encoding routinely occurring sequences              propagation extended to the input layer, the representations
of events, a story that is just a single script is by definition         are modified as if they were an extra layer of weights, while at
not worth telling. Instead, humans use scripts as building               the same time making them publicly available in the lexicon.
blocks in more complex stories.                                          This mechanism, called FGREP, creates a reactive training
   The model presented in this paper is designed to extend               environment where the required input → output mappings
subsymbolic story processing to this next level. Each story is           change as the I/O representations change.
a combination of several scripts in a sequence. Much of the                 Single units in the resulting representations do not stand
behavior of original DISCERN is retained, but the stories                for clearly identifiable semantic features or label distinct cat-
that can be parsed and paraphrased are much more complex                 egories. All aspects of an input item are distributed over the
and realistic.                                                           whole set of units in a holographic fashion, making the sys-
                                                                         tem robust against noise and damage. Each representation
                   Model Architecture                                    also carries expectations about its possible contexts. The
Figure 2 depicts the organization of the new model, presented            emerging representations improve the system’s performance
as an extended version of DISCERN. In this section, the ar-              in the processing task and therefore efficiently code the un-
chitecture of the model is described in detail, with particular          derlying relations relevant to the task. This coding results
attention given to the parts that are new. More details on               in good generalization capabilities, superior to parallel dis-
the original DISCERN can be found in (Miikkulainen, 1993).               tributed systems with semantic feature–encoded representa-
                                                                         tions.
Overview of DISCERN                                                         The lexicon can be extended by cloning new instances of
DISCERN is an integrated natural language processing                     the items, that is, by generating a number of items with
model built entirely from distributed artificial neural net-             the same semantics but with distinct identities. This goal is
works. Modularity is a key concept in DISCERN. The differ-               accomplished by combining the semantic representation with
ent script-processing subtasks such as parsing, paraphrasing,            a unique ID representation. This ID+content technique is
and question answering, as well as the lexical, semantic, and            motivated by sensory grounding of words, and forms a basis
episodic memory components, are implemented in separate                  for symbolic processing in subsymbolic systems. It is possible
                                                                     661

to approximate a large number of items by dividing them              STORY 1                                STORY 2
into equivalence classes, resulting in combinatorial processing      Emotional valence: negative            Emotional valence: positive
                                                                     $airline Bob express jet               $airline John express jet
capabilities with linear cost.                                         Chicago long coach                     Chicago long coach
                                                                     Bob goes to express check-in .         John goes to express check-in .
   Representing input and output as sequences overcomes the          Bob gets boarding-pass to coach seat . John gets boarding-pass to coach seat .
                                                                     Bob goes through airport security .    John goes through airport security .
combinatorial explosion in communicating structurally com-           Bob gets on jet plane to Chicago .     John gets on jet plane to Chicago .
                                                                     Flight is long .                       Flight is long .
plex data. Internal representation can be made more general          Plane arrives at Chicago airport .     Plane arrives at Chicago airport .
by using data-specific assemblies, that is, by letting part of       Bob gets off plane .
                                                                     $outlaw Bob terrorist-cell
                                                                                                            John gets off plane .
                                                                                                            $relationship John Mary trusts
the representation determine how the rest of the assemblies            plants-bomb airport van
                                                                     Bob belongs to terrorist-cell .
                                                                                                              girlfriend
                                                                                                            John meets Mary .
                                                                                                                             good
should be interpreted. These techniques are implemented in           Bob is bad .                           Mary is girlfriend of John .
                                                                     Bob drives a van .                     Mary cares about John .
recurrent FGREP networks, of which there are two kinds. A            Bob plants-bomb at airport .           Mary is good to John .
                                                                                                            John trusts Mary .
sequential-input network reads a sequence of input items into
a stationary output representation, displaying expectations        Table 1: Two example stories with the same starting scene but
about the full context of each item. A sequential-output net-      different emotional valence. Each sentence of a story is parsed
work produces a sequence of output items from stationary           from a sequence of words, such as “Bob goes to express check-
input.                                                             in,” into a case-role representation such as “Bob goes express
                                                                     check-in,” which is a concatenation of FGREP representations
   The FGREP modules, together with a central lexicon, are         for words that fill the case roles agent, act, indirect object, at-
used as the basic pattern transformation building blocks in        tribute, direct object, origin, and destination, with “ ” indicating
DISCERN. Processing is carried out by a hierarchical organi-       that the role is not filled (i.e., a blank representation). Similarly,
zation of four FGREP modules, trained to paraphrase stories        the sequence of sentence case-role representations is parsed into a
                                                                   slot-filler representation of the script, such as “$airline Bob ex-
based on a sequence of scripts using natural language input        press jet Chicago long coach,” which is a concatenation of FGREP
and output. The complexity of this task is reduced by ef-          representations for words that fill the slots script, agent, patient,
fectively dividing it into subgoals. Each module is trained        primary attribute, secondary attribute, place, duration, and ter-
separately and in parallel, each developing the same lexicon.      tiary attribute. The same script can be associated with different
                                                                   emotional valences in different stories.
   The properties reviewed above also apply to the extended
model. In addition, there are several differences, outlined
below.
Extension to Multi-Script Stories
As noted in Figure 2, the major changes to the original DIS-
CERN occur in two of the modules: the memory encoder
and the story generator. The episodic memory has also been
modified from its original version in order to make the be-
havior resulting from these changes clear. In addition, an
assembly representing the emotional valence of a story was
added to make it possible to characterize the behavior of the      Figure 3: The memory encoder module. During training, the
model in more varied conditions.                                   network learns to replicate the input activations as well as possible
                                                                   in the output layer. As a result, the hidden layer activations
Emotional Valence Emotion is represented by a single               become a compressed version of the information contained at the
assembly (12 units) in the output of the story parser. In the      input layer.
experiments reported in this paper, the three valences “pos-
itive,” “neutral,” and “negative” are used, and their repre-       generator. However, in the extension there is no limit on the
sentations are learned with FGREP the same way as repre-           number of scripts – and thus the number of slot-filler rep-
sentations for story words. Each story is associated with a        resentations – involved in a story. It is therefore necessary
single emotional valence, sustained throughout the parsing         to design a way of compressing stories of arbitrary length
and paraphrasing process. The valence does not express any         into a fixed-size representation, while still retaining enough
specific propositional aspect of a story; it simply represents     information for the story generator.
the emotional context for the story in the model’s memory.            Recursive Auto-Associative Memory, or RAAM (Pollack,
Emotion can therefore be used to prime recall and processing       1990), is an architecture that forms compact distributed rep-
of the stories in the extended model.                              resentations of recursive data structures such as lists. Be-
   The story parser learns to produce each story’s emotion         cause the stories handled by the model are essentially lists
from the sequence of sentences it receives as input, just as       of scripts, RAAM provides a way of compressing them. Fig-
it learns to produce the rest of the slot-filler representation    ure 3 shows the structure of the memory encoder module.
for each script. It is worth noting that the script does not       The top and bottom rows represent the input and output
uniquely determine the emotion in all cases. This point is         layers, and the middle row represents the hidden layer, which
illustrated by the two example stories in Table 1.                 is smaller than the input and output layers. During training,
   To the story parser, the emotional valence is in many ways      the network learns to replicate the input activations as well
just like the other slots in a story representation, since its     as possible in the output layer. As a result, the hidden layer
possible bindings all reside in the lexicon. In the memory         activations become a compressed version of the information
modules and the story generator, emotion plays an impor-           contained at the input layer. A list is compressed by starting
tant role, affecting the system’s choice between alternative       at the end of the list and building up a compression for it one
continuations of the story.                                        item at a time. The representation for that item (in this case,
Memory Encoder Module In the original DISCERN,                     a slot-filler representation for one script) makes up the first
the entire story could be represented by one slot-filler repre-    part of the input to the network, and the second part of the
sentation. These fixed-length representations could be easily      input consists of the compression of all the items that follow,
handled by both the episodic memory module and the story           which was created by the network on the previous time step.
                                                               662

                                                                           Once the entire story has been read and the slot-filler rep-
                                                                        resentations for all its scripts (as well as a representation
                                                                        of overall emotional valence) have been formed by the story
                                                                        parser, they are presented one at a time to the memory en-
                                                                        coder network, starting with the last script and working back-
                                                                        wards as described at the end of the explanation of the mem-
                                                                        ory encoder module above. After a script has been presented
Figure 4: The organization of episodic memory. Items in the             to the memory encoder, the activation in this network’s hid-
memory are created by the memory encoder module. Cues to the
memory will be compared against the part of the entries shown           den layer is combined with the slot-filler representation of
here in color.                                                          that script and the emotion of the whole story to form an
                                                                        entry in the episodic memory. Then the second part of the
                                                                        memory encoder network’s input is set to match its hidden
                                                                        layer activations, the first part is set to the slot-filler repre-
                                                                        sentation of the previous script in the story and the emotion
                                                                        of the entire story, and the process is repeated to form an
                                                                        episodic memory entry for this previous script. In this way
                                                                        the episodic memory is populated with the scripts of one
                                                                        story. This entire process is repeated for all remaining sto-
                                                                        ries in the corpus.
                                                                           Once the model has read all stories, the paraphrasing phase
                                                                        begins. The first entry in the episodic memory, which corre-
                                                                        sponds to the beginning of a story, is retrieved, and the story
Figure 5: The story generator module in the extended model.             generator inputs are set to this representation. It consists of
This module produces a sequence of case-role sentence representa-       the slot-filler representation of the first script, the emotional
tions that constitute the full paraphrase of the story. In addition,    valence of the story, and the RAAM compression of the en-
it produces a cue to the episodic memory that will determine the
story generator’s own next input.                                       tire story. The story generator then produces a case-role
                                                                        sentence representation corresponding to the first sentence
Episodic Memory Module The task of the episodic                         of the first script; at the same time, it produces a cue to the
memory is to store the slot-filler representations of all scripts       episodic memory. The memory retrieved based on this cue
that together make up each story. The episodic memory was               becomes the next input to the story generator; at this point,
implemented as a simple array of items created by the mem-              it is the same representation as in the previous step. After
ory encoder module, as depicted in Figure 4. Cues to the                all sentences of the script have been output, the cue changes,
memory consist of an emotion plus a compression of the re-              the representation of the next script is retrieved, and the
maining story, and the retrieved script will be the one whose           story generator continues by generating the sentences of the
emotion and compression most closely match the cue (in                  second script. In this manner, the story generator is able
terms of Euclidean distance). In this way, the successive               to step through several successive scripts, while at the same
scripts’ slot-filler representations can be retrieved from the          time maintaining a memory (in its hidden layer) of the entire
memory one after another, and given to the story generator              story.
as input.                                                                  The sentence generator uses the case-role representation
                                                                        to output a sequence of words which tell the first sentence of
Story Generator Module The input to the story genera-                   the first script of the story. This process continues until the
tor consists of the sequence of script slot-filler representations      end of this story is reached, and then it is repeated for all
retrieved from the episodic memory module. As in the origi-             remaining stories in the episodic memory.
nal DISCERN, this module produces a sequence of case-role
sentence representations as its output that constitute the full                                  Experiments
paraphrase of the story. However, in the extension to DIS-              The model was trained and tested with a corpus of 9 stories, 8
CERN, the story generator has an additional task: at each               of which consisted of two scripts and one of which consisted of
step, in addition to outputting the sentence, it has to produce         three. Each of the 9 scripts from which the stories were com-
a cue to the episodic memory that will determine the story              posed consisted of 4–7 sentences. Many of these sentences
generator’s own next input. Figure 5 shows the modified                 were common to several of the scripts. In some cases the
architecture of the story generator module.                             same sequences of sentences occurred in several scripts; for
                                                                        example, the sequence “<PERSON> is bad. <PERSON>
Processing Multi-Script Stories                                         drives a <CARTYPE>.” occurred in 3 of the 9 scripts. The
After the modules have been trained, they are connected to-             ID+content technique, described briefly in Section 3a, was
gether in a chain, as depicted in Figure 2. Stories in the              used to create 3 distinct characters from the semantic rep-
corpus are presented to the model one word at a time. The               resentation “PERSON.” In the discussion that follows, the
sentence parser builds a case-role representation of each sen-          term “instances” refers to these 3 words.
tence as it comes in, and at the end of each sentence that                 The modules were trained separately and simultaneously
representation is passed as input to the story parser. With             for 30,000 epochs, by which point the error had plateaued.
this sequence of sentence case-role representations as input,           The modules were then connected together in a chain, as
the story parser builds a slot-filler representation of the cur-        shown in Figure 2, and the performance of the entire sys-
rent script. At the same time, it builds a representation of            tem was analyzed. First, performance under normal con-
the emotional valence of the current story. At the end of each          ditions was evaluated, when the stories simply need to be
script, the slot-filler representation is put aside for later use       paraphrased. Then the behavior of the system was tested
by the memory encoder.                                                  under various special conditions such as errors and ambigu-
                                                                    663

  Module               All words    Instances        Ei < 0.15       Eavg           Module          All words    Instances   Ei < 0.15    Eavg
  Sent. pars.            100.0        100.0             98.9         0.011          Story gen.         98.5         96.4        95.8      0.060
  Story pars.             99.4        100.0             97.6         0.014          Sent. gen.         97.2         93.7        95.8      0.039
  Story gen.              99.7        100.0             97.5         0.045       Table 4: Performance of the paraphrasing modules with the sys-
  Sent. gen.              99.6         98.2             98.2         0.027       tem’s emotion frozen at “positive.” The system generates all sto-
                                                                                 ries correctly, including those with an emotional valence other
Table 2: Paraphrasing performance of the model under normal                      than “positive.”
conditions. The first column indicates the percentage of words
correctly output by each module. The second column shows
performance on words created with the ID+content technique.                         Second, if the input story contains a sentence that is out
The third column represents the percentage of output units
whose error was less than 0.15, and the fourth column shows the                  of order, the story parser is often still able to generate the
average error over all output units for each module.                             correct slot-filler representation, which results in the events
                                                                                 appearing in normal order in the paraphrase. This behavior
                                                                                 is more dependable the closer the sentence is to its correct
ity, leading to predictions about human behavior.                                position, and the closer it is to the end of the story. These
                                                                                 effects are also seen in humans (Abelson, 1981).
Behavior under Normal Conditions                                                 Effect of parsing errors In the original DISCERN, if a
The overall accuracy of the model with all modules connected                     plausible but incorrect role binding arises during the parsing
in a chain is shown in Table 2. Note that all modules produce                    phase for a given story and is not corrected prior to memory
nearly 100% of words correctly, meaning that the system is                       formation, that story will consistently be generated with the
performing its task nearly perfectly at all steps.                               incorrect role binding in the future. This behavior is not au-
   For a qualitative characterization of this performance, con-                  tomatically inherited by the extended model, since the mem-
sider the stories in Table 3. These two stories have the same                    ory encoding and retrieving processes have been substantially
emotional valence and include the $lawchase script with all                      modified. However, the extended model also exhibits this be-
of the same role bindings, but the model is able to generate                     havior. Even though the information flow is more complex,
both correctly. This indicates that the story compressions                       it is still likely to be consistent.
produced in the memory encoder module are rich enough                               To our knowledge, there is not yet data about what can
that the story generator can use these compressions alone to                     cause humans to make consistent errors when paraphrasing a
distinguish between stories.                                                     story. The model predicts that one cause of such errors may
                                                                                 be parsing errors that are not corrected before the memory
                                                                                 is formed.
  STORY 1                             STORY 2
  Emotional valence: negative         Emotional valence: negative                Effect of Emotion Priming Setting the emotional va-
  $outlaw Bob terrorist-cell          $lawchase Bob police
    plants-bomb airport van             plants-bomb airport van                  lence in the input of the story generator to a particular
  Bob belongs to terrorist-cell .
  Bob is bad .
                                      Police go to airport .
                                      Witness talks to police .                  emotion during story generation creates an effect of emotion
  Bob drives a van .
  Bob plants-bomb at airport .
                                      Police find evidence of Bob
                                         in plants-bomb .
                                                                                 priming.
  $lawchase Bob police                Bob is bad .
    plants-bomb airport van           Bob drives a van .                            When the story generator is presented with an unambigu-
  Police go to airport .
  Witness talks to police .
                                      Police try to catch Bob .
                                      $airline Bob express jet
                                                                                 ous input pattern, the model will always generate the correct
  Police find evidence of Bob
     in plants-bomb .
                                        Chicago long coach
                                      Bob goes to express check-in .
                                                                                 story, even in cases where the emotion of the story does not
  Bob is bad .                        Bob gets boarding-pass to coach seat .     match the emotion of the model. For example, the model is
  Bob drives a van .                  Bob goes through airport security .
  Police try to catch Bob .           Bob gets on jet plane to Chicago .         able to generate the first story in Table 1 correctly even when
  $lawcatch Bob police                Flight is long .
    plants-bomb is is is praised      Plane arrives at Chicago airport .         the story generator input emotion is set to “positive,” as long
  Bob is bad .                        Bob gets off plane .
  Bob is caught by police .                                                      as the rest of the input pattern is an accurate representation
  Bob is charged with plants-bomb .
  Bob is jailed .
                                                                                 of that story. The overall performance of the paraphrasing
  Police are praised .                                                           portion of the model when its emotion was frozen at “posi-
Table 3: Distinguishing between similar stories. Even though                     tive” for all stories is given in Table 4. Note that performance
these two stories share the same emotional valence and include                   is disrupted very little: the vast majority of words are still
the $lawchase script with all the same role bindings, the model                  output correctly.
can distinguish between them and is able to generate both of
them correctly.                                                                     If the input to the story generator is ambiguous, the system
                                                                                 will still output one of the stories consistent with the cue,
                                                                                 but among the alternatives it favors the story that matches
Behavior under Special Conditions                                                its current emotional state. For example, when the story
A model’s response to errors and ambiguity can give valuable                     generator emotion is set to “positive” and the rest of the cue
insight into its validity as an explanation of human behavior.                   is equally consistent with each of the stories in Table 1, the
Such experiments with the model can also lead to predictions                     model will generate the second story.
about human behavior.                                                               This behavior is consistent with human data. Human re-
Inherited Behaviors of Original DISCERN The orig-                                call is biased toward memories that are associated with a
inal version of DISCERN displays several desirable error-                        person’s present emotional state, either because the memory
correcting behaviors. Many of these behaviors are automat-                       was formed during a similar mood or because its content is
ically inherited by the extended model.                                          affectively valenced in a similar way (Blaney, 1986).
   For example, when isolated words appear in inappropriate                      Effect of Imperfect Memory Retrieval To investigate
contexts in the input, they are automatically corrected in the                   the effects of imperfect memory retrieval, random noise was
memory trace and paraphrase. This property follows from                          added to the story generator input at every memory retrieval
the distributed nature of all the representations used in both                   step. The noise consisted of uniformly distributed random
the original DISCERN and the extended model.                                     numbers in the range [− k2 , k2 ], where k is a parameter in the
                                                                             664

range [0, 1].                                                       model was also shown to replicate several aspects of human
   The model proves to be quite robust to this noise. When          behavior. Because it is built on neural networks, which mimic
k ≤ 0.2, almost no effect is observed. As the amount of             many of the low-level computational aspects of the brain, it
noise increases, the model exhibits graceful degradation, with      constitutes a promising basis for future studies on normal
errors appearing first in the instance words (i.e., people).        and impaired human story processing.
Only much later, when k is in the range of 0.3 − 0.4, do
significant errors begin to appear in the other words.                                     Acknowledgments
   Interestingly, the errors are not evenly distributed in the      This work was supported by NIMH grant R01MH066228.
story generator output: some sentences consistently exhibit
more error than others. This behavior can be seen as a possi-                                   References
ble explanation for human data, which indicate that certain         Abelson, R. P. (1981). Psychological status of the script concept.
events in a script are more central than others and will be         The American Psychologist, 36:715–729.
recalled more reliably (Abelson, 1981; Graesser et al., 1979;       Blaney, P. H. (1986). Affect and memory. Psychological Bulletin,
Graesser et al., 1980). The model suggests that the other           99:229–246.
sentences may be recalled at will, but because their represen-      Cullingford, R. E. (1978). Script Application: Computer Under-
tation is full of errors, they will be rejected by the cognitive    standing of Newspaper Stories. PhD thesis, Department of Com-
system monitoring the output, and therefore they will not be        puter Science, Yale University. Technical Report 116.
included in the paraphrase.                                         DeJong, G. F. (1979). Skimming Stories in Real Time: An Ex-
             Discussion and Future Work                             periment in Integrated Understanding. PhD thesis, Department
                                                                    of Computer Science, Yale University. Technical Report 158.
The results presented in this paper show how subsymbolic            Den Uyl, M., and van Oostendorp, H. (1980). The use of scripts
story processing can be extended to multi-script stories. The       in text comprehension. Poetics, 9:275–294.
behavior of the model matches normal human behavior in
                                                                    Dolan, C. P. (1989). Tensor Manipulation Networks: Connec-
several ways. It is robust to script events being slightly out      tionist and Symbolic Approaches to Comprehension, Learning and
of order, and less robust if an event is far from its correct       Planning. PhD thesis, Department of Computer Science, Univer-
place. It corrects words that appear in inappropriate con-          sity of California Los Angeles. Technical Report UCLA-AI-89-06.
texts. Given an unambiguous cue, the model can tell the             Graesser, A. C., Gordon, S. E., and Sawyer, J. D. (1979). Recogni-
correct story regardless of its emotional state, and in the         tion memory for typical and atypical actions in scripted activities.
case of an ambiguous cue, it will choose to tell the alterna-       Journal of Verbal Learning and Verbal Behavior, 18:319–332.
tive most consistent with its emotional state. When memory          Graesser, A. C., Woll, S. B., Kowalski, D. J., and Smith, D. A.
retrieval is noisy, the model has more trouble with some sen-       (1980). Memory for typical and atypical actions in scripted activ-
tences of a given script than others. It also makes predictions     ities. Journal of Experimental Psychology: Human Learning and
about human behavior. When humans make consistent er-               Memory, 6:503–515.
rors in the retelling of a given story, it may be because the       Harris, C. L., and Elman, J. L. (1989). Representing variable
story was originally parsed in a wrong but plausible way and        information with simple recurrent networks. In Proceedings of the
was encoded in memory with these mistakes.                          11th Annual Conference of the Cognitive Science Society, 635–
                                                                    642. Hillsdale, NJ: Erlbaum.
   Although the model is based on DISCERN, it represents
a significant step beyond DISCERN’s original capabilities.          Henderson, J. (1994). Connectionist syntactic parsing using tem-
                                                                    poral variable binding. Journal of Psycholinguistic Research,
Being able to process stories of multiple scenes is important       23:353–379.
if the model is to be used to understand human behavior.
One particularly promising direction for future work is to          Jain, A. N. (1991). PARSEC: A Connectionist Learning Archi-
                                                                    tecture for Parsing Spoken Language. PhD thesis, Computer Sci-
use the model to test hypotheses about the possible causes          ence Department, Carnegie Mellon University. Technical Report
of schizophrenia. The underlying pathology of this disease is       CMU-CS-91-208.
unknown; it is typically diagnosed on the basis of its most rec-
                                                                    Mayberry, III, M. R. (2004). Incremental Nonmonotonic Parsing
ognizable symptoms, which are language-related. As might            Through Semantic Self-Organization. PhD thesis, Department of
be expected, current theories about the causes of schizophre-       Computer Sciences, The University of Texas at Austin. Technical
nia focus on anomalies at the level of systems of neurons.          Report AI-TR-04-310.
A symbolic model of language processing would be of little          McClelland, J. L., and Kawamoto, A. H. (1986). Mechanisms of
use in investigating such causes. On the other hand, what           sentence processing: Assigning roles to constituents. In Parallel
differentiates schizophrenic language from normal language          Distributed Processing, Volume 2, 272–325. Cambridge, MA: MIT
are often chaotic transitions from one script to the next, or       Press.
an intermingling of emotionally charged personal and imper-         Miikkulainen, R. (1993). Subsymbolic Natural Language Process-
sonal scripts that result in delusions. Previous subsymbolic        ing: An Integrated Model of Scripts, Lexicon, and Memory. Cam-
models, which could process only stories consisting of a sin-       bridge, MA: MIT Press.
gle script, consequently would be inadequate as well. The           Pollack, J. B. (1990). Recursive distributed representations. Ar-
model presented in this paper, however, is both built on sub-       tificial Intelligence, 46:77–105.
symbolic principles and able to process multi-script stories        Rohde, D. L. (1999). A connectionist model of sentence compre-
with retrieval biases driven by emotion priming. As such, it        hension and production. Dissertation Proposal, School of Com-
forms a promising foundation for investigating the high-level       puter Science, Carnegie Mellon University.
effects of the low-level pathologies hypothesized to underlie       St. John, M. F. (1992). The story gestalt: A model of knowledge-
schizophrenia.                                                      intensive processes in text comprehension. Cognitive Science,
                                                                    16:271–306.
                          Conclusion                                Schank, R., and Abelson, R. (1977). Scripts, Plans, Goals, and
A cognitive model of story understanding was presented that         Understanding: An Inquiry into Human Knowledge Structures.
                                                                    Hillsdale, NJ: Erlbaum.
is able to process stories consisting of multiple scripts. The
                                                                665

