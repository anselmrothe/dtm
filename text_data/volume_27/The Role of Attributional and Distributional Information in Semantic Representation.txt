UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
The Role of Attributional and Distributional Information in Semantic Representation
Permalink
https://escholarship.org/uc/item/8xj0h1qv
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 27(27)
Authors
Andrews, Mark
Vigilocco, Gabriella
Vinson, David B.
Publication Date
2005-01-01
Peer reviewed
 eScholarship.org                                   Powered by the California Digital Library
                                                                      University of California

 The Role of Attributional and Distributional Information in Semantic
                                                    Representation
                                     Mark Andrews (mark@gatsby.ucl.ac.uk)
                           Gatsby Computational Neuroscience Unit, University College London
                                                      London, WC1N 3AR
                                    Gabriella Vigliocco (g.vigliocco@ucl.ac.uk)
                                         David Vinson (d.vinson@ucl.ac.uk)
                                   Department of Psychology, University College London
                                                      London, WC1E 6BT
                           Abstract                               that two words appear in similar texts might imply that
                                                                  these words refer to similar things in the world. It is,
   In recent studies of semantic representation, two distinct     however, reasonable to assume that these sources are
   sources of information from which we can learn word
   meanings have been described. We refer to these as at-         distinct, or that one source is not entirely explained by
   tributional and distributional information sources. At-        or dependent upon the other. For example it can be ar-
   tributional information describes the attributes or fea-       gued that attributional information is more important
   tures associated with referents of words, and is acquired      in learning words referring to concrete entities and ac-
   from our interactions with the world. Distributional in-
   formation describes the distribution of words across dif-      tions, for which the properties of the thing or event in the
   ferent linguistic contexts, and is acquired through our        world can be experienced via the senses, whereas distri-
   use of language. While previous work has concentrated          butional information may be more important in learning
   on the role of one source, to the exclusion of the other,      abstract words, those that we learn primarily via lin-
   in this paper we study the role of both sources in combi-      guistic context. Thus both types of information can be
   nation. We describe a general framework based on prob-
   abilistic generative models for modelling both sources of      exploited in order to learn word meanings. In previous
   information, and how they can be integrated to learn se-       studies, the contributions of either source alone, inde-
   mantic representation. We provide examples comparing           pendent of the other, have been studied. In particular,
   the learned structures of each of three models: attri-         within cognitive psychology, researchers have primarily
   butional information alone, distributional information
   alone, and both sources combined.                              focused on the development of models of meaning repre-
                                                                  sentation based on attributional information (Collins &
                                                                  Quillian, 1969; Hinton & Shallice, 1991; McRae, Sa, &
Keywords: Probabilistic Models; Semantic Represen-                Seidenberg, 1997; McClelland & Rogers, 2003; Minsky,
tation; Information Integration.                                  1975; Smith, Shoben, & Rips, 1974; Vigliocco, Vinson,
                                                                  Lewis, & Garrett, 2004), whereas recently within compu-
                       Introduction                               tational linguistics, machine learning and related areas in
We can distinguish between two distinct sources that              cognitive science, researchers have primarily focused on
provide us with information about the meanings of                 distributional information alone (Burgess & Lund, 1997;
words. The first is what we will call attributional in-           Dagan, Lee, & Pereira, 1997; Griffiths & Steyvers, 2003;
formation. This describes physical, emotional and con-            Hofmann, 1999b; Landauer & Dumais, 1997; Schütze,
ceptual (or otherwise nonlinguistic) attributes associated        1992). In this paper, we describe a model that uses both
with the referents of words. These are features or at-            sources in combination as the basis of semantic repre-
tributes associated with words that are based upon or             sentation.
built up from our interactions in the environment, and
our knowledge of the objects and relationships in the                     Generative Models of Semantic
world. A second source of information about word mean-
ings is what we will call distributional information. This                            Representations
describes the distribution of words across different lin-         Probabilistic generative models provide a general means
guistic or textual contexts. This type of information has         by which to model semantic representation. This ap-
been memorably summarized by the phrase “you shall                proach has already been pursued both within machine
know a word by the company it keeps”, (Firth, 1957).              learning (Blei & Ng, 2003; Hofmann, 1999b, 1999a; Teh,
In other words, the type of linguistic contexts in which          Jordan, Beal, & Blei, 2004) and within cognitive science
a word occurs can provide clues as to what that word              (Griffiths & Steyvers, 2003, 2002). Generative models
might mean.                                                       describe the data of interest in terms of explicit proba-
   While these two sources of information are not un-             bilistic relations and variables. The nature of these rela-
correlated, neither are they identical, and it is plausi-         tionships and variables are inferred or learned from data.
ble to assume that they are both utilized when learning           This general class of models can be used to model the
the meaning of words. The two sources are correlated              role of both attributional and distributional information.
because words that refer to similar things or events in           Indeed, many of the previous models of semantic repre-
the world are likely to appear in similar linguistic con-         sentation, mentioned above, can be described as implic-
texts. The reverse case could also be argued. Knowing             itly falling within this general class. In this paper, we
                                                              127

will use this approach to model both the independent             with the words dog and pet, etc. This two sources of
and joint role of distributional and attributional infor-        information could be combined to provide a richer un-
mation in semantic representations.                              derstanding of the semantics of the word cat than could
   Considering the role attributional information alone          be learned by either source alone.
plays in semantic representation, a suitable generative
model would describe how concrete terms (or more prop-                 The Model: Formal Specifications
erly, their referents) are generated. In this case, a simple     We can make the preceding description more formal as
generative model might assume that concrete terms are            follows. The observable data that we are modelling con-
defined in terms of a distribution over latent attribute         sist of both texts and the attributes associated with
classes, and that these attribute classes are themselves         words. As mentioned, texts take the form of para-
probability distributions over binary properties or fea-         graphs, sentences and strings of consecutive words in
tures. Concrete terms are distributions over these fea-          a natural language corpus. If there are J texts in a
tures, obtained by repeatedly sampling from the distri-          corpus, we can label them as {z1 , z2 . . . zj . . . zJ }. The
bution of latent attribute classes and then sampling from        texts are, in general, of variable length. For example,
the distributions over binary properties associated with         text zj is of length Tj and consists of the sequence of
these classes.
                                                                 words {w1j , w2j . . . wtjj . . . wTj j }. For each possible j and
   Considering the role distributional information alone
plays in semantic representation, an appropriate gen-            t, wtjj will be a word in the vocabulary of word-types
erative model would describe how a text is generated.            V = {v1 , v2 . . . vk . . . vK }. What defines a word-type for
Henceforth, we will use the term text to refer to any            our purposes is described below, but in general, word-
linguistic or textual utterance. While this could be an          types are a set of common dictionary words in English.
entire article, book, or transcribed conversation, we will          A subset of the words in any human language will
usually use the term to refer to paragraphs, sentences,          concrete words, or words that refer to objects, events,
or strings of consecutive words in written documents or          actions, etc., in the world. Assume that of the K dic-
transcribed speech. A simple generative model of texts           tionary words in V we have obtained a set Vf of N con-
might assume that texts are multinomial distributions            crete words, where N ≤ K. For each word in Vf we
over latent semantic classes or topics, and that these la-       have a probability distribution over a set of L binary
tent topics are themselves multinomial distributions over        features. We can represent this binary feature vector by
words. A text is generated by repeatedly sampling from           y = {y1 , y2 . . . yl . . . yL }, where yl is the lth binary fea-
the distribution over latent topics, and then sampling           ture of y. Each feature is a property or attribute that
from their corresponding distributions over words. In            could be associated with the physical referent of the con-
such a model, the semantic representation of a given             crete words in Vf .
word is defined in terms of its posterior probability dis-          To model semantic memory according to the descrip-
tribution over the latent classes. In other words, the se-       tion provided above, we provide generative models for
mantic representation of a given word is the probability         the case where attributional data alone is used, where the
distribution over the latent topics that can be inferred         distributional data alone is used, and where both data
whenever the word is generated.                                  sources are used. The graphical models (or Bayesian
   Combining the two sources of information is straight-         Networks) for these three generative models are shown in
forward. The data we observe consists of sets of words,          Figure 1. A graphical model describes the conditional in-
and associated with each word is a distribution over             dependence structure of the variables in a model. Taken
binary non-linguistic attributes. As will be clarified,          together, we have the observable variables wt , yt and
we can assume that a distribution over latent variables          zt that represent, respectively, the words, features and
accounts for both the distribution of words in a text            text occurring at a time t. In addition, we introduce the
and the distribution of binary features associated with a        latent-variable xt ∈ {ξ1 , ξ2 . . . ξm . . . ξM }. As a latent,
given word. The semantic representation of a word is de-         or hidden, variable the value of xt is unobserved. We
fined in terms of its posterior distribution over these la-      see that in the attributional model, the binary attribute
tent topics. These semantic representations will be con-         vector yt is conditioned upon xt , while xt is conditioned
strained to account for both the distributions of words          upon the word label wt . In the distributional model, we
in texts, and the distributions of binary attributes asso-       have the observable words conditioned upon the latent-
ciated with given words.                                         variable xt , which is then conditioned upon the text zt .
   The intuitive rationale behind the model is as follows:       In the combined model, both the words wt and binary at-
On its own, the statistical structure and patterns in a          tribute vector yt are conditioned upon the latent-variable
language can provide information about word meanings.            xt , with xt conditioned upon the text zt . The paramet-
A subset of the words in the language are associated             ric forms of these conditional distributions are as follows.
with physical objects or events in the world, and this           (The parameters in the models are referred to generically
information can be integrated with statistical patterns          as θ, until further specified).
in language. As a contrived example, knowledge that
the word cat refers to those creatures with claws and
                                                                 Attributional Model
whiskers that meow, etc. can be integrated with im-              In the attributional model, the probability of the ob-
plicitly acquired statistical knowledge that cat co-occurs       served binary attributes yt , conditioned upon wt can be
                                                             128

                                                                                                           yt
                                  yt                                  wt
                                                                                                           wt
                                  xt                                   xt
                                                                                                           xt
PSfrag replacements
                                  wtPSfrag replacements                ztPSfrag replacements
                                                                                                           zt        1≤t≤T
                                      1≤t≤T                               1≤t≤T
                      (a) Attributional Model               (b) Distributional Model                 (c) Combined Model
  Figure 1: The generative models that utilize a) attributional, b) distributional and c) combined information sources.
  factored as                                                              re-write (3) as
                          X                                                                              M             K
            P yt wt , θ =        P yt xt , θ P xt wt , θ ,         (1)                   P wt zt , θ =
                                                                                                         X
                                                                                                                 πm[zt ]
                                                                                                                         Y     I(w =v )
                                                                                                                              qmk t k ,         (4)
                           {xt }
                                                                                                            m            k=1
  where the latent variable xt is integrated over. The prob-               where I(a) is an indicator function, taking the value 1 if
  ability that the lth binary variable of yt , i.e ylt , takes the         its argument a is true, and 0 otherwise. The distribu-
  value 1 given that xt = ξm , is denoted by pml . The prob-               tional model is, as such, a mixture of multinomial distri-
  ability that xt takes the value ξm given the value of wt ,               butions.
                    [w ]
  is denoted by πm t . We can re-write (1) as                              Combined Model
                                                                           In the combined model, as shown in Figure 1, both the
                      M           L
                    X
                           [wt ]
                                 Y    (1−y t )          (1−ylt )           word label wt and attribute vector yt are conditioned
     P yt wt , θ =        πm         pml l (1    − pml )         . (2)     upon the latent variable xt , which is conditioned upon
                    m=1          l=1                                       the text zt . Integrating over the values of xt , the condi-
                                                                           tional likelihood of the observables is
  As such, the attributional model is a mixture of M mul-                                         
  tivariate Bernoulli distributions.                                           P y t , w t zt , θ
                                                                                                                                                (5)
                                                                                  X                                               
                                                                               =        P yt xt , θ P wt xt , θ P xt zt , θ
  Distributional Model
                                                                                  {xt }
  In the distributional model, the probability of the ob-
  served word wt , conditional upon observing text zt can                                                                                 [z ]
                                                                           and substituting in parameters pml , qmk and πmt as in
  be factored as                                                           (2) and (4)
                        X                                
            P wt zt , θ =        P wt xt , θ P xt zt , θ .         (3)             M            K               L
                                                                                                     I(w =vk )       (1−ylt )             t
                                                                                  X            Y               Y
                                                                                         [zt ]
                           {xt }                                               =        πm          qmk t           pml       (1−pml )(1−yl ) . (6)
                                                                                   m           k=1             l=1
  The probability of observing that wt = vk given that                     As such, the combined model is a mixture of both mul-
  xt = ξm is denoted by qmk . The probability that xt = ξm                 tivariate Bernoulli distributions and multinomial distri-
                                                        [z ]
  upon observing that zt = j, is denoted by πmt . We can                   butions.
                                                                       129

Model Learning                                                                In a previous study, Vigliocco et al. (2004) compiled
                                                                           frequencies of a set 1029 binary attributes associated
Given a set of training data D consisting of both texts
                                                                           with a set of 456 common words. The 456 words con-
and attributes associated with concrete words, for each                   sisted of 230 nouns, of which 169 referred to objects and
model above we would ideally wish to estimate P θ D ,
                                                                           71 to actions, and 216 verbs, all of which referred to
or the posterior probability of the parameters given the
                                                                           actions. The attribute-types and their frequencies were
data. In any analyses of the models, we would then in-
                                                                           collected from speaker-generated lists of attributes asso-
tegrate over this entire distribution. In related work, we
                                                                           ciated with the 456 words. This was done in a manner
are using Markov-Chain Monte Carlo (MCMC) simula-
                                                                           similar to that used in McRae et al. (1997). Certain
tions to compute these posteriors, however these studies
                                                                           word types referred to distinct verb and noun senses, e.g.
will not be described here. For present purposes, we will
                                                                           (the) hammer and (to) hammer. For present purposes,
approximate P θ D by its modal point θmp , which as-
                                                                           these word-types were regarded as identical and the vec-
suming a prior distribution over the parameters, is given
                                                                           tors associated with the words were collapsed. Out of
by its maximum likelihood estimate θmle .
                                                                           this set of words, a subset of exactly 300 also occurred
   The standard procedure for maximum-likelihood (or
                                                                           in our reduced (i.e. 7, 393 word-types) text corpora.
maximum posteriori) estimation in latent-variable mod-
els is Expectation-Maximization (EM). This consists of                        In summary, the text corpora consist of 51, 160 texts,
iteratively computing         a lower-bound on the likelihood              each described by a frequency distribution over 7, 393
                         
of the data P D θ , and maximizing this bound with re-                     word-types. Of these 7, 393 word-types, a subset of 300
spect to the parameters. This leads to a set of parameter                  are also described by a frequency distribution over 1029
update rules that can be guaranteed to monotonically in-                   attribute-types. The word-attribute set alone, the text
crease the likelihood and converge to an (at least local)                  set alone and the word-attribute and text-set together
maximum. For example, in the case of the combined                          were used to train the distributional model, attributional
                                                              [z ]         model and combined models, respectively. In all cases,
model above, the update rules for pml , qmk and πmt are                    this was done using EM to find the maximum likelihood
                                                                           estimates of the parameters given the observed data-
                J X  Tj
             X                                                             sets. In the simulations described here, the attributional
                        P xt = ξm , wt , yt , zt = j, θ ylt ,
                                                       
   pml ∝                                                           (7)     model used a latent-variable of dimensionality 150 with
              j=1 tj =1                                                    both the distributional model and the combined model
   qmk                                                                     used latent-variables of dimensionality 300.
           J X  Tj
       X                                                          (8)     Analysis of Trained Models
    ∝               P xt = ξm , wt , yt , zt = j, θ I(wt = vk ),
        j=1 tj =1                                                          As described above, the latent variables in each model
                 Tj                                                        can be seen as distributions over words, attributes or
   πm[zt ]
            ∝
               X                                    
                     P xt = ξm , wt , yt , zt = j, θ .             (9)     both. In Figure 2, we illustrate some of these latent
                                                                           variables. For each case, we draw samples of the words
               tj =1
                                                                           and/or attributes with which they are associated. The
                                                                           distributional model learns latent variables that corre-
The update rules for the attributional model and distri-                   spond to multinomial distributions over words. These
butional model are special cases of the above rules, with                  distributions can be intuitively viewed as topics that
the appropriate changes having been made.                                  have been learned by the model. In the three exam-
                                                                           ple latent variables shown for the distributional model
                           Simulations                                     (i.e. the three leftmost columns of Figure 2), we see
The text corpora used consisted of fiction and non-fiction                 topics that could be labelled government, business and
books publicly available at the Oxford Text Archive                        religion. In the attributional model, latent variables
(≈ 6.5 · 106 words) and Project Gutenberg (≈ 11.6 · 106                    correspond to distributions over binary attributes and
words), Reuters Newswire texts (≈ 2.5 · 106 words), and                    can intuitively be viewed as attribute classes. Examples
a set of Usenet articles (≈ 5.25 · 106 words). We folded                   of attribute-classes, and the samples over attributes to
British into American spellings (e.g. centre to center,                    which they correspond, are shown in the middle three
favour to favor, realised to realized, etc.), folded affix-                columns of Figure 2). These classes could be labelled
variations of words into their word-stems (e.g. swims,                     human-body, fruit-vegetable and clothing. In the case of
swam, swum, swimming changed to swim, etc.), erad-                         the combined model, latent variables correspond to dis-
icated non-words (using a standard American-English                        tributions over both words and attributes. In this sense,
dictionary), and eradicated stop-words (using a standard                   they are merges or combinations of both attributional
list of ≈ 550 stop-words). This reduced the corpora to a                   and distributional classes. We provide some examples in
total size of ≈ 7.7 · 106 words, with 16, 956 word-types.                  the rightmost three columns of Figure 2). The classes
By further eradicating all word-types that appear with                     learned could be labelled transport, medical and war,
a frequency of greater than 104 or less than 102 , we can                  each defined both by clusters of words and clusters of
reduce the total size to ≈ 6.1·106 words, and 7, 393 word-                 attributes. The words are given in uppercase, while the
types. This corpus was divided into a set of 51, 160 texts,                attributes are in lowercase.
each of which were ≈ 150 words long.                                          In each model, we can measure how much any given
                                                                       130

                NATION        MONEY        ALLAH       human       fruit      leg     CAR     PATIENT       WAR
             AUTHORITY      HUNDRED         BIBLE       face       green   clothing  HORSE    MEDICAL       GUN
             PRINCIPLE        COURT        BELIEF       hair       grow      wear     RIDE    DOCTOR        KILL
              CENTURY          LAND       APOSTLE        eye        red      body    DRIVE     HEALTH     ATTACK
                 UNITE          PAY       CHURCH      shoulder    round    protect  DRIVER    MEDICINE    KNIGHT
            GOVERNMENT      THOUSAND    DISBELIEVE       leg      sweet      cover  transport    nose        kill
               SOCIETY          TAX        CHRIST       hand       juice     body    vehicle     body      weapon
              RELIGION         CITY         JESUS       body        tree    warm      4-legs    human       anger
           CONSTITUTION      SCIENCE       MARRY        foot        eat      long     wheel       eye        fear
             POLITICAL        OFFICE       SPIRIT      mouth       food    humans      car       head        yell
Figure 2: Examples of latent classes learned by (from left to right) the distributional, attributional and combined
models. For each example latent class, we have drawn samples of the words and/or the attributes they correspond
to. Capitals refer to words, while lower case refer to attributes.
word predicts any other. The extent to which word vj             Combined : threat knife murder stab bomb threaten kick
predicts vi is given by                                          snap knock argue
                                                                Ride
          P w = vi w = vj , θ
                                                                 Distributional : motorcycle bicycle horse chase pant slide
                                                      (10)
             X                                
           =      P w = vi x, θ P x w = vj , θ .                 thumb truck clatter ankle
              {x}                                                Attributional : carry drive travel train move pull ap-
                                                                 proach walk push truck
In both the distributional
                              model and the combined            Combined : motorcycle bicycle horse travel pull slide
model, P w = vi x, θ is the likelihood term of the              truck chase carry push
model, while P x w = vj , θ is given by Bayes rule,
                                                                                          Discussion
                                                                 The attributes associated with a word, and its distri-
                       X              
      P x w = vj , θ ∝      P w = vj x P(x, z) .      (11)
                        {z}
                                                                 bution across texts both provide information about its
                                                                 the meaning. Either source alone can provide informa-
On the other                                                     tion not provided by the other: Attributes provide in-
                 hand, for the attributional model             formation about the physical (or nonlinguistic) relation-
P x w = vj , θ is directly available, while P w = vi x, θ
is obtained by Bayes rules, i.e.                                 ships in the world between the referents of the word,
                                                                 while distributional information provide a rich patterns
                                    
    P w = vi x, θ ∝ P x w = vi , θ P w = vi θ
                                                  
                                                      (12)       of linguistic (or non-physical) contexts where the word is
                                                                 found. While attributional information is undoubtedly
How predictive any word is, given an observation of an-          used to learn concrete words, distributional information
other word, can be taken as a measure of semantic relat-         provides valuable information about the behavior of ab-
edness. This measure may, perhaps, be more theoreti-             stract words. While in previous studies, the role in se-
cally motivated than other measures that are based upon          mantic representations of each source, independent of
a distance metric. This argument is already presented in         the other, has been investigated, in this paper we de-
detail in Griffiths and Steyvers (2003). What a predic-          scribe how both sources can be used in combination.
tion of a word vi by a word vj means can be interpreted              In using both sources of information, patterns in one
as given that we are observing vj in this text, how proba-       source can interact with those of another. As a trivial
ble is word vi . Below, we show words predicted by some          example, if words va and vb are related with respect to
example words in each of the three models. For compar-           attributional information, while word vb and vc are re-
ison purposes between the models, here we provide only           lated with respect to distributional information, that we
prediction of words that were in both the text-based and         might infer that va and vc are related. Inferences may
attribute-based data-sets.                                       be made given the ensemble of correlations between and
                                                                 within the two data sources. Such inferences are sug-
Dog                                                              gested by the latent variables that are learned and shown
Distributional : growl bark chase lick whine cat tail paw        in Figure 2. For example, in the case of the combined
wolf snap                                                        model’s latent classes shown in Figure 2, we can see that
Attributional : cat rabbit goat tail pig fox sheep horse         attributional information leads to knowledge of patterns
bear fur                                                         of body-parts, while distributional information leads to
Combined : cat growl tail bark paw whine chase sheep             knowledge of patterns relating to medicine. Together
lick wolf                                                        both can lead to an inference that the realm of medicine
                                                                 and medical things are coupled with realm of body-parts
Gun                                                              and functions.
Distributional : threat stab knife bomb kick kill argue
snap knock murder                                                         Conclusions and Further Work
Attributional : murder sword dagger bomb threat knife            Models that can integrate attributional and distribu-
shield threaten stab scream                                      tional information may inform us about how knowledge
                                                             131

derived from experience in the world and knowledge de-          Firth, J. R. (1957). A synopsis of linguistic theory 1930-
rived from language may be integrated to lead to human                 1955. In Studies in Linguistic Analysis (special vol-
semantic representations. This is the starting point and               ume of the Philological Society, Oxford) (p. 1-32).
motivation behind this present work. Models based on                   Oxford: Blackwell.
attributional information develop semantic representa-          Griffiths, T., & Steyvers, M. (2002). A probabilistic
tions from the nonlinguistic attributes associated with                approach to semantic representation. In Proceed-
the referents of words. This is an intuitively appealing
idea about how word meanings are acquired. From a                      ings of the 24th annual conference of the cognitive
developmental perspective, these models provide an ac-                 science society.
count of how word meanings can be linked to conceptual          Griffiths, T., & Steyvers, M. (2003). Prediction and se-
knowledge that develops independent of language. How-                  mantic association. In S. T. S. Becker & K. Ober-
ever, attributional information does not represent the                 mayer (Eds.), Advances in neural information pro-
only source of information about word meaning. From                    cessing systems 15 (pp. 11–18). Cambridge, MA:
early stages in their development, children are exposed to             MIT Press.
language and it is reasonable to assume that they implic-       Hinton, G., & Shallice, T. (1991). Lesioning an attrac-
itly use the distributional information inherent in their              tor network: Investigations of acquired dyslexia.
linguistic input in order to learn new words, as well as to            Psychological Review, 98, 74-95.
enrich the semantic representation for words referring to       Hofmann, T. (1999a). Probabilistic latent semantic anal-
concrete referents. Using probabilistic models we have
shown how these two different sources of information can               ysis. In Proceedings of the fifteenth conference on
be integrated to form semantic representations.                        uncertainty in artificial intelligence.
   Two projects are to be carried out to further the ideas      Hofmann, T. (1999b). Probabilistic latent semantic in-
presented here. The first regards the models that are                  dexing. In Proceedings of the twenty-second annual
used. The EM algorithm is prone to local minima, as                    international sicir conference on research and de-
well as data over-fitting. This can limit the usefulness of            velopment in information retrieval.
the models that we have presented here. Bayesian learn-         Landauer, T., & Dumais, S. (1997). A solutions to
ing methods based on MCMC sampling can be used to                      Plato’s problem: The Latent Semantic Analyis
sample from the posterior probability of the parameters,               theory of acquistion, induction and representation
and integrate over this for both learning and inference.               of knowledge. Psychological Review, 104, 211-240.
These methods have already been pursued in related              McClelland, J., & Rogers, T. (2003). The parallel dis-
probabilistic models used for learning semantic repre-
                                                                       tributed processing approach to semantic cogni-
sentations (Blei & Ng, 2003; Griffiths & Steyvers, 2003,
2002; Teh et al., 2004). The second project concerns                   tion. Nature Reviews Neuroscience, 4 (4), 310-322.
comparison with human behavioral data. We speculate             McRae, K., Sa, V. de, & Seidenberg, M. (1997). On
that semantic relationships formed when both attribu-                  the nature and scope of featural representation of
tional and distributional information are used in combi-               word meaning. Journal of Experimental Psychol-
nation will be more accurate in describing human per-                  ogy: General, 126, 99-130.
formance than models based on either source alone.              Minsky, M. (1975). A framework for representing knowl-
                                                                       edge. In P. Winston (Ed.), The psychology of com-
                       References                                      puter vision (p. 211-277). McGraw-Hill.
Blei, D., & Ng, M., A. andand Jordan. (2003). Latent            Schütze, H. (1992). Dimensions of meaning. Proceedings
      dirichlet allocation. Journal of Machine Learning                of Supercomputing.
      Research, 3 (993-1022).                                   Smith, E., Shoben, E., & Rips, L. (1974). Structure and
Burgess, C., & Lund, K. (1997). Modeling parsing con-                  process in semantic memory: Featural model for
      straints with high-dimensional context-space. Lan-               semantic decisions. Psyhological Review, 81, 214-
      guage and Cognitive Processes, 12, 177-210.                      241.
Collins, A., & Quillian, M. (1969). Retrieval time from         Teh, Y., Jordan, M., Beal, M., & Blei, D. (2004). Hier-
      semantic memory. Journal of Verbal Learning and                  archical dirichlet processes. In Advances in neural
      Verbal Behavior, 12, 240-247.                                    information processing systems (Vol. 17).
Dagan, I., Lee, L., & Pereira, F. (1997). Similarity-           Vigliocco, G., Vinson, D. P., Lewis, W., & Garrett, M. F.
      based methods for word sense disambiguation. In                  (2004). Representing the meanings of object and
      35th annual meeting of the acl (p. 56-63).                       action words: The featural and unitary semantic
                                                                       space hypothesis. Cognitive Psychology, 48, 422-
                                                                       488.
                                                            132

