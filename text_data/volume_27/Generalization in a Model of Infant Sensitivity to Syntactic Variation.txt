UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Generalization in a Model of Infant Sensitivity to Syntactic Variation
Permalink
https://escholarship.org/uc/item/8w33d595
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 27(27)
Authors
Shultz, Thomas R.
Thivierge, Jean-Philippe
Titone, Debra
Publication Date
2005-01-01
Peer reviewed
 eScholarship.org                                    Powered by the California Digital Library
                                                                       University of California

               Generalization in a Model of Infant Sensitivity to Syntactic Variation
                                         Thomas R. Shultz (thomas.shultz@mcgill.ca)
               Department of Psychology and School of Computer Science, McGill University, 1205 Penfield Avenue
                                                     Montreal, QC H3A 1B1 Canada
                              Abstract                                its own network topology as it learns by recruiting new
                                                                      hidden units as needed. New hidden units are recruited one
   Computer simulations show that an unstructured neural-             at a time and installed each on a separate layer. The
   network model (Shultz & Bale, 2001) covers the essential           candidate hidden unit that is recruited is the one whose
   features of infant differentiation of simple grammars in an        activations correlate most highly with the current error of
   artificial language, and generalizes by both extrapolation and     the network. CC has been used to simulate many aspects of
   interpolation. Other simulations (Vilcu & Hadley, 2003)            psychological development (Shultz, 2003). For such
   claiming to show that this model did not really learn these        developmental simulations, there are a number of
   grammars were flawed by confounding syntactic patterns with
                                                                      advantages of constructive learning algorithms over static
   other factors and by lack of statistical significance testing.
   Thus, this model remains a viable account of infant ability to
                                                                      networks that only adjust connection weights, but do not
   learn and discriminate simple syntactic structures.                grow during learning (Shultz, 2005a; Shultz, Mysore, &
                                                                      Quartz, 2005).
One of the enduring debates in cognitive science concerns                Like other encoder networks, the encoder version of CC
                                                                      learns to reproduce its inputs on its output units.
the proper theoretical account for human cognition. Should
                                                                      Discrepancy between inputs and outputs is considered as
cognition be interpreted in terms of symbolic rules or
                                                                      error, which CC attempts to reduce. Infants are thought to
subsymbolic neural networks? It has been argued that                  construct an internal model of stimuli to which they are
infants’ ability to distinguish one syntactic pattern from            being exposed, and then differentially attend to more novel
another could only be explained by a symbolic rule-based              stimuli that deviate from their models. Encoder networks are
account (Marcus, Vijayan, Rao, & Vishton, 1999). After                capable of simulating this attention preference. Network
being familiarized to sentences in an artificial language             error is often used as an index of stimulus novelty in these
having a particular syntactic form (such as ABA), infants             simulations.
preferred to listen to sentences with an inconsistent syntactic          The three-word sentences used in the infant experiments
form (such as ABB). The claim about the necessity of rule-            were coded by Shultz and Bale (2001) with a continuous
based processing was promptly contradicted by a number of             sonority scale, shown in Table 1, based on previous
neural-network modelers, several of whom produced                     phonological research (Vroomen, van den Bosch, & de
unstructured models that captured the basic finding of more           Gelder, 1998). Sonority is the quality of vowel likeness and
interest in novel than familiar syntactic patterns (Altmann &         it has both acoustic and articulatory aspects. In Table 1, it
Dienes, 1999; Elman, 1999; Negishi, 1999; Shultz, 1999;               can be seen that sonorities ranged from -6 to 6 in steps of 1,
Shultz & Bale, 2001; Sirois, Buckingham, & Shultz, 2000).             with a gap and change of sign between consonants and
   However, Vilcu and Hadley (2001, 2003) reported that               vowels.
two of these simulations (Altmann & Dienes, 1999; Elman,                 Each word in the three-word sentences used in the infant
1999) could not be replicated. Vilcu and Hadley (2003)                experiments was coded on two input units for the sonority
were able to replicate the results of one simulation (Shultz          of the consonant and the sonority of the vowel. For
& Bale, 2001). But Vilcu and Hadley (2003) claimed that               example, the sentence ga ti ga was coded on the network
their extensions of this model failed to generalize, both in          inputs as (-5 6 -6 4 -5 6). The consonant /g/ was coded as -5,
                                                                      and the vowel /a/ as 6, yielding (-5 6) for the word ga,
terms of interpolation within the training range and
                                                                      which was the first and last word in this sentence. The
extrapolation outside of this range. They concluded that this
                                                                      consonant /t/ was coded as -6, and the vowel /i/ was coded
model did not really learn the grammars.                              as 4, yielding a code of (-6 4) for the ti word. Likewise the
   The present paper contains new simulations establishing            sentence ni ni la was coded on the inputs as (-2 4 -2 4 -1 6).
that this model (Shultz & Bale, 2001) does indeed learn the              The original simulation captured the essential features of
simple grammars used in the infant experiments,                       the infant data including exponential decreases in attention
interpolating and extrapolating successfully.                         to a repeated syntactic pattern, more interest in sentences
                                                                      inconsistent with the familiar pattern than in sentences
                  The Original Simulations                            consistent with that pattern, occasional familiarity
Shultz and Bale (2001) used an encoder version of the                 preferences, more recovery to consistent novel sentences
cascade-correlation (CC) learning algorithm to simulate the           than to familiar sentences, and generalization both outside
infant data. CC is a constructive algorithm for learning from         and inside of the range of the training patterns (Shultz &
examples in feed-forward neural networks (Fahlman &                   Bale, 2001).
Lebiere, 1990). Being a constructive algorithm, CC builds
                                                                  2009

     Table 1: Phoneme sonority scale used in the original           ABB, but the same principle holds there as well. It is
                         simulations.                               important, whether testing or simulating infants, to use the
    Phoneme category              Examples          Sonority        same phonemes in both patterns so as not to confound
    low vowels                    /a/ /æ/                  6        phonemes with syntactic patterns. This is because both
    mid vowels                    /Є/ /e/ /o/ /ɔ/          5        infants and artificial neural networks can be sensitive to
    high vowels                   /I/ /i/ /U/ /u/          4        both phonemic content and syntactic structure. When the
    semi-vowels and laterals /w/ /y/ /l/                  -1        two are confounded, results cannot be unambiguously
    nasals                        /n/ /m/ / ŋ /           -2        interpreted as being due to one or the other factor.
    voiced fricatives             /z/ /ʒ/ /v/             -3
    voiceless fricatives          /s/ /ʃ/ /f/             -4                        Table 2: Original test patterns.
    voiced stops                  /b/ /d/ /g/             -5                   Experiment Sentence Sonority sums
    voiceless stops               /p/ /t/ /k/             -6                   1              wo fe wo              4 1 4
Note. Example phonemes are represented in International                                       de ko de             0 -1 0
Phonetic Alphabet. From “Infant familiarization to artificial                                 wo fe fe              4 1 1
sentences: Rule-like behavior without explicit rules and                                      de ko ko             0 -1 -1
variables.” By T. R. Shultz and A. C. Bale. In L. R.                           2              ba po ba             1 -1 1
Gleitman & A. K. Joshi (Eds.), Proceedings of the Twenty-                                     ko ga ko            -1 1 -1
Second Annual Conference of the Cognitive Science Society                                     ba po po             1 -1 -1
(p. 461), 2000. Mahwah, NJ: Erlbaum. Copyright 2000 by                                        ko ga ga            -1 1 1
the Cognitive Science Society, Inc. Adapted by permission.                     3              ba ba po             1 1 -1
                                                                                              ko ko ga            -1 -1 1
                       Interpolation                                                          ba po po             1 -1 -1
                                                                                              ko ga ga            -1 1 1
Vilcu and Hadley (2003) based their critique of the Shultz
and Bale (2001) model on extended simulations that seemed
to show that the model cannot actually interpolate or                              Table 3: Modified test patterns.
extrapolate. Interpolation is the ability to generalize within
                                                                               Experiment Sentence Sonority sums
the range of the training patterns. Interpolation was tested
                                                                               1              vo fe vo              2 1 2
by introducing a phonemic change to one of the four test
patterns in each experiment. The original and modified test                                   de ko de              0 -1 0
patterns are shown in Tables 2 and 3, respectively.                                           vo fe fe              2 1 1
  These tables also include sonority sums for these                                           de ko ko             0 -1 -1
                                                                               2              ma po ma              4 -1 4
sentences, computed as the sonority of the consonant plus
the sonority of the vowel. Knowledge representation                                           ko ga ko            -1 1 -1
analyses had established that CC encoder networks                                             ma po po             4 -1 -1
represented these single-syllable words by computing such                                     ko ga ga             -1 1 1
                                                                               3              ma ma po              4 4 -1
sums, or equivalently by computing sonority differences
(Shultz & Bale, 2001). For example, a network would learn                                     ko ko ga            -1 -1 1
to code the word wo as the sonority of the consonant /w/                                      ma po po             4 -1 -1
plus the sonority of the vowel /o/: -1 + 5 = 4.                                               ko ga ga             -1 1 1
  In Table 3, the syllables changed by Vilcu and Hadley
(2003) are identified by a solid underline. Apparently their           In the present simulations, I eliminated Vilcu and
idea was to trip up the networks with a very small change of        Hadley’s confound by extending the same phonemic change
a single consonant; from wo to vo in the third test sentence        to the other syntactic pattern in each experiment, marked in
of Experiment 1, and from ba to ma in the first test sentence       Table 3 by dashed underlines. In Experiment 1, for example,
of Experiments 2 and 3. With these changes, Vilcu and               I used vo fe vo as well as vo fe fe. In Experiment 2, I tested
Hadley reported that networks could no longer distinguish           ma po po, as well as ma po ma. And in Experiment 3, I
consistent from inconsistent test patterns, although they did       included ma po po as well as ma ma po. These additional
not report any testing of statistical significance.                 changes ensure that comparisons across syntactic patterns
  However, by changing only one test pattern in each                reflect only syntactic differences and not phonemic
experiment, Vilcu and Hadley (2003) confounded phoneme              differences. Once the confounding is removed, there are
with syntactic pattern. Shultz and Bale (2001) had followed         robust differences between consistent and inconsistent test
the design of the infant experiments by using the same              patterns as in the original simulations. In each experiment,
phonemes in both consistent and inconsistent test sentences.        with eight networks per condition as with the infant
In Experiments 1 and 2, for example, two of the test                experiments, consistent test patterns showed less error than
sentences follow an ABA pattern and two follow an ABB               did inconsistent test patterns (p < .0001).
pattern. Depending on condition, one of these syntactic                Apparently reasoning along similar lines, Vilcu and
patterns is consistent with those the infant is familiar with,      Hadley (2003) reported a simulation in which they changed
whereas the other pattern is inconsistent. The syntactic            /f/ to /b/ in both the first ABA test sentence and the first
patterns in Experiment 3 are slightly different, AAB vs.            ABB test sentence of Experiment 1. Their networks failed to
                                                               2010

discriminate consistent test sentences from inconsistent test      as far outside the training range as the most extreme values
sentences, but again no statistical significance test was          used by Vilcu and Hadley (2003).
provided. I repeated that simulation with eight networks per           In one set of simulation experiments, portrayed in Table
condition and did find a significant main effect of                4, the highest vowel sonority was paired with the lowest
consistency, F(1, 15) = 5.52, p < .05, reflecting more error       consonant sonority, keeping the sonority sums for syllables
to inconsistent test sentences (M = 11.69) than to consistent      at a constant value of 0.0 in Pattern A and 3.0 in Pattern B.
test sentences (M = 9.30).                                         These simulations were characterized by a negative
   Moreover, I could not replicate Vilcu and Hadley’s              correlation between consonant and vowel sonority values,
(2003) finding of a lack of discrimination between                 which can be seen in Table 4 by ignoring the first, anchor
consistent and inconsistent test patterns even using their         row.
single-pattern changes that confound phoneme with
syntactic pattern. In each of the three experiments, run with          Table 4: Test patterns for evaluating extrapolation in the
20 networks per condition to increase statistical power, there          simulation of Experiment 1: Highest vowel paired with
was less network error to consistent test patterns than to                         lowest consonant and vice versa.
inconsistent test patterns, p < .0001.                                                       Pattern A              Pattern B
   These new results contradict Vilcu and Hadley’s (2003)         Distance              Consonant Vowel Consonant Vowel
claim that the Shultz and Bale (2001) networks do not             Original anchors             -6.0      6.0          -1.0       4.0
interpolate successfully. With properly controlled tests,         Inside +-0.5                 -5.5      5.5          -1.5       4.5
interpolation ability is reliable and strong. Moreover, even      Close +-0.5                  -6.5      6.5          -0.5       3.5
with the confounding introduced by Vilcu and Hadley, the          Far +-1.0                    -7.0      7.0           0.0       3.0
networks still interpolate well. The lack of statistical          Farther +-2.0                -8.0      8.0           1.0       2.0
analysis in Vilcu and Hadley’s research may have obscured         Even farth. +-3.0            -9.0      9.0           2.0       1.0
differences between familiar and novel test patterns.             Farthest +-4.0             -10.0      10.0           3.0       0.0
                         Extrapolation                                 In another set of simulations, shown in Table 5, the vowel
To test the for extrapolation outside of the sonority training      columns in Table 4 were switched, pairing the highest
range in the Shultz and Bale model, Vilcu and Hadley                vowel with the highest consonant. Here the correlation
(2003) assigned four consonant values beyond the minimum            between consonant and vowel sonority values is positive,
value of -6 (i.e., -7, -8, -9, -10), and combined them with         which can be seen in Table 5 by ignoring the first, anchor
two vowel values beyond the maximum value of 6 (i.e., 7,            row. In this set of simulations, the sonority sums of the
8). They found that networks showed more error to                   syllables were allowed to vary with distance from the
consistent test patterns than to inconsistent test patterns, a      training range. Both sets of simulations focused on
direction opposite to that of both the infants and the              Experiment 1 and used eight networks per condition as in
networks. However, once again Vilcu and Hadley did not              the infant study. It was unclear whether these two different
test the statistical reliability of this difference.                pairing methods for creating test patterns would produce
   A major problem with testing outside the sonority range is       different results, so it seemed appropriate to run the
that such extreme values do not correspond to the sounds in         simulations both ways.
human languages. Testing network generalization in this
way is thus somewhat irrelevant to simulations of                      Table 5: Test patterns for evaluating extrapolation in the
psychology.                                                             simulation of Experiment 1: Highest vowel paired with
   Furthermore, in arguing that networks fail to extrapolate                       highest consonant and vice versa.
beyond the training range, Vilcu and Hadley ignored the                                     Language A             Language B
Shultz and Bale (2001) results showing that with less             Distance              Consonant Vowel Consonant Vowel
extreme deviations beyond the limits of the training range,       Original anchors             -6.0      4.0          -1.0       6.0
networks do successfully extrapolate, with the consistency        Inside +-0.5                 -5.5      4.5          -1.5       5.5
effect growing significantly larger with more extreme (i.e.,      Close +-0.5                  -6.5      3.5          -0.5       6.5
+-7) as compared to less extreme (i.e., +-6.5) sonorities.        Far +-1.0                    -7.0      3.0           0.0       7.0
Here I report on a replication of the Shultz and Bale             Farther +-2.0                -8.0      2.0           1.0       8.0
extrapolation results and extend the study to the more            Even farth. +-3.0            -9.0      1.0           2.0       9.0
extreme sonority values used by Vilcu and Hadley (2003).          Farthest +-4.0             -10.0       0.0           3.0      10.0
   The sonority values I used are shown in Tables 4 and 5,
along with a reminder of the original training anchor values
used by Shultz and Bale (2001). As in Shultz and Bale                  In each of the two simulations, test error was subjected to
(2001), I included test values inside the training range (by        a mixed ANOVA in which familiarization condition served
+-0.5) and values that were outside of this range but close to      as a between-network factor and consistency and distance
it (by +-0.5) or far from it (by +-1.0). There were three           served as repeated measures. In both experiments there were
additional sonority values ranging farther outside of the           significant main effects of consistency and distance as well
training range in steps of +-1.0, labeled in Tables 4 and 5 as      as an interaction between them, p < .0001. The relevant
farther, even farther, and farthest. The farthest values were       means are presented in Figures 1 and 2 for constant and
                                                                    varying sonority sums, respectively. Note that extrapolation
                                                               2011

is involved at all distances from the training range except                                   model because that is the simulation they could replicate
for the condition labeled inside. As in the Shultz and Bale                                   that covered the Marcus et al. infant findings. Because Vilcu
(2001) simulations, error increased with distance from the                                    and Hadley’s extensions of the Shultz and Bale model failed
training range, error was greater to inconsistent than to                                     to generalize to novel sentences, in terms of both
consistent test patterns at each distance, and the consistency                                interpolation and extrapolation, they concluded that this
effect was larger with increasing distance.                                                   model does not really learn the grammars.
                                                                                                 However, results presented here showed that, by not
                                                                                              confounding phoneme and syntactic pattern as in Vilcu and
                                         Consistent       Inconsistent                        Hadley’s experiments, there was robust interpolation and
                200
                                                                                              extrapolation, in the form of reliable differences between
                                                                                              consistent and inconsistent test patterns. Moreover, even
                150                                                                           with Vilcu and Hadley’s confounds left in, these effects
  Mean error
                100
                                                                                              were still reliable by conventional statistical tests. The
                                                                                              inadvertent experimental confounds and lack of statistical
                 50                                                                           significance tests in Vilcu and Hadley’s research appeared
                  0                                                                           to obscure reliable differences between test patterns, thus
                      Inside   Close         Far       Farther         Even     Farthest      leading to underestimation of network ability to learn these
                                                                      farther                 simple grammars. If generalization by interpolation and
                                       Distance from training range                           extrapolation is the sine qua non for grammar learning, then
                                                                                              these networks did learn these simple grammars.
  Figure 1: Mean error to consistent and inconsistent test                                       To be fair and complete, Vilcu and Hadley (2003) raised
patterns at various distances from the training range, where                                  another argument against these network models besides the
                sonority sums were constant.                                                  alleged generalization difficulties. They also argued that the
                                                                                              Shultz and Bale (2001) networks only learned the numerical
  These results indicate that the Shultz and Bale networks                                    contours of the artificial sentences, and not the syntactic
interpolate and extrapolate very well. Error increases with                                   relations involving the duplicated words. Vilcu and Hadley
distance outside the training range because the networks do                                   supported this argument with a simulation in which sonority
not recognize the particular novel phonemes and syllables                                     contours of the familiar ABA sentences always formed a
being presented. But even with very novel sounds, the                                         peak, whereas sonority contours of the test sentences could
networks are sensitive to the relative syntactic novelty of the                               form either a peak or a valley. When sonority contours of
sentences. As noted, outside of the range of human speech                                     test sentences formed a peak, then there was the usual
sounds, it is difficult to design realistic tests of the model’s                              novelty preference; but when sonority contours of the test
predictions, but the present results provide in-principle                                     sentences formed a valley, then there appeared to be a
evidence of network extrapolation ability.                                                    familiarity preference. Again, there were no tests of
                                                                                              statistical significance.
                                                                                                 Although this appears to suggest that networks are
                                         Consistent      Inconsistent                         sensitive to input contours and not syntax, it ignores the fact
                                                                                              that, in both the infant experiments and the Shultz and Bale
                250
                                                                                              (2001) simulations, sonority contours were balanced within
                200
                                                                                              each language rather than confounded with syntax. The
  Mean errror
                150                                                                           contours of these familiar sentences were not simply
                100                                                                           sonority peaks or sonority valleys as Vilcu and Hadley
                50                                                                            (2003) suggested, but rather a complex combination of
                 0
                                                                                              sonority-sum contours containing peaks, valleys, and
                      Inside   Close         Far       Farther         Even     Farthest
                                                                                              plateaus in Experiments 1 and 2, and increases, decreases,
                                                                      farther                 and plateaus in Experiment 3 (Shultz & Bale, 2005b).
                                       Distance from training range                              In the ABA familiarization condition of Experiment 1,
                                                                                              eight of the training sentences formed a sonority-contour
  Figure 2: Mean error to consistent and inconsistent test                                    peak and the other eight formed a sonority-contour valley
patterns at various distances from the training range, where                                  (Figure 3). In the ABB condition of that experiment, eight
             sonority sum was allowed to vary.                                                of the familiar sentences showed an increasing sonority
                                                                                              contour and the other eight showed a decreasing sonority
                                       Discussion                                             contour (Figure 4). The same was true of Experiment 2
                                                                                              except that two of the ABA (Figure 5) and two of the ABB
Vilcu and Hadley’s (2003) critique of neural-network
                                                                                              (Figure 6) familiar sentences showed a completely flat
models of infant learning of artificial grammars is important
                                                                                              sonority profile. Both the AAB (Figure 7) and ABB (Figure
because it addresses a debate that has dominated cognitive                                    6) familiar sentences of Experiment 3 had a similar mix of
science for the last 20 years – whether human cognition is                                    sonority contours: seven had an increasing contour, seven a
better explained by symbolic rules or subsymbolic                                             decreasing contour, and two a flat profile. For simplicity,
connections. They focused on the Shultz and Bale (2001)                                       Figures 3-7 all show schematic sonority profiles.
                                                                                           2012

                                                                   but infants might well be sensitive to sonority contours.
                                                                   Sonority contours might help the infant identify syllable
                                                                   boundaries which might, in turn, facilitate word
                                                                   identification. If so, this would be a difficult pattern of
   Sonority sum
                                                                   results for a purely syntactic, symbolic model to account for.
                                                                        Sonority sum
                  1              2                 3
                           Word position
  Figure 3: Sonority profiles in the training patterns of the
      ABA familiarization condition of Experiment 1.                                   1              2                3
                                                                                                Word position
                                                                       Figure 6: Sonority profiles in the training patterns of the
                                                                       ABB familiarization conditions of Experiment 2 and 3.
   Sonority sum
                                                                        Sonority sum
                  1              2                3
                           Word position
  Figure 4: Sonority profiles in the training patterns of the
      ABB familiarization condition of Experiment 1.                                   1              2                3
                                                                                                Word position
                                                                       Figure 7: Sonority profiles in the training patterns of the
   Sonority sum
                                                                           AAB familiarization condition of Experiment 3.
                                                                     Networks learned to decode representations of the two
                                                                   duplicate words in a sentence by using highly similar sets of
                                                                   weights entering the output units that represent the duplicate
                                                                   words (Shultz & Bale, 2001, 2005b). This virtually identical
                                                                   pattern of weights entering the output units representing the
                  1              2                 3               duplicate words allowed the network to recognize the near
                           Word position                           identity of the duplicate words.
                                                                     The relatively large connection weights to the duplicated-
                                                                   word outputs from the first hidden unit showed that this
  Figure 5: Sonority profiles in the training patterns of the      hidden unit recognized the category (A or B) of these
      ABA familiarization condition of Experiment 2.               duplicate words. The second hidden unit performed the
                                                                   complementary job of recognizing the category of the single
  To deal with this complex mix of contours, the networks          word, as indicated by its relatively large weights to outputs
(and presumably the infants) discovered near-identity              representing that single word.
relations to differentiate the syntactic patterns of old vs. new     Analyses of hidden-unit activations showed that the first
sentences. In none of these experiments was it sufficient to       hidden unit learned to encode the sonority sum of the
learn a single sonority profile as suggested by Vilcu and          duplicated words, and the second hidden unit learned to
Hadley.                                                            encode the sonority sum of the single word. This means that
  It is unknown how infants would perform in an                    the duplicated words were being treated in similar fashion.
experiment with Vilcu and Hadley’s confounds between                 Additional analysis of network knowledge representations
syntactic pattern and sonority contour in familiar sentences,      used     principle-component      analyses     of     network
                                                                2013

contributions. Network contributions are products of              Fahlman, S. E., & Lebiere, C. (1990). The cascade-
sending-unit activations and connection weights entering the        correlation learning architecture. In D. S. Touretzky (Ed.),
output units. They effectively summarize all of the                 Advances in Neural Information Processing Systems 2
information used by the network to generate its outputs             (pp. 524-532). Los Altos, CA: Morgan Kaufmann.
(Shultz, Oshima-Takane, & Takane, 1995). This analysis            Marcus, G. F., Vijayan, S., Bandi Rao, S., & Vishton, P. M.
revealed two components, one representing sonority                  (1999). Rule learning by seven-month-old infants.
variation in the duplicate-word category and the other              Science, 283, 77-80.
representing sonority variation in the single-word category.      Negishi, M. (1999). Do infants learn grammar with algebra
This provides additional evidence that the networks learned         or statistics? Science, 284, 433.
to treat the duplicate words in a nearly identical fashion.       Shultz, T. R. (1999). Rule learning by habituation can be
  All of this is not to say that these rather simple networks       simulated in neural networks. Proceedings of the Twenty-
could acquire the full grammar of a human language. It is           first Annual Conference of the Cognitive Science Society
certain that some aspects of human syntactic acquisition            (pp. 665-670). Mahwah, NJ: Erlbaum.
would require different and more powerful models. But the         Shultz, T. R. (2001). Assessing generalization in
ability of the Shultz and Bale (2001) networks to master the        connectionist and rule-based models under the learning
simple artificial grammars used by Marcus et al. (1999) with        constraint. Proceedings of the Twenty-third Annual
infants is well established. Indeed, these unstructured neural      Conference of the Cognitive Science Society (pp. 922-
networks can learn these grammars more effectively and              927). Mahwah, NJ: Erlbaum.
generalize better than a leading symbolic rule-learning           Shultz, T. R. (2003). Computational developmental
algorithm, C4.5 (Shultz, 2001).                                     psychology. Cambridge, MA: MIT Press.
  An interesting feature of this controversy is that it can be    Shultz, T. R. (2005a, in press). Constructive learning in the
surprisingly difficult to replicate computer simulations.           modeling of psychological development. In Y. Munakata
Vilcu and Hadley (2003) were unable to replicate the results        & M. H. Johnson (Eds.), Processes of change in brain
of two other connectionist simulations of the infant data.          and cognitive development: Attention and performance
Also, the present paper reveals that I could not replicate the      XXI. Oxford: Oxford University Press.
results of some of the Vilcu and Hadley simulations. It is        Shultz, T. R., & Bale, A. C. (2005b). Neural networks
commonplace that human or animal results cannot always              discover a near-identity relation to distinguish simple
be replicated, but the notion that replication can be a             syntactic forms. Submitted for publication.
problem with computer simulations seems novel. The                Shultz, T. R., & Bale, A. C. (2001). Neural network
mathematical and computational precision of these models            simulation of infant familiarization to artificial sentences:
have led many to assume that replication of results would           Rule-like behavior without explicit rules and variables.
not be a problem. The numerous non-replications uncovered           Infancy, 2, 501-536.
in this relatively small literature suggest that researchers      Shultz, T. R., Mysore, S. P., & Quartz, S. R. (2005, in
should perhaps replicate simulations routinely. In this             press). Why let networks grow? In D. Mareschal, S.
context, it should be remembered that several other                 Sirois, & G. Westermann (Eds.), Constructing cognition:
unstructured network simulations of the infant data have not        Perspectives and prospects. Oxford: Oxford University
been shown to be difficult to replicate (Negishi, 1999;             Press.
Shultz, 1999; Sirois, Buckingham, & Shultz, 2000).                Shultz, T. R., Oshima-Takane, Y., & Takane, Y. (1995).
  Another important lesson of this exercise is that, even           Analysis of unstandardized contributions in cross
with computer simulations, it is important to use statistical       connected networks. In D. Touretzky, G. Tesauro, & T.
tests to evaluate the significance and reliability of results.      K. Leen, (Eds). Advances in Neural Information
Such tests are particularly critical with neural network            Processing Systems 7 (pp. 601-608). Cambridge, MA:
models, because of their stochastic properties. It is not           MIT Press.
always sufficient to rely on visual comparisons of means.         Sirois, S., Buckingham, D., & Shultz, T. R. (2000).
                                                                    Artificial grammar learning by infants: An auto-associator
                    Acknowledgments                                 perspective. Developmental Science, 4, 442-456.
                                                                  Vilcu, M., & Hadley, R. F. (2001). Generalization in simple
Thanks to the Natural Sciences and Engineering Research
                                                                    recurrent networks. Proceedings of the Twenty-third
Council of Canada for financial support and to Alan Bale
                                                                    Annual Conference of the Cognitive Science Society (pp.
for helpful discussions.
                                                                    1072-1077). Mahwah, NJ: Erlbaum.
                                                                  Vilcu, M., & Hadley, R. F. (2003). Two apparent
                         References                                 “counterexamples” to Marcus: A closer look. Proceedings
Altmann, G. T. M., & Dienes, Z. (1999). Rule learning by            of the Twenty-fifth Annual Conference of the Cognitive
  seven-month-old infants and neural networks. Science,             Science Society (pp. 1188-1193). Mahwah, NJ: Erlbaum.
  284, 875.                                                       Vroomen, J., van den Bosch, A., & de Gelder, B. (1998). A
Elman, J. L. (1999). Generalization, rules, and neural              connectionist model for bootstrap learning of syllabic
  networks: A simulation of Marcus et al. [Online].                 structure. Language and Cognitive Processes, 13, 193-
  Retrieved April 27, 1999 from the World Wide Web:                 220.
  http//www.crl.ucsd.edu/~elman/Papers/MVRVsim.html
                                                              2014

