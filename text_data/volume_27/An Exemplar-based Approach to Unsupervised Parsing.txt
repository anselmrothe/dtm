                             S
                                                                   PRP$ NN VBD JJ         1
                                                                   NN VBD JJ              0
               NP                         VP                       VBD JJ                 2
                                                                   Total                  3         PRP$
                                                                                                              NN
    His/PRP$       dog/NN      was/VBD        big/JJ                                                               VBD    JJ
Figure 1. The correct parse of the sentence “His dog was
big”. PRP$ = personal pronoun, NN = Noun, VBD = Verb,
past tense, JJ = Adjective.                                        PRP$ NN VBD JJ         1
                                                                   PRP$ NN                1
                                                                   VBD JJ                 2
0.0000-|PRP$-NN-VBD-JJ|                                            Total                  4
           |PRP$-NN-VBD-JJ|                                                                         PRP$      NN   VBD    JJ
0.0011-|PRP$|NN|--VBD-JJ-|
           |PRP$|NN|MD-VB-VBN|
                                                                   PRP$ NN VBD JJ         1
0.0017-|PRP$|NN|-VBD-JJ|                                           PRP$ NN VBD            0
           |PRP$|NN|VBD-VBN|                                       PRP$ NN                1                               JJ
                                                                   Total                  2
0.0018-|PRP$-NN-|VBD|JJ|                                                                                           VBD
           |DT-NN-NN|VBD|JJ|                                                                        PRP$      NN
Figure 2.     The minimum cost alignments and the cor-          Figure 3. Possible binary parses of “His dog was big.” and
responding costs for the four exemplars most similar to         their associated counts. In this case, parse number two would
“His/PRP$ dog/NN was/VBD big/JJ”. Note aligning mul-            be chosen.
tiple exemplars against a target sentence can approximate a
traditional parse. MD = Modal verb, VB = Verb, VBN =
Verb, past participle, DT = Determiner.
                                                                    PRP$ NN VBD JJ 1
Figure 1).                                                          PRP$ NN VBD                0
   We start by converting the sentence to a part                    NN VBD JJ                  0
of speech (POS) sequence - “PRP$ NN VBD JJ”,                        PRP$ NN                    1
where PRP$ = possessive pronoun, NN = noun,                         PRP$ VBD                   0
VBD = past tense verb and JJ = adjective. Next
we identify near neighbour POS sequences from a                     VBD JJ                     2
large corpus and align each of these with the sen-                 Figure 3 shows the three possible binary parses
tence (see Figure 2). In this case, we are using the            of the example sentence and the counts of the asso-
34,000 POS sequences that appeared at least twice               ciated spans. In this case, the second parse would
in the first 350,000 sentences from the TASA cor-               be chosen as it has the highest total span count.
pus1 . The number to the left of each alignment
is the corresponding span-based edit distance (de-                        Definitions of Edit Distances
fined below). Note that these alignments induce                 The above algorithm relies on selecting alignments
constituent structure. In this case, for example, we            between POS tag sequences. In the following sec-
would propose that PRP$-NN should constitute one                tions, we introduce the notion of span-based nor-
constituent and VBD-JJ another.                                 malized edit distance (SNED) to play this role.
   While not constrained to be tree-like this struc-
ture may tend to correspond to a tree for many                  Edit Distance
structurally unambiguous cases. However, for the                Following the notation of Marzal and Vidal (1993),
purposes of testing against gold standard parses                let Σ be a finite alphabet and Σ∗ be the set of all
from the treebank, we induce a tree by determining              finite-length strings over Σ. Let X = X1 X2 ...Xn be
the number of times each span of POS tags was                   a string of Σ∗ , where Xi is the ith symbol of X. We
identified by the model as a constituent. The                   denote by Xi...j the substring of X that includes the
binary parse with the highest total constituent                 symbols from Xi to Xj , 1 ≤ i, j ≤ n. The length of
count is then chosen using the obvious dynamic                  such a string is |Xi...j | = j − i + 1. If i > j, Xi...j is
programming algorithm.             In the example, the          the null string λ, |λ| = 0.
nonsingleton spans have the following counts:                      An elementary edit operation is a pair (a, b) 6=
    1
      We thank the late Stephen Ivens and Touchstone Ap-        (λ, λ), where a and b are strings of length 0 or
plied Science Associates (TASA) of Brewster, New York for       1. The edit operations are termed insertions (λ, b),
providing this valuable resource.                               substitutions (a, b) and deletions (a, λ). An edit
                                                            584

transformation of X into Y is a sequence S of ele-                Spans                     Contexts
mentary operations that transforms X into Y . Typ-                PRP$ NN VBD JJ            SS:EE
ically, edit operations have associated costs γ(a, b).            PRP$ NN VBD               SS:big
The function γ can be extended to edit transforma-
                                           P                      NN VBD JJ                 his:EE
tions S = S1 S2 ...Sl by letting γ(S) = li=1 γ(Si ).
                        ∗     ∗
   Given X, Y ∈ Σ and SXY the set of all edit trans-              PRP$ NN                   SS:was
formations of X into Y , then the edit distance is                NN VBD                    his:big
defined as:                                                       VBD JJ                    dog:EE
                                          ∗                       PRP$                      SS:dog
             δ(X, Y ) = min{γ(S)|S ∈ SXY     }         (1)
                                                                  NN                        his:was
   Note that the triangle inequality is a consequence             VBD                       dog:big
of this definition, so provided γ(a, a) = 0, γ(a, b) >
0, if a 6= b, and γ(a, b) = γ(b, a)∀a, b ∈ Σ ∪ {λ}, δ is          JJ                        was:EE
a metric.
                                                                Figure 4.     Spans and associated contexts for the string
   Dynamic programming algorithms of complexity                 His/PRP$ dog/NN was/VBD big/JJ. Note SS and EE are
O(mn), where n is the length of X and m is the                  tags indicating the start and end of the sentence, respectively.
length of Y , exist to calculate edit distance and to
retrieve minimal edit transformations (Wagner &
Fischer, 1974).
                                                                Calculating POS Span Costs
Normalized Edit Distance                                        In order to apply the SNED algorithm one requires
Let L(S) be the length of a given edit transforma-              a γ function that indicates the cost of substituting
tion. Then the normalized edit distance defined by              one string of POS tags for another. To calculate
Marzal and Vidal (1993) is:                                     substitution costs we combined ideas from the Con-
                                                                text Constituent Model (CCM, Klein & Manning,
                                             ∗                  2001) and the Topics model (Griffiths & Steyvers,
          d(X, Y ) = min{γ(S)/L(S)|S ∈ SXY       }     (2)
                                                                2002). The Topics model is a probabilistic genera-
   Note that normalized edit distance is not a met-             tive model in which documents (i.e. contexts) are
ric. It can, however, be calculated in O(nm2 ) time             assumed to generate topics which in turn generate
using an algorithm provided by Marzal and Vidal                 words. A document, then, is defined by its mix-
(1993).                                                         ture distribution of topics and a topic is defined
   Marzal and Vidal (1993) also show that NED does              by its mixture distribution of words. The Topics
not produce the same answer as postnormalizing,                 model assumes that these distributions are Dirich-
by finding the minimum path and dividing by its                 let (see also Latent Dirichlet Allocation, Blei, Ng,
length. Furthermore, for a handwritten character                & Jordan, 2002) and employs are Markov Chain
recognition task, normalized edit distance produced             Monte Carlo method to estimate the required con-
better performance than either standard edit dis-               ditional probabilities from a corpus (see Griffiths &
tance or post normalized edit distance.                         Steyvers, 2002, for a deeper coverge of the model).
Span-based Normalized Edit Distance                             In our application, we employ the same mechanism
(SNED)                                                          but assume that word contexts (i.e. the words im-
                                                                mediately before and after a given span) generate
Any viable theory of sentence processing must ac-               topics which in turn generate POS spans3 . For in-
count for the way in which people form constituents             stance, the example sentence, His/PRP$ dog/NN
from series of words in sentences. The evidence for             was/VBD big/JJ, would generate the spans and
the phrasal structure of sentences is extensive and             contexts shown in Figure 4.
is of multiple types including phonological, mor-                  Note that spans that tend to substitutable for
phonological, semantic and syntactic (see Radford,              each other will have similar sets of contexts (see
1988, for a summary). Consequently, when analyz-                Redington, Chater, & Finch, 1998; Dennis, 2003a,
ing sentence structure we would prefer a version of             for similar insights at the lexical level). For in-
the normalized edit distance algorithm that aligns              stance, we might expect the pattern of contexts in
spans of symbols rather than individual symbols.                which we find VBD JJ to be similar to the pattern
Providing a definition of span-based edit distance              in which we find MD VBD VBN as they are both
involves relaxing the restriction in the standard al-           verb phrases.
gorithm, so that the strings a and b are drawn
from Σ∗ . So, the edit operations become (a, b) =                  Each POS span is associated with a distribution
(Xi...j , Yk...l ) for 0 ≤ i ≤ j ≤ n, 0 ≤ k ≤ l ≤ m.            as an alternative formulation. We employ the obvious dy-
Similarly, one can define span-based normalized edit            namic programming algorithm, which has time complexity
distance in an analogous way2 .                                 O(m3 n2 ), where m and n are the lengths of the strings.
                                                                   3
                                                                     In the Context Constituent model both spans and con-
   2
     For the current purposes, we assume that a, b 6= λ al-     texts are defined in terms of POS tags. We found, however,
though it would be useful to draw a and b from Σ∗ ∪ {λ}         that using words for the contexts improved performance.
                                                            585

over topics (i.e. P (t|s) where t is the topic and                                   search will have a significant impact on the per-
s the span). Calculating a similarity between                                        formance of the algorithm as a whole. In our case,
POS spans, then, can be achieved by comparing                                        it is sufficient to have a set of approximate nearest
these distributions. A number of measures are                                        neighbours, so we use a version of Locality Sensitive
possible. We chose to take the dot product of the                                    Hashing (LSH, Indyk & Motwani, 1998; Gionis et
distributions, which is equivalent to the probability                                al., 1999) adapted to work in Σ∗ rather than in <d
that independent topic samples from each of the                                      as is typical.
distributions would be identical:                                                       The basic idea of LSH is to create multiple hash
                      P                                                              functions each of which is designed so that similar
  γ(s1 , s2 ) =              i P (ti |s1 )P (ti |s2 )                                sequences are likely to collide. Finding the nearest
                                                                                     neighbours of a target string involves applying the
  Figure 5 shows a hierarchical cluster solution for                                 hash functions to the new case and accumulating
the vectors corresponding to the 60 most frequent                                    the strings that appear in the corresponding buck-
spans. Note that there is clear similarity structure                                 ets.
with spans representing sentences, verb phrases, N                                      To create the hash functions in Σ∗ we create a
and N structures well separated.                                                     set of rewrite rules that map one POS sequence to
                                                                                     a simpler one. Different hash functions are created
                                              Height                                 by permuting the rewrite rules. For example,
                        0.0          0.2         0.4       0.6        0.8    1.0     suppose we have the exemplar sentence “Her little
                                                                                     dolly felt sad.”, which translates to PRP$ JJ NN
                   NN
               JJ−NN
                                                                                     VBD JJ, in our corpus and we wish to find the
               NN−NN
                      NNS
                                                                                     nearest neighbours of the target sentence “His dog
                     NN−NNS
                       NN−VBD                                                        was big.” (PRP$ NN VBD JJ). Further, suppose
                             NN−VBZ
                                NNS−VBD                                              that we have the following rewrite rules JJ NN →
                               NNP
                                           JJ−NNS
                                                   NNS−VBP                           NN, PRP$ NN → NN, DT NN → NN. Let:
           DT−NN
             PRP
         DT−NNS
              DT−JJ−NN
                                                                                     h1 = [JJ NN → NN, PRP$ NN → NN, DT NN → NN]
                   NNP−NNP
                     NNP−NNP−NNP                                                     h2 = [PRP$ NN → NN, DT NN → NN, JJ NN → NN]
                                   CD−NN
                                    DT−NNP
                                                                                        Now for the two strings we get the following keys:
                                         CD−NNS
                    DT
                 PRP$
                        DT−JJ
                                        CD
                                                  CD−CD
                  IN−NNP
                                   RB
                                                                                                              Target
               IN−DT−NN
                    IN−NN
                      TO−VB
                                      IN−CD
                                                                                     h1(PRP$ NN VBD JJ) = NN VBD JJ
                                VB
                                                              IN−JJ                  h2(PRP$ NN VBD JJ) = NN VBD JJ
                            RB−VB
                                                     CC
        PRP−VBZ
        PRP−VBD
         PRP−VBP                                                                                            Exemplar
         NNP−VBZ
           NNP−VBD
                       IN                                                            h1(PRP$ JJ NN VBD JJ) = NN VBD JJ
                 NNS−IN
                     NN−IN                                                           h2(PRP$ JJ NN VBD JJ) = PRP$ NN VBD NN
                       DT−NN−IN
                                         VBN−IN
                           JJ
                        VBN
                                  VBG
                                                                                        Because the two strings have a hash key in com-
            VBD
                                                                      IN−DT
                                                                                     mon, string two will be found when the system is
          MD−VB
              VBP                                                                    queried with string one. In practice, locality sen-
                                                                                     sitive hashing is fast and is not greatly affected by
                VBZ
                         VBZ−RB
                              VBD−RB
                                  MD
                                                         VBD−CD                      the size of the corpus. In our trials, we constructed
                              TO−CD
                                                                    VBZ−DT
                                                                                     a five hash system with hash functions containing
                                                                          TO
                                                                                     200 rewrite rules that were selected by taking the
                                                                                     most similar POS span pairs that mapped a longer
                                                                                     span into a shorter one.
Figure 5. Hierarchiical clustering solution for the vectors                                         Evaluating the model
corresponding to the 60 most frequent spans.
                                                                                     The procedure outlined above was applied to all of
                                                                                     the sentences from the Wall Street Journal section
                                                                                     of the Penn treebank (Marcus et al., 1993) that
Finding Nearest Neighbors                                                            were of length 10 or less. To assess performance
A final issue to be resolved is how the algorithm                                    the parses produced by the model were compared
selects nearest neighbour sequences to align. Given                                  against the gold standard parses provided by the
that there may be large numbers of potential sen-                                    treebank. Three measures were calculated:
tences the performance of the nearest neighbour
                                                                                 586

   • Unlabelled Recall: The mean proportion of
constituents in the gold standard that the model
                                                                  1.0
proposed.                                                                            Random
   • Unlabelled Precision: The mean proportion of                                    SNED
                                                                                     SNED30
constituents in the models answer that appear in                                     Klein Manning (Greedy with Tags)
                                                                                     Klein Manning (CCM)                Optimal F1
the gold standard.                                                                   Klein Manning (CCM with Tags)
   • F1 : The harmonic mean of unlabelled recall
and unlabelled precision.                                         0.8
   Because the treebank provides parses that are
not binary, but the procedure used makes this as-
sumption it is not possible to achieve perfect perfor-
mance. Klein and Manning (2001) calculated that
the best possible F1 measure that can be achieved                 0.6
is 88.1%.
   Figure 6 shows the performance of the model
against chance selection of trees and against three
versions of the Constituent Context Model (CCM)
proposed by Klein and Manning (2001). Clearly,                    0.4
all of these models are performing well above chance
with the performance of SNED close to other meth-
ods such as the context constituent model (Klein &
Manning, 2001).
   A key issue in the performance of the model is the
number of nearest neighbours that are returned by                 0.2
the locality sensitive hashing algorithm. A signifi-
cant number of sequences had no nearest neighbours
and as a consequence performance on these exam-
ples is likely to be compromized. Figure 7 shows
the impact of restricting the analysis to the items               0.0
that return nearest neighbour sets of different sizes.                      Recall                  Precision              F1
If the SNED based algorithm is restricted to those
sequences for which at least 30 neighbours are re-
turned (i.e. SNED30) performance is close to the
theoretically achievable maximum of 88.1%. Note,
however, that one must interpret this figure care-             Figure 6.    Results of Unsupervised Parsing Experiment.
fully as it is also possible that there is a selection         Note SNED 30 refers to performance calculated over those
                                                               sequences for which there were at least 30 neighbours.
effect that is inflating these results.
                   Conclusions
                                                               et al., 1998), but also at the word span level. This
The version of the Syntagmatic Paradignmatic                   suggests that unsupervised parsing will be feasible
model (Dennis, in press, 2004) presented in this               thus withdrawing one of the key, if unstated, argu-
paper provides a demonstration of an exemplar-                 ments in favour of the nativist account of language
based approach to syntax. Many of the most in-                 acquisition.
fluential models in memory (Shiffrin & Steyvers,
1997), learning (Logan, 1988), decision-making                                   Acknowledgments
(Dougherty, 1999), phonology (Nakisa & Plunkett,
1998), lexical access (Goldinger, 1998), and catego-           We would like to acknowledge the many discussions
rization (Nosofsky, 1986) are exemplar-based, but              that have influenced the current work. In particu-
models of this kind have played a much less signif-            lar, we would like to thank Dan Jurafsky, Jim Mar-
icant role in cognitive models of syntax (although             tin, Walter Kintsch, Tom Landauer and Jose Que-
see Daelesmans, 1999, for examples in the com-                 sada for helpful comments and suggestions. This re-
putational lingusitics literature). Furthermore, the           search was supported by Australian Research Foun-
syntagmatic paradigmatic approach has been used                dation Grant A00106012, U.S. National Science
to extract proposition-like information from corpora           Foundation Grant EIA-0121201 and U.S. Depart-
(Dennis, 2004, in press) and consequently seems to             ment of Education Grant R305G020027.
provide a useful unifying framework.
   Perhaps more critically, however, the results pre-                                    References
sented in this paper demonstrate that significant              Blei, D. M., Ng, A. Y., & Jordan, M. I. (2002). Latent
grammaical structure can be extracted from a nat-                    dirichlet allocation. In Nueral information porcessing
ural corpus, not just at the word level (Redington                   systems (Vol. 14). Lawrence Erlbaum Associates.
                                                         587

                                                                                 Elman, J. L. (1991). Distributed representations, simple
                                                                                     recurrent networks and grammatical structure. Machine
                                                                                     Learning, 7, 195-225.
             1.0
                                                                                 Elman, J. L. (1999). The origins of language: A conspiracy
                                                                                     theory. In B. Mc Whinney (Ed.), The emergence of
                                                                                     language. Hillsdale, NJ: Lawrence Erlbaum Associates.
                                                                                 Gionis, A., Indyk, P., & Motwani, R. (1999). Similarity
             0.9                                                                      search in high dimensions via hashing. In The VLDB
                                                                                      journal (p. 518-529).
                                                                                 Goldinger, S. D. (1998). Echoes of echoes? an episodic theory
                                                                                      of lexical access. Psychological Review, 105 (2), 251-279.
             0.8                                                                 Griffiths, T. L., & Steyvers, M. (2002). Prediction and se-
                                                                                       mantic association. In Nips.
Proportion
                                                                                 Hadley, R. F. (1994). Systematicity in connectionist language
                                                                                      learning. Mind and language, 9 (3), 247-272.
             0.7                                                                 Harrington, M., & Dennis, S. (2003). Structural priming in
                                                                                      sentence comprehension. In Twenty fifth conference of
                                                                                      the cognitive science society (Vol. 25). Lawrence Erl-
                                                     Recall
                                                                                      baum Associates.
                                                     Precision
                                                     F1                          Indyk, P., & Motwani, R. (1998). Approximate nearest neigh-
             0.6                                                                      bors: towards removing the curse of dimensionality. In
                                                                                      (pp. 604–613).
                                                                                 Klein, D., & Manning, C. D. (2001). Distributional phrase
                                                                                      structure induction. In W. Daelemans & R. Zajac
                                                                                      (Eds.), Connl-2001 (p. 113-120). Toulouse, France.
             0.5
                                                                                 Logan, G. D. (1988). Towards an instance theory of autom-
                                                                                     atization. Psychological Review, 95, 492-527.
                   0   10        20             30               40   50
                            Number of Nearest Neighbours                         Marcus, M., Kim, G., Marcinkiewicz, M. A., MacIntyre, R.,
                                                                                     Bies, A., Ferguson, M., et al. (1993). Building a large
                                                                                     annotated corpus of english: The penn treebank. Com-
                                                                                     putational Linguistics, 19 (2), 313-330.
Figure 7. Performance as a function of the number of near-
est neighbours. Bars indicate the 95% confidence intervals.                      Marzal, A., & Vidal, E. (1993). Computation of normalized
                                                                                     edit distance and applications. IEEE trans. on Pattern
                                                                                     Analysis and Machine Intelligence, 15 (9), 926–932.
Daelesmans, W. (1999). Introduction to the special issue on                      Nakisa, R. C., & Plunkett, K. (1998). Evolution for rapidly
     memory-based language processing. Journal of exper-                              learned representations for speech. Language and cog-
     imental and theoretical artificial intelligence, 11, 369-                        nitive processes, 13 (2/3), 105-127.
     390.
                                                                                 Nosofsky, R. M. (1986). Attention, similarity and the
Dennis, S. (2003a). An alignment-based account of serial                              idenitification-categorization relationship. Journal of
    recall. In Twenty fifth conference of the cognitive science                       Experimental psychology: General, 115, 39-57.
    society (Vol. 25). Lawrence Erlbaum Associates.
                                                                                 Pinker, S. (1994). The language instinct: How the mind
Dennis, S. (2003b). A comparison of statistical models for the                        creates language. New York: William Morro.
    extraction of lexical information from text corpora. In
    Twenty fifth conference of the cognitive science society                     Radford, A. (1988). Transformational grammar: A first
    (Vol. 25). Lawrence Erlbaum Associates.                                           course. Cambridge: Cambridge University Press.
Dennis, S. (2004). An unsupervised method for the extraction                     Redington, M., Chater, N., & Finch, S. (1998). Distributional
    of propositional information from text. Proceedings of                            information: A powerful cue for acquiring syntactic cat-
    the National Academy of Sciences, 101, 5206-5213.                                 egories. Cognitive Science, 22 (4), 425-469.
Dennis, S. (in press). A memory-based theory of verbal                           Shiffrin, R. M., & Steyvers, M. (1997). Model for recogni-
    cognition. Cognitive Science.                                                      tion memory: Rem - retrieving effectively from memory.
                                                                                       Psychonomic Bulletin & Review, 4 (2), 145-166.
Dennis, S. (submitted). Introducing word order in an LSA
    framework. In Latent Semantic Analysis: A road to                            Wagner, R. A., & Fischer, M. J. (1974). The string-to-string
    meaning.                                                                         correction problem. Journal of the ACM, 21 (1), 168-
                                                                                     173.
Dougherty, M. R. P. (1999). Minerva-dm: A memory pro-
    cesses model for judgements of likelihood. Psychological
    Review, 106 (1), 180-209.
                                                                           588

