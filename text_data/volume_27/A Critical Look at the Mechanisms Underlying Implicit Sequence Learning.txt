UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
A Critical Look at the Mechanisms Underlying Implicit Sequence Learning
Permalink
https://escholarship.org/uc/item/8hs6b0xc
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 27(27)
Authors
Gureckis, Todd M.
Love, Bradley C.
Publication Date
2005-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

      A Critical Look at the Mechanisms Underlying Implicit Sequence
                                                         Learning
                                  Todd M. Gureckis (gureckis@psy.utexas.edu)
                                      Bradley C. Love (love@psy.utexas.edu)
                              Department of Psychology; The University of Texas at Austin
                                                    Austin, TX 78712 USA
                           Abstract                                 Second, we explore the ability of this simple model
                                                                 to account for sequential learning phenomena in a va-
   In this report, a model of human sequence learning            riety of implicit learning situations including the serial
   is developed called the linear associative shift register
   (LASR). LASR uses a simple error-driven associative           reaction time (SRT) task and statistical word learning
   learning rule to incrementally acquire information about      paradigms. LASR provides a similar account of the type
   the structure of event sequences. In contrast to recent       of processing which underlies performance in both kinds
   modeling approaches, LASR describes learning as a sim-        of tasks, suggesting that they may rely on similar under-
   ple and limited process. We argue that this simplicity        lying mechanisms.
   is a virtue in that the complexity of the model is better
   matched to the demonstrated complexity of human pro-             We begin by introducing the LASR model and the
   cessing. The model is applied in a variety of situations      principles upon which it is based. Next, we consider a
   including implicit learning via the serial reaction time      study conducted by Lee (1997) assessing implicit learn-
   (SRT) task and statistical word learning. The results of      ing of sequentially structured material. Finally, we ex-
   these simulations highlight commonalities between dif-
   ferent tasks and learning modalities which suggest sim-       plore the ability of LASR to account for statistical word
   ilar underlying learning mechanisms.                          learning in infants as reported by Saffran, Aslin, and
                                                                 Newport (1996).
                       Introduction
                                                                    The Linear Associative Shift-Register
One of the most striking aspects of human behavior is
the ease with which we can acquire new skills with little                           (LASR) Model
conscious effort. In order to better understand this phe-        LASR is a mechanistic model of implicit sequence learn-
nomena, a large literature has developed exploring the           ing. The model describes implicit sequence learning as
ability of participants to implicitly learn about the se-        the task of appreciating the associative relationship be-
quential structure of a series of events (see Cleeremans,        tween past events and future ones. LASR assumes that
Destrebecqz, Boyer, 1998, for a review). However, the            subjects maintain a limited memory for the sequential
type of memory and learning mechanisms which might               order of past events and that they use a simple error-
support such learning are not well understood (see Keele,        driven associative learning rule (Widrow & Hoff, 1960;
Ivry, Mayr, Hazeltine, & Heuer, 2004 or Sun, Sluzarz, &          Rescorla & Wagner, 1972) to incrementally acquire in-
Terry, 2005 for some recent proposals).                          formation about sequential structure. Despite its sim-
   In this paper, we develop a simple model of se-               plicity, the model can very quickly learn to appreciate
quence learning behavior called the linear associative           rather complex dependencies between events which are
shift-register (LASR). The model is unique from past ap-         structured in time. The model is organized around 3
proaches in that it describes implicit sequence learning as      principles:
a simple and limited process which operates on a small
                                                                 1. Past events are stored in a temporary buffer
temporary buffer of past events. This contrasts with
                                                                 The model begins by assuming a simple shift-register
other models of sequence learning which have described
                                                                 memory for past events. Individual elements of the reg-
learning as a more complex and flexible process (Cleere-
                                                                 ister are referred to as slots. New events encountered
mans & McClelland, 1991; Cleeremans, 1993; Lebiere &
                                                                 in time are inserted at one end of the register and all
Wallach, 2000).
                                                                 past events are accordingly shifted one time slot. Thus,
   There are two main goals of this report. First, we
                                                                 the most recent event is always located in the right-most
demonstrate how a very simple learning mechanism such
                                                                 slot of the register (see Figure 1). This form of mem-
as LASR can provide a detailed account of a number of
                                                                 ory maintains the sequential order of recent events us-
findings from the implicit sequence learning literature.
                                                                 ing spatial position (similar to many other models, see
A key criticism we develop is that in previous modeling
                                                                 Sejnowski and Rosenberg (1987) or Cleereman’s (1993)
accounts (such as the simple recurrent network (SRN)
                                                                 buffer network).
of Cleeremans, 1993), the complexity of the model is
not well matched to the demonstrated complexity of the           2. Learning to predict what comes next This
learner. While LASR cannot explain all aspects of our            simple memory mechanism forms the basis of a detector
rich sequential behavior, we believe the model provides          (see Figure 1). A detector is a simple, single-layer linear
a unique baseline against which to test more complex             network or perceptron (Rosenblatt, 1958) which learns
theories and experiments.                                        to predict the occurrence of a single future event based
                                                             869

                        1                    0                    0             events is indexed based on the current time t. Thus,
             A  =     0        B      =    1        C    =      0               mt−1 refers to the input vector experienced on the pre-
                                                                                vious time step, and mt−2 refers to the input experienced
                   0                    0                    1
                                                                                two time steps in the past.
                        A  Detector                                             Response Given N possible events or choice options,
                                                                                the model has N detectors. The activation dk of the
                                                     Slots                      detector k at the current time, t, is computed as the
                                                                                weighted sum of all events in all slots multiplied by an
                                                                 0
                                                                                exponential attenuation factor:
                                                               1    Current                      P X N
                                                                    Event                      X
                              Previous Events (t-n)        0                              dk =          w(t−i)jk · mt−i
                                                                                                                      j  · e−α·(i−1)          (1)
         t-P                      t-3    t-2    t-1        t
        m                 ...    m      m    m          m                                       i=1 j=1
       Shift Register Memory
                                                                                where w(t−i)jk is the weight from the jth outcome at
                                                                                time slot t − i to the kth detector, and mt−i    j   · e−α· (i−1)
Figure 1: A shift-register memory and a single detector.
                                                                                is the outcome of the jth option at time t − i multiplied
New events encountered enter into the register from the                         by the memory attenuation factor. The α is a free pa-
right and are stored in the sequence they arrived in the                        rameter which controls the rate of decay for traces in
memory register.                                                                memory. The final output of each detector, ok , is a sig-
                                                                                moid transform of the activation, dk , of each detector:
on past events. Because each detector predicts only a                                                             1
                                                                                                       ok =            .                      (2)
single event, a separate detector is needed for each pos-                                                    1 + edk
sible event. Each detector has a weight from each event                            When being compared to human data, the output of
outcome at each time slot. On each trial, activation                            each detector is converted into a response probability or
from each memory-register slot is passed over a connec-                         tendency (pk ) using the Luce choice rule (Luce, 1959):
tion weight and summed to compute the activation of
the detector’s prediction unit. The task of a detector                                                            ok
                                                                                                       pk = PN                                (3)
is to adjust the weights from individual memory slots                                                                oj
                                                                                                                 j=1
so that it can successfully predict the future occurrence
of it’s response. Each detector learns to strengthen the                        For example, human reaction time is assumed to in-
connection weights for memory slots which prove pre-                            versely relate to this response tendency so that faster
dictive of the detector’s response while weakening those                        responses in the task correspond to higher values of
which are not predictive or are counter-predictive.                             pk (Cleeremans & McClelland, 1991).
3. Recent events have more influence on learning                                Learning Learning in the model is implemented using
than past events The model assumes that events in                               the well known delta-rule for training single layer net-
the recent past are remembered better than events which                         works (Widrow & Hoff, 1960) with a small modification
happened long ago. This effect is implemented by atten-                         introduced by Rumelhart and McClelland (1986) (some-
uating the activation strength of each register position                        times referred to as the generalized delta-rule for single
by how far back in time the event occurred. Because of                          layer networks). For each detector, the difference be-
this, an event which happened at time t − 1 has more                            tween the actual outcome of the current trial, tk , and
influence on future predictions than events which hap-                          the output of the detector, ok , is computed and used to
pened at t − 2, t − 3, etc... Similarly, learning is slower                     adjust the weights:
for slots which are positioned further in the past because
their activation strength is reduced (see Equation 4).                             ∆wijk = η · (tk − ok ) · mij · e−α·(i−1) · dk (1 − dk )    (4)
Model Formalism                                                                 The ∆wijk value is added to the corresponding weight
                                                                                after each learning episode. The η is a learning rate pa-
The following section describes the mathematical formal-                        rameter and e−α·(i−1) is the memory attenuation factor
ism of the model. The model is easily described using                           described above and dk (1 − dk ) is a factor representing
three equations and three intuitive parameters.                                 the derivative of the sigmoid transfer function with re-
Memory As illustrated at the top of Figure 1, input                             spect to the weights which moves learning on each trial
to the model on each time step is a N -dimensional vec-                         in the direction of gradient descent on the error. In the
tor mt where each entry mti corresponds to the presence                         simulations reported here α = 0.2 and η = 0.9.
(mti = 1) or absence (mti = 0) of event i on the cur-
rent trial, t. The complete history of past events is thus                               Evaluating the LASR model
a N xP matrix, M, where N is the number of possible                             In the following section we explore the ability of this
events, and P is the number of events so far experienced                        model to account for a number of published findings con-
and stored in memory. The shift-register memory of past                         cerning implicit sequence learning. The results illustrate
                                                                            870

                                                                                0.9                                      Simulation Results LASR was applied to the task in
                              650
                                                                                                                         a similar manner to how participants were trained with
                                                                                        1-p for correct response
                                                                                                                         the same number of trials and the same sequential struc-
       Reaction Time (ms)
                              625
                                                                                                                         ture as the Boyer, Destrebecqz, and Cleeremans (1994)
                                                                                0.8
                                                                                                                         replication. On each trial, the magnitude of the model’s
                                                                                                                         response for the correct outcome was recorded. Figure 2
                                                                                           k
                              600                                                                                        (top panel) shows the model’s response as a function of
                                                                                0.7                                      position. At the first set position, the model’s error is
                                                                                                                         about 0.83 which is chance (i.e, 5/6) but as more of the
                              574               Human                                                                    sequence is revealed, the model continues to reduce this
                                                LASR                                                                     error (thus predicting faster RT).
                                                                                0.6
                                    1       2       3         4       5    6                                                The model is able to replicate the key qualitative
                                                    Position                                                             results of the study despite having no mechanism for
                                                                                                                         grouping sequence elements, and only a simple single
                              700                                              1.0                                       layer of weights. A closer look at how the model solves
                                                                                                                         the problem gives some insight into the structure of the
                                                                                     1-p for correct response
                              675                                                                                        task. Figure 3 shows the setting of each of the weights
         Reaction Time (ms)
                                                                               0.9
                              650
                                                                                                                         in the model at the end of learning. The key pattern to
                                                                                                                         notice is that the diagonal entries for each past time slot
                                                                               0.8
                              625                                                                                        are strongly negative while all other weights are close to
                                                                                        k
                                                                                                                         zero. The diagonal of each weight matrix represents the
                              600                                              0.7                                       weight from each event to it’s own detector. Thus, the
                              575
                                                                                                                         model attempts to inhibit any response that occurred in
                                                                               0.6                                       the last few trials.
                              550
                                        2       4        6        8       10
                                                                                                                            The impact of this is demonstrated in Figure 2 (bot-
                                                        Lag                                                              tom panel) which shows response probability as a func-
                                                                                                                         tion of the number of events separating two repeated
Figure 2: Top: Human reaction time and model re-                                                                         events (lag). Since the same event could not repeat on
                                                                                                                         successive trials, repeated events were at minimum sep-
sponse as a function of set position in Boyer, Destre-
                                                                                                                         arated by 1 event (lag-1). This might happen if the fifth
becqz, and Cleeremans’ (1998). Bottom: Human and                                                                         event of one sequence repeated as the first element of
model response as a function of the lag separating two                                                                   the next sequence. Figure 2 (bottom) shows that as the
occurrences of the same event. All human data repli-                                                                     lag between two repeated events increases, the model
cated approximately from figures in Boyer, et al. (1998)                                                                 accurately predicts faster RT. The memory attenuation
                                                                                                                         of past events causes them to become less inhibited as
                                                                                                                         they move further into the past (i.e., events at lag-10 are
how the simple principles which define the LASR model                                                                    more strongly inhibited than events at lag-1). Boyer, et
are able to provide a strong account of learning and show                                                                al. (1998) examined this same lag effect in the reaction
the relationship between the data collected across a num-                                                                time of participants in their replication and found an
ber of paradigms.                                                                                                        identical effect (also shown in Figure 2, bottom panel).
                                                                                                                         Participant RT was inversely related to the number of
Sequence Learning via the SRT task                                                                                       trials that separated the repeated event. The model de-
The majority of SRT studies have used simple repeating                                                                   scribes performance in the task as a simple negative re-
sequences of various lengths. One notable exception is                                                                   cency effect.
Lee (1997). In this study, the pattern of stimuli was de-                                                                   Boyer, et al. (1998) explored how the SRN accounted
termined by a simple, yet subtle rule: each of six choice                                                                for human performance in this task. The SRN provides
options had to be visited once in each set of six trials in                                                              a similar conceptual account by learning to increase the
a random order. Examples of legal six-element sequence                                                                   likelihood of an response as a function of the number
sets are 132546, 432615, and 546123. Boyer, Destre-                                                                      of events since last experienced. However, the learning
becqz, and Cleeremans (1998) provide a replication of                                                                    mechanism of the SRN is much more complicated than
Lee (1997) and showed that reaction time monotonically                                                                   that of LASR because the model must acquire appropri-
decreases as a function of set position 1-6 (see Figure 2,                                                               ate hidden unit representations in addition to adjusting
top panel).                                                                                                              weights in the upper layer of the network. As a result,
   What is unique about the sequence employed by Lee                                                                     Boyer, et al. (1998) had to train the SRN on consider-
(1997) is that while it is generated by a simple rule, each                                                              ably more trials than humans or LASR. Both humans
stimulus item can be followed by any other item. The                                                                     and LASR were trained for 4320 trials (24 blocks of 180
key predictive structure is contained in the set of six suc-                                                             trials each), whereas the SRN was trained for 30,240
cessive elements which avoid repetition. Can the simple                                                                  trials. In addition, the hidden unit representations the
one-layer associative learning mechanism in LASR ac-                                                                     SRN acquires are difficult to interpret because the model
count for such a result?                                                                                                 learns to predict successors of particular aggregate con-
                                                                                                                   871

                                                                                                  1 23456
                                                                                                          1
                                                                                                          2
                                                                                                          3
                                                                                                          4
                                                                                                          5
                                                                                                          6
                    t-12    t-11  t-10    t-9     t-8   t-7    t-6     t-5    t-4    t-3    t-2     t-1
                                                -           0            +
Figure 3: The final LASR weights for the Lee (1997) sequence learning problem. Negative weights are darker red.
Positive weights are darker blue. The weights leaving each memory slot (t − 1, t − 2, etc...) are shown as a separate
matrix. Each matrix shows the weights from each stimulus element to each detector. For example, the red matrix
entry in the top left corner of t-1 slot is the weight from event “1” to the detector for event “1”.
texts. Instead, LASR clearly describes performance in           the same nonsense words which were presented during
the task as a simple negative recency effect, where re-         the familiarization phase while the remaining two were
cent events are inhibited.                                      three syllable non-words which contained the same syl-
   Boyer, et al. (1998) point out that in both their repli-     lables heard during the familiarization phase but in a
cation and in the original Lee (1997) study, participants       different order than they appeared in the initial phase.
demonstrate a faster reaction time to the latter elements       In Experiment 2, the test phase contrasted knowledge
of the each sequence even in the first block of learning        about words versus part-words where part-words con-
and the magnitude of this effect remains relatively con-        sisted of syllables arranged in the same order as during
stant throughout learning. Given the natural prevalence         familiarization, without directly corresponding to any of
of the gambler’s fallacy (i.e. negative recency) in sequen-     the words used to generate the familiarization sequence.
tial decision making tasks, it’s possible that some kind           The results of both studies are shown in Figure 4 and
of preexisting biases influenced their performance in the       indicate that infants were able to discriminate words
task (Gilovich, Vallone, & Tversky, 1985; Jarvik, 1951;         from both non-words (Experiment 1) and part-words
Nicks, 1959). LASR also shows the learning effect in            (Experiment 2) as reflected by longer listening times for
Figure 2 (top panel) in the first block of learning due         the latter test stimuli. These findings demonstrate that
to it’s rapid adaptation to the task. However, assuming         infants are able to extract information about the statisti-
Boyer, et al.’s interpretation of the human data is cor-        cal properties of a sequence given even a short incidental
rect (and not a floor effect of RT as participants gain         exposure to auditory stimuli.
experience in the task), it would be straightforward to
                                                                Simulation Results To simulate these results with
simulate an initial bias in LASR by initializing the learn-
                                                                LASR, each syllable was treated as a separate event in
ing weights with a slightly negative value instead of zero
                                                                the model. In both Experiment 1 and 2 there were 12
at the beginning of learning.
                                                                possible syllables, thus the model had 12 detectors. On
Statistical Word Learning                                       each simulated trial, the model attempted to predict the
                                                                next syllable in the sequence given the syllables which it
It is clear from the previous simulations that despite it’s     had experienced so far.
simplicity, LASR can provide an accurate description of            During each trial of the the test phase, the memory
sequential learning behavior in the SRT. However, a key         register was cleared by setting all values back to zero and
question remains concerning the generality of these find-       the output of the correct detector was recorded following
ings: is sequential learning in the SRT sub-served by           the presentation of each syllable of the test sequence. In
similar mechanisms as other areas of cognitive process-         order to compare infant looking time and model perfor-
ing which rely on sequence processing? To evaluate this         mance (a necessarily indirect relationship), the output
hypothesis, we apply LASR to the infant word learning           ok of the correct detector for each syllable of the test
study conducted by Saffran, Aslin, and Newport (1996).          sequence was summed to compute an overall familiarity
   Saffran et al. (1996) familiarized 8-month-old infants       score for the test item. These familiarity scores were
with a 2-minute recording of a computer-synthesized             then related to looking time via linear regression.
voice evenly reading a continuous stream of syllables              Figure 4 shows resulting performance of the model
at an even tempo. The stream was composed of four               averaged over 1000 simulated experiments. The model
three-syllable nonsense words which were repeated in            predicts increased looking time for both non-words and
random order (examples word are “tu-pi-ro” and “go-             part-words. Examination of the final setting of the de-
la-bu”). The only cues concerning the beginning and             tector weights reveal that the weights grow to approxi-
end of words in the stream was the transitional proba-          mate the transitional probabilities between syllables at
bilities between syllables which were higher between two        different lags in the training sequence.
syllables which occurred together within a words than              LASR provides a similar account of sequence learn-
between two syllables which spanned word boundaries.            ing in both the Lee (1997) and Saffran, et al. (1996)
   On each trial of the test phase, infants were pre-           experiments. In each case, the model’s weight grow to
sented with repetitions of one of four three-syllable test      approximate the lag-n transition probabilities in the se-
strings. In Experiment 1, two of the test words were            quence (i.e. the probability of an event at time t given
                                                            872

                                                                                 bi bu da do go ku la pa pi ro ti tu         bi bu da do go ku la pa pi ro ti tu
                                 Experiment 1     Experiment 2                                                          bi
                            9                                                                                           bu
                                                                                                                        da
                                                                                                                        do
                           8.5                                                                                          go
    Looking Time in Sec.
                                                                                                                        ku
                                                                                                                        la
                             8                                                                                          pa
                                                                                                                        pi
                           7.5                                                                                          ro
                                                                                                                        ti
                                                                                                                        tu
                            7
                                                                                               t-2                                           t-1
                           6.5                                                    bi bu da do go ku la pa pi ro ti tu        bi bu da do go ku la pa pi ro ti tu
                                    Infants                                                                             bi
                             6      LASR                                                                                bu
                                                                                                                        da
                                                                                                                        do
                                                                                                                        go
                                                                                                                        ku
                                 Word Non-word   Word   Part-word                                                       la
                                                                                                                        pa
                                                                                                                        pi
                                                                                                                        ro
                                                                                                                        ti
Figure 4: Comparison of infant and LASR results for                                                                     tu
Saffran, et al. (1996) Experiment 1 and 2.                                                       t-2                                          t-1
a particular event on on trial t − n, Remillard & Clark,                  Figure 5: The final LASR weights for the Saffran, et
2001). This is made clear in Figure 5 which compares the                  al. (1996) infant word learning experiment. The labels
final setting of LASR’s weights (top) and the true tran-                  on the rows correspond to possible syllable outcomes at
sition probabilities in the training sequence (bottom) at                 each time slot, while the labels on the columns refer to
lag t − 1 and t − 2. LASR natural picks up on the sta-                    the corresponding detector. The bottom two matrices
tistical structure of the sequence and allows it to extract               show the actual transition probabilities between syllables
what might appear to be segmented knowledge about                         at lag-1 and lag-2 in the training sequence. Black, grey,
the sequence. The SRN has also been used to explain                       and white squares represent a transitional probability
sequential word learning results similar to those studied                 of 1.0, 0.33, and 0.0, respectively. The model weights
by Saffran, et al. (Elman, 1990; Allen & Christiansen,
                                                                          closely mirror the transition probabilities.
1996). However, the type of information acquired by the
SRN differs from LASR because the SRN is capable of
learning the true second order conditional probabilities
due to it’s hidden unit representations. The testing pro-
cedure used with infants does not distinguish between                        Second, we showed how a simple, single layer learning
these two types of learning, but, (as these simulations                   mechanism is able to account for findings which have
show) the simpler lag-n statistic is sufficient to account                previously been accounted for using more complex
for learning.                                                             mechanisms. A full evaluation of LASR is not possible
                                                                          in this short paper, but preliminary work suggests that
                             Conclusions and Discussion                   the model provides a similar account of the processes
                                                                          underlying implicit learning in many other studies.
The results of these simulations offer two conclusions.                   With this in mind, we offer LASR model as a possible
First, we show how the types of sequential learning re-                   “null” model for implicit sequence learning studies. It
ported in both the SRT and statistical word learning                      is important when developing models based on indirect
paradigms might be accounted by the same simple prin-                     measures of knowledge (such as reaction time) that
ciples which define the LASR model. Recently, a number                    theories aren’t developed which reach beyond the data.
of authors have argued that behavior in both types of                     We argue that LASR provides a tight match between
tasks could tap similar learning processes. Evidence in                   the complexity of the model and the demonstrated
support of this hypothesis includes the fact that the type                processing complexity of the learner. In this sense, our
of sequential learning demonstrated by infants with ar-                   argument bears some resemblance to other arguments
tificial syllable languages has replicated to more general                put forward in the SRT literature (Perruchet, Gallego,
auditory stimuli such as tones (Saffran, Johnson, Aslin,                  & Savy, 1990; Reed & Johnson, 1994; Remillard &
& Newport, 1999) and to motor sequences in the SRT                        Clark, 2001). However, we build upon these criticisms
task (Hunt & Aslin, 2001) suggesting that this type of                    by providing a viable modeling framework which shows
processing is not specific to linguistic material. Cross-                 promise as both an explanation and as a tool.
species comparisons show that non-human primates are
also able to discriminate words and non-words in the syl-                 Acknowledgments Many thanks to Matt Jones,
lable task, again in support of the idea that learning in                 David Gliden, Levi Larkey, and Yasu Sakamoto for help-
such tasks is not a property of a language specific learn-                ful many helpful conversations during the development of
ing system (Conway & Christiansen, 2001).                                 these ideas. This work was supported by Grant FA9550-
                                                                    873

04-1-0226 and National Science Foundation CAREER                Lee, Y. (1997). Learning and awareness in the serial re-
Grant 0349101 to Bradley C. Love.                                  action time. In (p. 119-124). Hillsdale, NJ: Lawrence
                                                                   Erlbaum Associates.
                      References
                                                                Luce, R. D. (1959). Individual choice behavior: A theo-
Allen, J., & Christiansen, M. (1996). Integrating multi-
                                                                   retical analysis. Westport, Conn.: Greenwood Press.
   ple cues in word segmentation: A connectionist model
   using hints. In Proceedings of the eighteenth annual         Nicks, D. (1959). Prediction of sequential two-choice
   conference of the cognitive science society (p. 370-            decisions from event runs. Journal of Experimental
   375). Mahwah, NJ: Lawrence Erlbaum Associates.                  Psychology, 57 (2), 105-114.
Boyer, M., Destrebecqz, A., & Cleeremans, A. (1998).            Perruchet, P., Gallego, J., & Savy, I. (1990). A critical
   The serial reaction time task: Learning without                 reappraisal of the evidence for unconscious abstrac-
   knowing, or knowing without learning? In Proceed-               tion of deterministic rules in complex experimental
   ings of the 20th annual meeting of the cognitive sci-           situations. Cognitive Psychology, 22, 493-516.
   ence society (p. 167-172). Hillsdale, NJ: Lawrence
   Erlbaum Associates.                                          Reed, J., & Johnson, P. (1994). Assessing implicit learn-
                                                                   ing with indirect tests: Determining what is learned
Cleeremans, A. (1993). Mechanisms of implicit learning:            about sequence structure. Journal of Experimental
   Connectionist models of sequence processing. Cam-               Psychology: Learning, Memory, & Cognition, 20 (3),
   bridge, MA: MIT Press.                                          585-594.
Cleeremans, A., Destrebecqz, A., & Boyer, M. (1998).            Remillard, G., & Clark, J. (2001). Implicit learning of
   Implicit learning: news from the front. Trends in               first-, second-, and third-order transitional probabil-
   Cognitive Science, 2 (10), 373-417.                             ities. Journal of Experimental Psychology: Learning,
                                                                   Memory, & Cognition, 27 (2), 483-498.
Cleeremans, A., & McClelland, J. (1991). Learning the
   structure of event sequences. Journal of Experimental        Rescorla, R., & Wagner, A. (1972). A theory of pavol-
   Psychology: General, 120, 235-253.                              vian conditioning: Variations in the effectiveness of
                                                                   reinforcement and non-reinforcement. In A. Black &
Cohen, A., Ivry, R., & Keele, S. (1990). Attention and
                                                                   W. Prokasy (Eds.), Classical conditioning ii: Current
   structure in sequence learning. Journal of Experi-
                                                                   research and theory (p. 64-99). New York: Appleton-
   mental Psychology: Learning, Memory, & Cognition,
                                                                   Century-Crofts.
   16 (1), 17-30.
Conway, C., & Christiansen, M. (2001). Sequential learn-        Rosenblatt, F. (1958). The perceptron: A probabilistic
   ing in non-human primates. Trends in Cognitive Sci-             model for information storage and organization in the
   ence, 5 (12), 539-546.                                          brain. Psychological Review, 65, 386-408.
Elman, J. (1990). Finding structure in time. Cognitive          Rumelhart, D. E., McClelland, J. L., & the PDP Re-
   Science, 14, 179-211.                                           search Group. (1986). Parallel distributed processing.
                                                                   explorations in the microstructure of cognition. Cam-
Gilovich, T., Vallone, R., & Tversky, A. (1985). The hot           bridge, MA, USA: MIT Press.
   hand in basketball: On the misperception of random
   sequences. Cognitive Psychology, 17, 295-314.                Saffran, J., Aslin, R., & Newport, E. (1996). Statistical
                                                                   learning by 8-month-old infants. Science, 274, 1926-
Hunt, R., & Aslin, R. (2001). Statistical learning in a se-        1928.
   rial reaction time task: access to separable statistical
   cues by individual learners. Journal of Experimental         Saffran, J., Johnson, E., Aslin, R., & Newport, E.
   Psychology: General, 130 (4), 658-680.                          (1999). Statistical learning of tone sequences by hu-
                                                                   man infants and adults. Cognition, 70, 27-52.
Jarvik, M. (1951). Probability learning and a nega-
   tive recency effect in the serial anticipation of alter-     Sejnowski, T., & Rosenberg, C. (1987). Parallel networks
   native symbols. Journal of Experimental Psychology,             that learn to produce english text. Complex Systems,
   41, 291-297.                                                    11, 145-168.
Keele, S., Ivry, R., Mayr, U., Hazeltine, E., & Heuer, H.       Sun, R., Sluzarz, P., & Terry, C. (2005). The interaction
   (2004). The cognitive and neural architecture of se-            of the explicit and the implicit in skill learning: A
   quence representation. Psychological Review, 110 (2),           dual-process approach. Psychological Review, 112 (1),
   316-339.                                                        159-192.
Lebiere, C., & Wallach, D. (2000). Sequence learning in         Widrow, B., & Hoff, M. (1960). Adaptive switching
   the act-r cognitive architecture: Empirical analysis of         circuits. Institute of Radio Engineers, Western Elec-
   a hybrid model. In R. Sun & C. Giles (Eds.), Sequence           tronic Show and Convention Record, 4, 96-104.
   learning (p. 188-212). Berlin: Springer-Verlag.
                                                            874

