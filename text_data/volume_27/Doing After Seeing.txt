UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Doing After Seeing
Permalink
https://escholarship.org/uc/item/0qs8w14q
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 27(27)
Authors
Hagmayer, York
Meder, Bjorn
Waldmann, Michael R.
Publication Date
2005-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

                                                        Doing After Seeing
                                           Björn Meder (bmeder@uni-goettingen.de)
                                  York Hagmayer (york.hagmayer@bio.uni-goettingen.de)
                            Michael R. Waldmann (michael.waldmann@bio.uni-goettingen.de)
                                  Department of Psychology, University of Göttingen, Gosslerstr. 14,
                                                        37073 Göttingen, Germany
                              Abstract                                  causal learning are sensitive to covariations but do not
   Causal knowledge serves two functions: it allows us to predict
                                                                        distinguish between causal and spurious relations. It is true
   future events on the basis of observations and to plan actions.      that these theories distinguish between observational
   Although associative learning theories traditionally                 (classical conditioning) and interventional learning
   differentiate between learning based on observations                 (instrumental conditioning), but, as the barometer example
   (classical conditioning) and learning based on the outcomes of       shows, they fail when predictions for instrumental actions
   actions (instrumental conditioning), they fail to express the        have to be derived from observational learning.
   common basis of these two modes of accessing causal                     Causal Bayes nets (Pearl, 2000; Spirtes, Glymour, &
   knowledge. In contrast, the theory of causal Bayes nets              Scheines, 1993; Woodward, 2003) provide a formal account
   captures the distinction between observations (seeing) and           of causal representations and inference that allows it to
   interventions (doing), and provides mechanisms for predicting        derive precise predictions for hypothetical interventions
   the outcomes of hypothetical interventions from observational
   data. In two experiments, in which participants acquired
                                                                        from observational knowledge. The goal of the present
   observational knowledge in a trial-by-trial learning procedure,      experiments is to investigate whether people who have
   the adequacy of causal Bayes nets as models of human                 observed individual trials presenting the states of a complex
   learning was examined. To test the robustness of learners’           causal model can later access their causal knowledge to
   competency, the experiments varied the temporal order in             derive observational and interventional predictions in a
   which the causal events were presented (predictive vs.               fashion anticipated by causal Bayes nets. To test the
   diagnostic). The results support the theory of causal Bayes          robustness of this competency we manipulated the temporal
   nets but also show that conflicting temporal information can         cues in the learning data (predictive learning from causes to
   impair performance.                                                  effects vs. diagnostic learning from effects to causes).
                          Introduction                                         Seeing vs. Doing in Causal Bayes Nets
The ability of acquiring and using causal knowledge is a
central competency necessary for explaining past events,                The formal framework of causal Bayes nets uses directed
predicting future events, and planning actions. But how do              acyclic graphs (DAGs) to represent causal relations between
people infer the consequences of their actions when                     variables, and parameters to express the strength of these
planning interventions in causal systems? In some cases                 relations (e.g., conditional probabilities). A complete causal
they might have tried out the interventions on previous                 model therefore combines qualitative assumptions about the
occasions so that they already know the potential outcomes              structure of the causal model with quantitative knowledge
of their actions. But what if only knowledge about                      about the size of the parameters associated with these causal
observational relations between causal events is available?             relations (e.g., base rates, causal strength, integration rules).
A tempting solution would be to equate observational                    An example is given in Figure 1. This causal model consists
knowledge with instrumental knowledge and proceed from                  of four (binary) variables A, B, C, and D, in which A can
there. Unfortunately this strategy will often lead to                  cause D either via B or via C.
ineffective actions. For example, the status of a barometer is             In the causal Bayes nets framework, the joint probability
statistically related to the upcoming weather, but                      distribution of this model can be factored into:
manipulating the barometer does not affect the weather.                       p(A.B.C.D)=p(A) · p(B|A) · p(C|A) · p(D|B.C)
Barometer readings and weather are spuriously related due
to their common cause, atmospheric pressure. As a                       This decomposition follows from applying the causal
consequence, observational predictions can capitalize on the            Markov condition (Spirtes et al., 1993; Pearl, 2000) to the
spurious statistical relation, whereas instrumental                     causal model. The causal Markov condition (informally)
predictions cannot. Effects do not change their causes; thus,           states that the state of any variable Xj in the system is
manipulating the barometer does not affect its cause                   independent of all other variables (except for its causal
atmospheric pressure, and therefore has no causal influence            descendants) conditional on the set of its direct causes, pax.
on the weather.                                                        For example, the causal Markov condition implies for the
  The difference between observing (seeing) and                        causal model shown in Figure 1 that variable D is
intervening (doing) is compelling in the barometer example.            independent of A conditional on its direct causes B and C.
Nevertheless, most theories of causal cognition do not                 Relations of conditional dependence and independence are
distinguish between different modes of accessing identical             critical for deriving observational and interventional
causal knowledge. For example, associative theories of                 inferences.
                                                                   1461

                                                                    graph and not on the original graph. Because of graph
                                                                    surgery, interventions in contrast to observations do not
                                                                    provide diagnostic evidence for the causes of the
                                                                    manipulated variable.
                                                                       To formalize the idea that a variable’s state is not based
                                                                    on the ‘natural course of events’ but was determined by an
                                                                    external intervention, Pearl (2000) introduced the so-called
                                                                    ‘Do-Operator’, written as Do (•).The expression ‘Do C=c’
                                                                    is read as “variable C is set to state c by means of an
                                                                    intervention”. Formally, the Do-operator renders a variable
                                                                    independent of its direct causes, which is equivalent to
                                                                    deleting all causal links pointing towards the variable fixed
                                                                    by the intervention. Based on the modified causal model, the
                                                                    probabilities of the other events can be computed. For
                                                                    example, the probability of A=a given that C is caused by an
   Figure 1: A parameterized causal model. Arrows indicate          intervention equals the base rate of A=a because the causal
 causal relations between variables; conditional probabilities      link connecting these two events was eliminated by the
  encode the strength of these relations. All parameters were       intervention,
 set except p(d|b.c) which is computed by a noisy-OR-gate.               p(a|Do(C=c)) = p(a|Do(C= ¬c)) = p(a).
Observational Predictions                                              In the same way the probability of D=d can be calculated
                                                                   using the modified causal model. Generating a value of C
Based on the structure of the causal model and its                 through an intervention closes the backdoor, hence no
parameters, the probabilities implied by observations of the       second causal pathway remains. Nevertheless the initial
states of observed variables can be computed using                 cause A may occur and influence D via B. Therefore the
probability calculus. For example, if variable C in the causal      correct formula to calculate the probability of D=d given a
model shown in Figure 1 is observed, the probability of A           (generative) intervention in C is:
being present can be computed using Bayes rule,
                                                                        p(d|Do(c))=p(a)·p(b|a)·p(d|b.c)+p(a)·p(¬b|a)·p(d¬b.c)
      p(a|c)=p(c|a) · p(a)/[p(c|a) · p(a)+p(c|¬a) · p(¬a)].             +p(¬a)·p(b|¬a)·p(d|b.c)+p(¬a)·p(¬b|¬a)·p(d|¬b.c).
This computation models a diagnostic inference from C to               Note that variable A is no longer conditionalized on C in
its cause A. A more interesting example is the prediction of        this formula. This implies that the interventional probability
variable D from an observation of variable C. Obviously             is necessarily lower than the observational probability,
there is the direct causal link connecting C to D. But there is     provided that A and B are positively related.
also a second causal pathway connecting C to D via A and               On the right hand side of the equations only parameters
B. If C is present, the probability of A increases, which in        are involved which can be derived from observational
turn increases the probability of B leading to an increase of       learning. Thus, subsequent to observational learning of a
D. Pearl (2000) vividly calls such confounding pathways             causal model and of its parameters, the consequences of
backdoors. Formally the probability for D given C can be            hypothetical interventions can be predicted without prior
calculated by:                                                      instrumental learning.
   p(d|c)=p(a|c)·p(b|a)·p(d|b.c)+p(a|c)·p(¬b|a)·p(d|¬b.c)+
   p(¬a|c)·p(b|¬a)·p(d|b.c)+p(¬a|c)·p(¬b|¬a)·p(d¬b.c).                              Psychological Evidence
                                                                    The examples described above show that normatively
Interventional Predictions                                          diagnostic and predictive inferences differ depending on
The literature on causal Bayes nets has focused on ideal            whether they are based on interventions or observations.
interventions in which the actions change the value of a            The interesting question is whether people make these
variable independent of the state of its parents (for more          distinctions as well. Thus far, very few experiments have
precise characterizations of these interventions, see, for          addressed this question (see Hagmayer, Sloman, Lagnado,
example, Woodward, 2003). For example, if we arbitrarily            & Waldmann, in press, for an overview).
change the reading of the barometer our action renders the             Sloman and Lagnado (2005) have studied counterfactual
barometer independent of its usual cause, atmospheric               inferences in given causal structures and have compared
pressure. Thus, interventions create independence, which            causal with logical arguments. For example, participants
can be expressed by removing the causal links between the           were given a causal chain model consisting of three events
variable that is targeted by the interventions and its parents.     that were all described to be present. Participants were then
For example, if variable C in the causal model depicted in          requested to imagine that the intermediate event was either
Figure 1 is set to a specific value through an intervention,        removed by an intervention or observed to be absent. In
the causal arrow from A to C can be eliminated. Following           accordance with the predictions of causal Bayes nets
previous work of Spirtes et al. (1993), Pearl (2000)                participants inferred that the initial cause in the chain would
describes this process as ‘graph surgery’. Interventional           be absent if the intermediate event was observed to be
predictions should be based on this modified manipulated            absent, but not if it was actively removed. Overall the
                                                               1462

results of Sloman and Lagnado’s experiments were                   require a stage of model manipulation (e.g., graph surgery)
consistent with causal Bayes nets. Because the focus of            prior to using the manipulated model for the predictions. To
these studies was on comparing logical with causal                 test whether learners distinguish between seeing and doing
reasoning, only qualitative reasoning based on the structure       even in conditions in which the temporal order of learning
of causal models was investigated.                                 events mismatches causal order, we varied learning order in
  Waldmann and Hagmayer (2005) wondered whether a                  the experiments.
dissociation between seeing and doing can also be found in           In Experiment 1 the temporal order during each trial
the realm of learning, and whether learners’ inferences           conformed to the causal order of the events in the causal
would be sensitive to the size of the parameters that were        model (see Fig. 1). Information about A was given first,
gleaned from the learning data. Participants in their             followed by information about the presence or absence of B
experiments were given instructions about the structure of        and C, and finally information about D was provided. In
causal models and subsequently received a list of cases on a      Experiment 2 the temporal order was reversed and no longer
single page that could be used to estimate the parameters of      matched the causal order of events. In this experiment it is
the models. Participants were then requested to derive            necessary to suppress temporal cues and estimate the
predictions for new hypothetical observations and                 parameters on the basis of data that is presented in the
hypothetical interventions. The results showed a surprising       reverse order (see Waldmann & Martignon, 1998). For
grasp of the differences between seeing and doing, which          example, in the causal model we have used in the
manifested itself in predictions that took into account the       experiments (Fig. 1), the final effect D is dependent on
size of the parameters which were estimated on the basis of       patterns of its two causes B and C. When temporal order is
the learning data.                                                 reversed in Experiment 2, participants observed the
  The present experiments move one step further in the             probability of B and C given D and the probability of A
realm of learning. In Waldmann and Hagmayer (2005) the             given B and C but have to infer the probabilities of B and C
parameters could be estimated on the basis of a list of cases      conditional upon A, and of D conditional upon B and C.
which provided simultaneous information about the                  Note that the patterns of covariation are nevertheless exactly
presence or absence of the variables. It can be argued that        the same across Experiments 1 and 2, that is, participants’
this task is still more a reasoning task than a learning task.     judgments are based on the very same data. Only the order
The typical temporal characteristics of causal learning are        in which information about the events was given is
better mirrored in trial-by-trial learning than in a highly        manipulated across the two experiments. Although we
processed list that lacks natural temporal cues. In fact,          expect that, consistent with causal-model theory, learners
Shanks (1991) has hypothesized that induction on the basis         will attempt to correctly learn the causal model regardless of
of aggregated data is handled by different learning                learning order, and will differentiate between seeing and
mechanisms than trial-by-trial learning. Thus, a                   doing, this competency might be marred by performance
demonstration of the competency to distinguish seeing and          deficits caused by the complexity of the task (see also
doing in the context of trial-by-trial learning would further      Waldmann & Walker, 2005).
weaken associative theories as models of causal induction.           A further novel aspect of this study is the presentation of a
  Moreover, studying inferences based on trial-by-trial            causal model that contains two parallel pathways that
learning introduces cues to causal structures that might           represent mutual confounds (i.e., backdoors). One additional
compete with the instructed causal models. The arrows              goal of the experiments was to test whether learners are
within causal models express our natural intuition about the       sensitive to the fact that interventions and observations
asymmetry of causes and effects: Causes generate effects           differ with respect to the way the second confounding
but not vice versa. In the real world causal order is often        pathway needs to be taken into account.
signaled by the temporal order in which causes and effects
are experienced. However, there are cases in which                                         Experiment 1
temporal order and causal order mismatch. For example,             The goal of the first experiment was to investigate whether
physicians often observe symptoms (i.e., effects) prior to         learners differentiate between seeing and doing after a trial-
learning about their causes. In these cases it is crucial that     by-trial learning phase in which learning order corresponds
the temporal order of experiencing events is ignored as a          to causal order. Twenty-four students from the University of
cue to causality.                                                  Göttingen participated. The model underlying the learning
  Trial-by-trial learning which presents learning events in a      data and its parameterization are shown in Figure 1. After
temporal order allows it to test the impact of temporal cues       the observational learning phase participants were asked to
on causal induction. A number of experiments designed to           imagine new cases in which either variable C was observed
test causal-model theory (Waldmann, 1996) have pitted              to be present or absent, or C was generated or eliminated by
temporal order against causal order. In these experiments it       an intervention. All participants had to estimate the
could be shown that learners are capable of learning causal        probability of A and D in each of these four cases (i.e., eight
models regardless of whether temporal order matches or             questions).
mismatches causal order. However, this competency breaks
down when complexity is increased (Waldmann & Walker,              Learning Phase. The variables of the causal model were
2005). Moreover, the competency was only tested with test          introduced as four fictitious chemical substances causally
questions that requested observational predictions.                interacting in wine casks. Participants were told that
Interventional questions are more complex because they             substance A causes the generation of substances B and C,
                                                              1463

which then can independently cause substance D (see Fig.             In contrast, if C is manipulated by an intervention the
1). It was also pointed out that the causal relations are            backdoor is closed because the link between A and C needs
probabilistic. In addition, participants were shown the graph        to be removed. However, D is still more likely when C is
of the causal model. They were instructed to learn the               generated than when it is inhibited because of C’s direct
strength of the causal relations from the learning data. The         causal influence. The difference, however, should be
kind of questions they would have to answer after the                smaller for the interventional than for the observational
learning phase was not mentioned until the test phase. The           probabilities.
learning phase consisted of 40 trials which implemented the            A further test of sensitivity to the difference between
probabilities shown in the causal graph in Figure 1. The             seeing and doing is provided by comparing the estimated
trials presented information on a computer screen about the          probabilities of D given observations of C or interventions
states of the variables. The temporal order corresponded to         in C. In the section about causal Bayes nets it was
causal order in Experiment 1. Thus, first information about         mentioned that the probability of D is higher if C is
variable A was presented, then, simultaneously, variables B          observed than when it is generated by an intervention. The
and C were shown, and finally information about event D              parameters of the presented causal model imply that the
was given.                                                           interventional probability of observing C (p(d|c)) is only
                                                                    slightly higher than the probability of generating C
Test Phase. The learning phase was followed by a test               (p(d|Do(c)). But the probability of D is considerably higher
phase in which participants had to answer eight questions.          when C is prevented p(d|Do(¬c)) than when it is observed
The questions first stated the current status of variable C          to be absent (p(d|¬c)).
(present vs. absent) and then asked about A (i.e., the cause
of C) or D (i.e., the effect of C). Thus, one question was           Results and Discussion
diagnostic, the other predictive. For the observational              Diagnostic inferences. The results for the diagnostic test
predictions, participants were instructed to imagine                 questions are shown in Table 1 along with the predictions
observing substance C in 40 previously unseen wine casks
and to estimate the number of casks in which substance A                 Table 1: Responses to diagnostic inference questions in
would also be found, (i.e., they estimated p(a|c) in a                       Experiment 1 (N=24) (Numbers indicate means of
frequency format). Participants were also asked to estimate                    conditional frequency estimates for 40 cases.)
the conditional frequency of A for 40 casks in which C was                                  Observation             Intervention
observed to be absent (p(a|¬c)). Two further questions
referred to interventions. These questions asked learners to                             p(a|c)     p(a|¬c)   p(a|Do( c))  p(a|Do(¬c))
imagine that substance C was added to 40 casks
(p(a|Do(c))), or that C was inhibited in 40 casks                       Causal Bayes
                                                                                           38          4          20            20
(p(a|Do(¬c))). The same set of questions was asked about               net predictions
D, the effect of C. Thus, participants estimated the number                Mean          30.50       17.08       25.54        27.25
of casks in which D would be found (i.e., p(d|c), p(d|¬c),                  SD            7.56       10.37       10.57         8.59
p(d|Do(c)), p(d|Do(¬c)). Interventional and observational
questions were blocked; the order of blocks was                      derived from causal Bayes nets. The pattern of estimated
counterbalanced.                                                     conditional frequencies qualitatively conformed to the
                                                                     pattern of the predicted values. As anticipated by causal
Causal Bayes Net Predictions. For the diagnostic                     Bayes nets, participants gave different estimates for the two
inferences, the crucial test for assessing whether participants      observational probabilities but judged the interventional
differentiated between observations and interventions                probabilities to be at the same level. An analysis of variance
concerns the comparison of the two observational                     with the factors ‘intervention vs. observation’ and ‘presence
probabilities p(a|c) versus p(a|¬c) to the two corresponding         vs. absence of C’ as within-subjects factors yielded a
interventional probabilities p(a|Do(c)) versus p(a|Do(¬c)).          significant       interaction,      F(1,23)=23.78,      p<0.001,
Whereas observing C=c or C=¬c is diagnostic evidence for             MSE=57.75. As predicted by the causal Bayes nets
the state of A, generating C=c or C=¬c by an intervention is         framework, there was no difference between the
not diagnostic for A. Therefore, the observational                   interventional questions, F<1, but a significant difference
probabilities should differ, whereas the interventional              between the observational questions, F(1,23)=35.51,
probabilities should stay at a constant level. According to          p<0.001, MSE=59.17. Although participants’ estimates did
causal Bayes nets, A should be expected to occur with the            not perfectly match the normative causal Bayes net
probability that corresponds to its base rate.                       predictions, the results provide evidence for participants’
   The predictive inferences are more complicated because            sensitivity to the difference between seeing and doing in
the second causal pathway generating D has to be taken into          diagnostic judgments.
account. Participants should consider both C’s direct causal         Predictive inferences. The results for the predictive
relation to D but also the alternative path AÆBÆD. The              questions concerning the probability of D are shown in
observation of C opens the backdoor to the second pathway            Table 2. This type of inference is more complicated than the
(i.e., the presence of C indicates that A is likely to be            diagnostic inference in the chosen causal model. Whereas
present). Therefore, participants should infer that D is more        the latter only requires considering the direct causal relation
likely to be present if C is observed than when it is absent.        between A and C (with the rest of the causal model being
                                                                1464

irrelevant for this task), the inference concerning variable D       after diagnostic learning (effects presented prior to their
requires taking into account the complete model. In                  causes) but this competency was only tested with
particular, the alternative confounding pathway AÆBÆD                observational test questions and displayed itself only with
needs to be considered.                                              causal models that were less complex than the causal model
   As can be seen in Table 2, participants were surprisingly         used in the present experiments (see Waldmann & Walker,
sensitive to the second confounding causal pathway: As               2005). Experiment 2 used the same experimental design,
predicted by causal Bayes nets, the difference between               cover story, and instructions as Experiment 1. Thus,
responses to the observational questions proved larger than          participants received instructions about the causal model
the difference between the responses to the interventional           displayed in Figure 1. Again 24 participants from the
questions. An analysis of variance with ‘intervention vs.            University of Göttingen participated. In contrast to
observation’ and ‘presence vs. absence of C’ as within-              Experiment 1, the temporal order of learning events did not
subjects factors yielded a significant interaction,                  match their causal order (i.e., diagnostic learning). In each
F(1,23)=8.73, p<0.01, MSE=54.65. In accordance with the              trial, participants were first informed about the status of
parameterization of the causal model, there was only a               effect D, then simultaneously about the mediating variables
slight, non-significant difference between p(d|c) and               B and C, and finally about the initial cause A. Participants
p(d|Do(c)), F(1,23)=1.0, p=0.33. This is important as there          had to mentally reverse the observed statistical relations to
might have been a general tendency to answer interventional          correctly estimate the parameters of the causal model. As in
questions differently from observational questions. The              Experiment 1, participants were requested to estimate the
crucial test of the predictions of causal Bayes nets is              conditional frequencies of A and D given observations of or
provided by the comparison between p(d|¬c) and                      interventions in C.
p(d|Do(¬c)). Participants judged the probability of the
occurrence of D to be significantly higher when C was               Results and Discussion. Tables 3 and 4 show the means of
prevented by an intervention than when it was merely                the conditional frequency estimates along with the
observed to be absent, F(1,23)=9.57, p<0.01, MSE=57.83.             predictions derived from causal Bayes nets.
   The results demonstrate a remarkable grasp of the
difference between intervening and observing after trial-by-            Table 3: Responses to diagnostic inference questions in
                                                                           Experiment 2 (N=24) (Numbers indicate means of
    Table 2: Responses to predictive inference questions in                   conditional frequency estimates for 40 cases.)
       Experiment 1 (N=24) (Numbers indicate means of
          conditional frequency estimates for 40 cases.)                                  Observation             Intervention
                     Observation             Intervention                               p(a|c)  p(a|¬c)    p(a|Do(c))    p(a|Do(¬c))
                                                                     Causal Bayes net
                   p(d|c)    p(d|¬c)   p(d|Do(c))   p(d|Do(¬c))         predictions       38        4          20             20
   Causal Bayes                                                           Mean          31.50    20.21       24.38           21.37
  net predictions
                    36          5          33           14
                                                                           SD            8.15     9.45       10.51           11.46
      Mean         29.67      14.79      27.54         21.58
       SD          10.04      11.56      11.64         12.55        Diagnostic inferences. The mean estimates for the
                                                                    conditional frequency of A given C closely resemble the
trial learning of a causal model. Both diagnostic and
predictive inference proved sensitive to the distinction             ones in Experiment 1. Again the general pattern corresponds
between seeing and doing. The experiment also provides               to the predictions of causal Bayes nets. An analysis of
evidence for learners’ sensitivity to the implications of            variance with the factors ‘intervention vs. observation’ and
alternative confounding pathways (i.e., backdoors) for               ‘presence vs. absence of C’ as within-subjects factors again
observations and interventions. Although the statistical             yielded a significant interaction, F(1,23)=28.15, p<0.001,
patterns correspond to the predictions of causal Bayes nets,        MSE=47.96. The difference between the two estimated
the estimates were not perfect, of course. Specifically,            observational probabilities proved considerably larger than
learners had difficulties with correctly assessing cases in         the one between the two interventional probabilities. Again
which events were observed to be absent (see also                   participants judged the probability of A to be at a similar
Waldmann & Hagmayer, 2005). The complexity of the                   level regardless of whether C was generated or prevented.
model and the limited number of learning trials might have          Specifically, the observational questions differ significantly,
contributed to the imperfections. Nevertheless, the                 F(1,23)=63.88, p<0.001, MSE=61.43 while there is no
competency of the participants was remarkable and provides          difference        between    the    interventional     questions,
clear evidence against traditional learning theories that fail      F(1,23)=1.52, p=0.23. The diagnostic inferences show a
to account for complex causal model learning.                       remarkable grasp of the difference between seeing and
                                                                    doing despite the added complexity of the diagnostic
                          Experiment 2                              learning procedure. In this task, participants proved capable
The main goal of Experiment 2 was to test whether people            of ignoring the misleading temporal cue of the learning
learn and access causal models adequately when the                  procedure.
learning order does not match causal order. Previous                Predictive inferences. As in Experiment 1, participants were
research has shown that people can make correct predictions         asked to estimate the probability of D when C was observed
                                                                1465

or manipulated by an external intervention. However, in              We believe that the reason for this difference is located in
contrast to Experiment 1, the estimates considerably               the parameter estimation processes. Participants in
deviated from the causal Bayes net predictions (see Table          Experiment 2 observed the probability of B and C given D
4). An analysis of variance yielded no significant                 but had to infer the probability of D given B and C as a
interaction. In addition, the observational probability            parameter of the causal model. Such an inversion is
estimate of p(d|¬c) did not differ significantly from the          complicated and demanding. Therefore, the learning process
corresponding interventional probability estimate of               may have led to inadequate estimates of the model’s
p(d|Do(¬c)), F<1. Thus, there was no evidence that                 parameters. The diagnostic questions could be correctly
participants correctly differentiated between seeing and           answered by recognizing that interventions render the
doing in the predictive task. Probably the increased               manipulated variables independent of their actual causes,
complexity caused by the misleading temporal cue and the           which implies that solely the base rate p(A) needs to be
complicated inference, which required taking into account a        accessed for giving a correct response. In contrast, the
secondary confounding pathway (i.e., backdoor), exceeded           predictive questions can only be answered correctly if the
the information processing capacity of learners                    parameters of the full causal model are correctly estimated,
                                                                   and if the model is correctly altered for the intervention
  Table 4: Responses to predictive inference questions in          questions. Thus, if the parameters are not acquired correctly
      Experiment 2 (N=24) (Numbers indicate means of               during learning the inferences are likely to be wrong. This
        conditional frequency estimates for 40 cases.)             account explains the deficits shown in Experiment 2 using a
                                                                   causal Bayes net analysis of the task. Future research will
                    Observation            Intervention            have to develop psychological models of learning that
                 p(d|c)   p(d|¬c)    p(d|Do(c))   p(d|Do(¬c))      integrate competence and performance.
  Causal Bayes
 net predictions  36         5           33           14                                   References
     Mean        30.54     18.33       29.71         20.33         Hagmayer, Y., Sloman, S. A., Lagnado, D. A., &
                                                                     Waldmann, M. R. (in press). Causal reasoning through
      SD         10.10     13.26       10.90         11.74
                                                                     intervention. In A. Gopnik & L. Schulz (Eds.): Causal
                                                                     learning: Psychology, philosophy, and computation.
                     General Discussion                              Oxford: Oxford University Press.
Taken together, the results of the two experiments provide         Pearl, J. (2000) Causality. Cambridge: Cambridge
convincing evidence that learners are capable of                     University Press.
distinguishing between observations and interventions even         Price, P. C., & Yates, J. F. (1995). Associative and rule-
in a more naturalistic learning environment. These findings          based accounts of cue interaction in contingency
contradict traditional associative learning theories, which          judgment. Journal of Experimental Psychology: Learning,
fail to model causal-model learning and which are incapable          Memory, and Cognition, 21, 1639-1655.
of deriving correct predictions for actions after purely           Shanks, D. R. (1991). On similarities between causal
observational learning. The present experiments also                 judgments in experienced and described situations.
demonstrate a surprising grasp of the implications of                Psychological Science, 5, 341-350.
confounding pathways. Thus, the experiments strongly               Sloman, S. A., & Lagnado, D. A. (2005). Do we “do”?
support causal-model theory and causal Bayes nets as                 Cognitive Science, 29, 5-39.
theoretical accounts of causal induction.                          Spirtes, P., Glymour, C., & Scheines, P. (1993). Causation,
   However, Experiment 2 also shows that the competence              prediction, and search. New York: Springer-Verlag.
of learners only displays itself when the complexity of the        Waldmann, M. R. (1996). Knowledge-based causal
task does not exceed learners’ information processing                induction. In D. R. Shanks, K. J. Holyoak, & D. L. Medin
capacity (see also Waldmann & Walker, 2005). A popular               (Eds.), The psychology of learning and motivation, Vol.
strategy to deal with such impairments is to postulate two           34: Causal learning (pp. 47-88). San Diego: Academic
systems, a reasoning component that handles summarized               Press.
data, and an associative learning component that is                Waldmann, M. R., & Hagmayer, Y. (2005). Seeing versus
specialized for trial-by-trial learning (Price & Yates, 1995;        doing: Two modes of accessing causal knowledge.
Shanks, 1991). Although Experiment 1, which used a trial-            Journal of Experimental Psychology: Learning, Memory,
by-trial learning procedure, already weakens this account, it        and Cognition, 31, 216-227.
might still be speculated that learners fell back on an            Waldmann, M. R., & Martignon, L. (1998). A Bayesian
associative mode in Experiment 2 in which the learning task          network model of causal learning. In M. A. Gernsbacher
was more complex. However, the data of Experiment 2 are              & S. J. Derry, Proceedings of the Twentieth Annual
inconsistent with this theory. Learners were not generally           Conference of the Cognitive Science Society (pp. 1102-
impaired, only the predictive inferences were affected. The          1107). Mahwah, NJ: Erlbaum.
less complex diagnostic inferences showed a remarkable             Waldmann, M. R., & Walker, J. M. (2005). Competence and
grasp of the differences between seeing and doing despite            performance in causal learning. Learning & Behavior.
the misleading temporal cue. Only the more complex                 Woodward, J. (2003) Making things happen. A theory of
predictive inferences were negatively affected.                      causal explanation. Oxford: Oxford University Press.
                                                              1466

