UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Internal Simulation of Behavior Has an Adaptive Advantage
Permalink
https://escholarship.org/uc/item/9bh058bv
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 27(27)
Author
Broekens, Joost
Publication Date
2005-01-01
Peer reviewed
  eScholarship.org                                Powered by the California Digital Library
                                                                    University of California

                     Internal Simulation of Behavior has an Adaptive Advantage
                                                 Joost Broekens (broekens@liacs.nl)
                                  Leiden Institute of Advanced Computer Science, Leiden University,
                                          Niels Bohrweg 1, 2333 CA, Leiden, The Netherlands
                              Abstract                                  influence of simulation on the learning of an adaptive agent.
                                                                        Our experiments are based on biasing the agent’s action-
   In this paper we test the hypothesis that internal simulation of     selection by a simulation of its future interactions. We use a
   behavior has a robust adaptive (learning) advantage. From an         computational reinforcement learning (RL) model. Our
   evolutionary perspective, it is plausible to assume that agents      model allows a small amount of anticipatory simulation
   that simulate behavior have an additional survival value             concurrent with its reactive mode of operation. Our agent
   compared to those that do not. We present experimental               lives in a gridworld in which it must autonomously learn to
   results with a computational model of learning and decision-         forage. The agent has the computational model as "brain".
   making. Our experiments are based on biasing the agent’s
                                                                        We compare to what extent different simulation strategies
   action-selection by a simulation of its future interactions.
   Using our model, we show that this influence of simulation on
                                                                        result in a different learning performance.
   learning results in a significant learning advantage. Because           This paper is structured as follows. We briefly discuss the
   increased individual adaptation is an evolutionary                   theoretical background, our computational model,
   advantageous feature, this is a relevant result for the              experimentation method, and agent system. We end with a
   evolutionary plausibility of the simulation hypothesis.              discussion of our results and a conclusion.
   Keywords: action-selection; adaptive agent; reinforcement
   learning; simulation hypothesis; computational model.
                                                                                         Theoretical Background
                                                                        Interactivism (Bickhard 1998; 2001) is a crucial element to
                          Introduction                                  our approach, besides reinforcement learning (Sutton and
                                                                        Barto, 1998) and the simulation hypothesis. Interactivism
It is important to understand the nature of reasoning in                explains reasoning as resulting from the continuous
adaptive agents, both natural and artificial. This                      interaction between an agent and its environment.
understanding is needed, for example, to efficiently solve              Importantly, interactions can be active and prepared. Active
problems related to the frame problem and to understand                 interactions prepare a set of next possible interactions,
mechanisms of generalization versus specialization of                   referred to as interaction potentialities. These potentialities
knowledge. Reasoning itself assumes there is something to               become active when interaction with the environment
reason about, i.e., knowledge. Reasoning in the context of              matches, and thus prepare further interactions. The concept
adaptive agents thus implies there are (at least) two parallel          of active and prepared interactions is compatible with the
and complementary processes: first, knowledge acquisition,              concept of simulated action and perception and the
i.e., learning, and second the actions resulting from                   associative chaining of interactions, and is used in our
reasoning upon this acquired knowledge, i.e., behavior.                 computational model.
Reasoning essentially is about making an informed choice, a                Interactivism and the simulation hypothesis have several
choice informed by the acquired knowledge and made                      other important assumptions in common:
possible by the constraints of the body of the agent.                      1). Thinking is not necessarily symbolic or ideally logical,
    The simulation hypothesis (Hesslow, 2002) states that               two of the important limitations of earlier models of
thinking consists of internal simulation of interaction with            cognition. Animals do not necessarily think symbolically
the environment. This hypothesis is based upon three main               (see, e.g., Anderson, 2003; Bickhard, 2001), and frequently
assumptions: simulation of actions£actions can be prepared              make mistakes (Cohen and Blum, 2002; Damasio, 1994).
not necessarily resulting in execution£, simulation of                     2). Perception and action are two sides of the same coin,
perception£perceptions can be generated by the brain itself             and highly related through (at least) sensory-motor control
and not necessarily need external stimulation£, and                     areas. This avoids the input-function-output paradigm of
anticipation£the existence of associative mechanisms both               cognition and is an important point£highly related to issues
between real actions and perceptions and between simulated              surrounding the frame-problem£for future development of
actions and perceptions. Continuous activation of these                 a neuronal version of our model. However, in this paper we
associative mechanisms constructs chains of simulated                   do not focus on this point in detail.
prospective interactions, known as covert behavior. These                  3). These hypotheses closely relate to Damasio’s (1994)
interactions bias actual behavior, known as overt behavior.             concept of thinking as an "as if" loop, involving simulated
   Hesslow (2002) equates thinking with conscious thought.              actions that are evaluated by their somatic markers,
We use a broader definition of the simulation hypothesis,               emotional impact estimators. Somatic markers are attached
namely: reasoning, as defined above, is facilitated by the              to outcomes of scenarios through learning. Three systems
internal simulation of interaction with the environment. Our            are critically involved, the prefrontal cortex (PFC), the
definition of reasoning implies at least two relevant                   somato-sensory cortex (SSC) and the body. The two
processes, i.e., learning and behavior. We have studied the             mechanisms behind these markers are the body-loop and the
                                                                    342

"as if" loop. When the PFC signals the body to be in a                   It must be possible to use existing (possibly slightly
certain state, the SSC organizes itself according to the body,            changed) mechanisms to simulate and evaluate the near
i.e., the body-loop. The "as if" loop consists of the PFC                 future. This is compatible with Svensson and Ziemke
instructing the SSC to organize itself, bypassing the body.               (2004) who stress that one of the keys to our
The body-loop thus involves action, while the "as if" loop                understanding of embodied cognition is to understand
involves simulated action. The "as if" loop produces                      how sensorymotor processes and higher-order cognition
imagined£future£states, and the somatic markers that are                  share neural mechanisms, and this is compatible with
attached to these states equal the predicted emotional                    Cruse' s (2002) line of argument mentioned earlier.
outcome (reward/punishment). This signal is used to bias                In this paper we test the hypothesis that internal
decision-making (Damasio, 1994). Even though we do not               simulation of behavior has a robust adaptive advantage
model the body of the agent, the somatic marker concept is           without the need to make major changes to learning and
very useful to understand the relation between                       evaluation mechanisms.
reinforcement learning, emotion and decision-making.
                                                                        Hierarchical-State Reinforcement Learning
Evolutionary Continuity and Adaptive Advantage
An important consequence of the simulation hypothesis is
that agents do not need a separate "decision module" that
evaluates the simulated interactions. Simulated interactions
are grounded in existing sensory-motor systems, elicit
previously learned emotional consequences and thereby bias
action-selection (Hesslow, 2002), much like Damasio’s
decision-making based on somatic markers and Bickhard'         s
(2000) action-selection based on preference towards one
interaction outcome rather than another. If the system
prepares to act in two different ways, but for some reason
one of these ways appears "more attractive", then the system
will eventually prepare the more attractive action and
thereby make a choice. Although this approach will result in             Figure 1a-d. Example instance of a model resulting from
sub-optimal decisions at some points, it does circumvent the         the sequence of situations "1-2-1-3" presented to an initially
necessity to logically search through large spaces of actions          empty model. Dark-gray nodes are active at time t, light-
in order to find the "best" action and it provides an efficient                       gray nodes were active at t-1.
heuristic for action selection.
   Cruse (2002) argues along the same lines, stating that the        To be explicit about the changes that enable our
evolution of cognitive properties does not necessarily               computational model to use a small bit of anticipatory
requires the introduction of new additional modules. In              simulation, we first explain the basic model (Broekens and
Cruse'  s case by module he primarily means "internal world          DeGroot, 2004) that does not implement the simulation
model", but the argument contains the same message:                  hypothesis. It is a predictive, connectionist, interactivist-
evolutionary continuity.                                             based computational model of learning and decision-
   So, two key issues of the simulation hypothesis are: (1) no       making, though it is not neural. It has the following
evolutionary leap between humans and other mammals, and              characteristics. (1) Information is stored as a directed graph
(2) no need for a special mechanism to evaluate the                  in which nodes encode interactions. (2) Interactions have a
imagined future state (Hesslow, 2002). These imply that:             stability property, learned through continuous interaction
 Notwithstanding evolutionary continuity, for simulation            with the environment, analogous to Bickhard’s (2000)
      to be evolutionary plausible, it seems fair to assume that     concept of stabilization and destabilization. (3) Interactions
      in a population of agents, those agents that simulate          have a value property, learned through reinforcement. (4)
      behavior (having that feature) have additional survival        Interactions can form between other interactions resulting in
      value compared to those that do not. In the case of an         a hierarchy of more and more complex interactions (Figure
      adaptive agent, this survival value results in an              1). By doing so the model builds hierarchical predictions of
      important way from the enhanced learning performance           future states. (5) Its initial state is empty, and it grows by
      of the agent. The better an agent is at adapting to a new      interaction with the environment: i.e., the model is only
      task, the more likely it is to survive. We will further        used in an online learning setting. Every interaction takes
      refer to this enhanced learning performance as adaptive        the same fixed amount of time. (6) When a new situation
      advantage, not to be confused with the long term               presents itself at time t, the model automatically creates a
      evolutionary advantage an agent is implied to have as a        new node representing that situation. (7) Interactions that
      result of the survival value resulting from the enhanced       were active at time t-1 are connected to this new node.
      learning performance.                                          These connections are also represented as new nodes that
 The simulation mechanism must be robust; small                     can subsequently function as more complex interactions
      changes in other parts of the agent'        s information      (e.g., creation of node "1-2" in Figure 1b). (8) Nodes are
      processing system may not seriously downgrade the              created only if they do not yet exist (e.g., no second node
      adaptive advantage.                                            "1" in Figure 1c, but a new interaction node "(1-2)-1)" that
                                                                 343

connects node "1-2" with node "1"). (9) Typically, at any                 "1-2" has been active twice as often as node "1-3", then x1=
moment in time, many interactions are active, but at each                  1-2 (i.e., the usage of node "1-2") and x2 = 1-3 ò 1-2 (i.e.,
level of complexity only one interaction is active (e.g.,                 the usage of node "1-3"). Therefore, in this case pty(x) equals
Figure 1d, dark gray nodes). Active interactions include                  1/3, which is the naïve probability that x1 occurs under the
those that have just taken place at time t, but also those                assumption y at time t. If pty(x) drops below the threshold
between the interactions at time t-1 and the interactions at              q£the destabilization rate£x is deleted including all its
time t (e.g., node "1-3" and node "(2-1)-3" in figure 1d), etc.           dependencies. Consequently, consistent interaction with any
(10) This process of interaction-chaining continues until a               part of the environment results in a stable sub-graph of
maximum level, k, defining the maximum amount of                          nodes. Inconsistent interaction results in the destabilization
knowledge about situations in the past that is present in the             of the involved nodes and eventually in the deletion of these
current state of the model, as well the maximum number of                 nodes. We use q in our experiments to simulate different
interactions that can be active at one time (c.f. Figure 1). So           rates of forgetting. Note that pty is the local probability
a node is a "gate" between a sequence of previous                         function for y and that every node has its own function pty
interactions and a set of potential next interactions. We call            (learned by interaction with the environment). Therefore, at
such a node an "interactron".                                             any time t at most (and during normal operation exactly) k
                                                                          of these functions are active, for there are k active nodes.
                                                                              Our reinforcement mechanism has two parts. First, when
                                                                          the agent acts, all active nodes y receive a reinforcement
                                                                                                
                                                                          signal, rt, at time t that changes the direct reinforcement
                                                                          value, ly, of these nodes with a learning rate r:
    Figure 2a-c. Three different experiments. Agent is black,
                                                                                             lt    1
                                                                                                      y = lt y + (r t - lt y ) * r
lava is dark (red), food='   F', roadblock='   B'
                                                , start location=’ S’         Second, every node has an indirect inherited
       Tasks from left to right: find food, forage, invest.               reinforcement n£the result of the back-propagated markers
                                                                          of hierarchically higher nodes. l and n are summed into the
   Our experiments are performed in gridworlds. A                         final value l+n, reflecting Damasio'                       s assumption that the
gridworld is a two dimensional grid containing positively                 somatic marker of a predicted situation equals its own value
and negatively reinforced locations and objects, in our case,             added to the sum of all the cumulative values of the
lava (negative reinforcement of –1), roadblocks (–0.5), food              interactions it predicts. When a node y is active, the marker
                                                                            t
(+1.0) and empty£neutral£cells (Figure 2). The agent is                      y(x) of any hierarchically higher node x prepared by node
able to walk on any type of cell, but is discouraged to walk
                                                                                           
                                                                          y, is used to update the indirect reinforcement, ny, of node y:
                                                                                                        Êm
on the lava (by the negative reinforcement). The agent                                                  n( y)
selects an action from its set of potential actions A={up,                              n t 1
                                                                                              y     =            t
                                                                                                                   y ( xi ) * p t y ( xi )
down, left, right}, executes the action in the gridworld and                                             i 1
perceives the result of that action. One single interaction                   Markers are thus propagated back through the interaction
with the environment (also referred to as situation) is                   hierarchy only when the interactions to which they are
defined as an action-perception pair. The agent'         s perceptual     attached are prepared. This lazy propagation reflects the
field has either a chessboard (Figure 2b) or a cityblock                  probabilistic properties of the interactions with the
(Figure 2ac) metric. For example, in Figure 2c, the agent                 environment. This mechanism follows standard TD learning
would perceive something like "plppp" representing the                    mechanisms (Sutton and Barto, 1998) except, e.g., the
(l)ava left of the agent and the (p)ath above, right, below,              probabilistic value-function defined per node.
and under the agent. If the agent came to this cell by moving                 Action-selection is based on a winner-take-all (WTA)
to (d)own, the interaction the model receives would be                    mechanism and biased by the of all prepared nodes. Note
"dplppp" (replacing, e.g., every node "1" in Figure 1).                   that any interaction is composed of both an action and a
                                                                          perception. All prepared interactions inhibit (negative ) or
Reinforcement Learning, Probabilities and Action                          exhibit (positive ) the level of activation lta_h at time t of
Selection                                                                 the agent'  s possible actions ah=a_h³A in the following way:
                                                                                                  l t ah = Ê
                                                                                                               Ê
                                                                                                                k n ( yi )
                                                                                                                             m t yi ( xi j )
In our model we have implemented stabilization and                                                                         *
destabilization of interactions based on the insights of
Bickhard (2000). If a node x is activated, the usage x of that                                                i 1    j 1
                                                                          with active nodes yi,, and nodes xij where i denotes
node is increased by 1. The function pty(x) calculates the
                                                                          dependency on yi and xij prepares ah (indicated by *).
conditional usage of node x under the assumption that y is
active at time t and is defined by:
                                                                              Additionally, if there are any good actions (any lta_h>0)
                                     Êu
                                     n( y )
                   p t y ( x) = u x
                                                                          the best action ah, i.e., lta_h=max(la_1,… ,la_m), is selected. If
                                            xi                            there are only bad actions (all lta_h<0) a stochastic selection
                                      i 1
                                                                          is made based on la_1,… ,la_m; the action with the highest
Where x1,…,xn(y) the potential interactions predicted by node
                                                                          activation therefore has the highest chance of being chosen
y, n(y) the number of potential interactions predicted by y
                                                                          resulting in a probabilistic WTA action selection. So,
and x³{x1,…,xn(y)}. For example, in Figure 1d, if we assume
                                                                          action-selection is simultaneously based on generic and
that y="1" and active at time t, that x= "1-3", and that node
                                                                      344

specific knowledge, allowing it to learn and use generic                In the simulation step (2) the stabilization-destabilization
aspects of the environment as well as more specific ones.            process is deactivated. Earlier experiments showed that,
   Experimental results have shown that this model is able to        when active, the agent’ s behavior is inconsistent with the
learn, unlearn and reuse information, and to solve a T-maze          environment, probably because simulating certain
like selection tasks where the agent learns to conditionally         interactions alters the knowledge of the environment
use a crossing (Broekens and DeGroot, 2004).                         because the conditional probabilities of the nodes change by
                                                                     simulating an interaction, distorting the real probabilities.
 Internal Simulation and Action-Selection Bias                          After resetting the state to one that is appropriate to the
Our basic predictive model does not include internal                 current situation (step 3), the simulation mechanism results
simulation of behavior. To study the influence of simulation         in: (1) a propagation of the markers m of the predicted
on learning we add the following capability: after every real        interactions at time t+1 to the n of the simulated interactions
interaction with the environment, the model simulates one            at time t according to the reinforcement learning principle
time-step ahead. Analogous to what Hesslow (2002)                    used, and (2) an update of the direct reinforcement value l
describes, the model always is one step ahead of the actual          of the simulated interactions at time t based on their own m.
situation. To enable simulation we changed the model in the          This means that, without further changes, simulation by
following way. Instead of selecting an action based on past          itself not only propagates predicted reward and punishment
interactions the following process is executed:                      but also changes the direct reinforcement from the
1. Select: at time t select a set of to-be-simulated                 environment. These learning effects are interesting to study,
     interactions from the interactions predicted by all k           however, here we want to study the effect of simulation on
     active nodes.                                                   learning by biasing action-selection. Therefore, after action-
2. Simulate: send the selected interactions to the model as          selection, step 5 is needed.
     if they were real interactions. The model advances to              The changes to the actual architecture of information
     time t+1.                                                       processing are minimal, an important fact in light of the
3. Reset-state: to be able to select an appropriate action,          evolutionary continuity argument of the simulation
     reset the model’ s state (the active interactions) to the       hypothesis. In a more dynamic model step 2, 3 and 5 would
     previous timestep, i.e., time t.                                have to be reconsidered.
4. Action-selection: select the next action using the
     standard selection mechanism (explained later). The                                Experimental Setup
     propagated markers of the simulated interactions have           Every simulation strategy is tested in three different tasks
     biased this action-selection.                                   that involve finding food, each task in its own unique
5. Reset-markers: reset m, l and n of the interactions that          environment. The darker cells around the agent in Figure 2
     were changed at step 2 (simulation) to the values of m,         show the agent’ s perceptual area for every task. Since we
     l and n of these interactions before step 2.                    also want to know how robust this advantage is, we vary the
   Step 1 selects predicted interactions to be simulated. In         learning rate r and the destabilization threshold q (see Table
our experiment we have used four different selection                 1). For the first task the agent has to learn its way from a
mechanisms (also referred to as simulation strategy).                randomly changing starting location (S in Figure 2a) to a
 First, no simulation (NON). The actions are selected as            randomly changing food location. When successful, the
     described in the previous section and the 5-step                agent is replaced at a starting location and tries again.
     simulation procedure is not executed.                           Repeating this process enables the agent to learn how to get
 Second, simulation of the predicted best interaction               from both starting locations to both food locations.
     (BEST). The winning interaction of the WTA selection               For the second task the agent has to learn how to optimize
     resulting from step 1 is sent to the model for simulation       foraging (Figure 2b). Now, the agent is initially placed in
     (step 2). Every real interaction is accompanied by a            the environment, after which it should just explore and find
                                                                     food. The food locations are randomly selected, and the
     reinforcement signal. As this is a simulation we lack
                                                                     challenge for the agent is to forage.
     such a signal. Instead, this signal is simulated using the
                                                                        For the third task, the agent has to learn to overcome an
     m of the winning interaction as reinforcement, so we            initial negative interaction (road-block, B in Figure 2c,
     simulate the predicted interaction and its associated           reinforcement of -0.5) in order to get to a larger positive
     marker, analogous to Damasio'    s (1994) "as if" loop.         one (food, "F" in Figure 2c). We changed the reinforcement
 Third, a selection of not just the best, but the predicted         of the food to +1.75 in order to compensate for the negative
     50% best interactions, a more balanced selection,               reinforcement of the road-block. With this experiment we
     (BEST50). Again we simulate the reinforcement signal            wanted to test how the simulation strategies handle an
     using the m’ s of the simulated interactions.                   "investment". By setting the reinforcement of the food equal
 Fourth, all of the predicted interactions (ALL).                   to 1.75 the average reinforcement of the food remains 1.0.
   In essence, BEST, BEST50 and ALL simulate three                      Every experimental setup (c.f. Table 1) is run 15 times.
different values for the selection threshold of the WTA              For every run the agent has 255 trials to find the food. It has
interaction selection (ranging from high to low respectively)        to learn the properties of the task within these 255 trials. For
that is used to select the interactions for the simulation step.     every trial the agent has a maximum of 1000 steps to
                                                                     actually get to the food. If the agent reaches this maximum,
                                                                     the agent advances to the next trial.
                                                                 345

     Table 1: Venn diagrams of statistical difference between      explains our quantitative comparison approach (Table 1).
    simulation strategies. Every diagram (cell in table) is a      We compare the average of the total number of steps needed
  representation of these differences in one setting. Overlap      to finish one run (one run equals 255 trials, we average over
   means there is no statistically significant difference (one     15 runs). If every step is assumed to cost some effort, this
  tailed t-test, a=0.05, n=15). Higher is worst (more steps),      average is a measure for the performance of a specific
  lower is better (less steps). Light is NON and dark is ALL       setting (a tuple of strategy, task, r and q). Comparison of
            and interpolated for BEST and BEST50.                  these averages gives an overall idea of the adaptive
                                                                   advantage of the different simulation strategies. The lower
   Task                                                            the average, the better the strategy is. Also, by comparing
   Find food                                                       these averages, we can identify the relation between
                                                                   strategy, task, r and q. Individual learning curves (Figure 3)
                                                                   are not needed to compare the overall performance of
                                                                   strategies and will only be considered if needed to explain a
                                                                   certain relation in more detail.
                     E
                                                                    a                               b
   Forage
                                                                    c                               d
                                                                         Figure 3. Prototypical simulation effect (ab), a=NON,
                                                                        b=ALL, and specific simulation effect (cd), c=NON,
                                                                          d=ALL. Trials on x, steps to complete a trial on y
                                                                      In general, the following results have been observed. The
                                                                   ALL simulation strategy has a robust adaptive advantage
   Invest                                                          compared to the other strategies, specifically at the forage
                                                                   task. ALL is either among the best-performance strategies or
                                                                   there is no difference between strategies at all. This suggests
                                                                   that internal simulation of interactions, even if it is just one
                                                                   step ahead, helps an agent to learn a task by providing an
                                                                   extra action-selection bias, and thereby provides an adaptive
                                                                   advantage for the agent. In general this is because ALL
                                                                   either converges faster or better (or both). As a prototypical
                                                     E             example, observe the difference in learning curves between
                                                     E
                                                                   (NON, forage, r=0.5 and q=0.01) and (ALL, forage, r=0.5
                                                                   and q=0.01) in Figure 3ab. More specific, this is because
                                                                   the forgetting rate and the difficulty of the task disrupt
                                                                   learning almost entirely in the NON case (invest, r=0.5 and
                                                                   q=0.01, Figure 3cd). Two notable exceptions (’E’, Table 1)
                                                                   are at the find food task with r=0.8, q=0 and at the invest
                  Results and Discussion                           task with r=0.8 and q=0.05. The first shows that BEST
                                                                   performs significantly better than NON, the second shows
Because our goal is to compare the relative effect in terms        that BEST50 performs significantly better than the rest.
of overall performance between different simulation                   To explain these effects we consider the following three
strategies, not to optimize parameters, r and q have been
                                                                   reasons. First, to solve the forage task the agent requires an
chosen based on workable values and we have not done an            explorative, broad view. There is no best path to learn, and
exhaustive search for "the best" parameters. This also             instead the agent has to learn where food can be found on
                                                               346

average. If the agent always tries the local "best" solution         reactive systems, see e.g. Cruse, 2002), this is an important
(i.e., uses NON or BEST), it runs a larger risk of ending up         shortcoming of our computational model. If the world is
in a local minimum. ALL forces the agent to simulate all of          complex, many interactions can be prepared (planned),
its prepared interactions, including those that appear bad but       resulting in an explosion of simulation effort (specifically
have a good result at t+2. Simulation of an apparent bad             for ALL). This is highly related to both (a) the fact that
interaction can still bias action-selection in a way that favors     nodes are distinct even if they share many of the features of
that interaction if the resulting interaction at t+1 is good.        other nodes and (b) the fact that our model does not extract
This results in a broader view. In contrast, to solve the find       relevant features from the environment, but instead features
food task, the agent needs to quickly propagate back the             are encoded in the nodes and assumed equally important.
values, it has to find the best path. A broad view is not            We are working on a neuronal implementation of the nodes
necessary here, for find food is a simple and very goal              in order to start to address this issue. However, in this paper
directed task. The simulation strategy that quickly                  we have focussed on the overall mechanism of simulation.
propagates the positive reinforcement back to the beginning          We believe that the positive adaptive effect of this
performs best. Both BEST and NON only try the best                   mechanism exists even if our use of nodes changes.
prepared interaction. So the advantage of the broad view of
ALL compared to NON and BEST is less important in the                                         Conclusion
find food task. This is supported by the fact that at the find       Using our computational model, we have shown that the
food task ALL is significantly better than BEST for only 2           influence of simulation on learning has a significant
settings, while in the forage task ALL is significantly better       learning advantage. This positive effect occurs in three
than BEST for all settings.                                          different learning tasks, and for a variety of learning rates as
   Second, compared to NON, ALL/BEST50 is robust to                  well as rates of forgetting. Since increased individual
different rates of forgetting q, but this effect is specifically     adaptation is an evolutionary advantageous feature, this is a
noticeable in the invest task, where even a small q (=0.01)          relevant result for the evolutionary plausibility of the
disrupts learning for NON but not for ALL. This task is              simulation hypothesis. We realize that such conclusions
difficult, so the agent needs more time to learn. This means         based on computational models should be made carefully.
that there is more time to forget parts of the already built
world model. Because ALL and BEST50 can simulate
interactions that appear bad but are good at t+2, they also                              Acknowledgements
have a higher chance at influencing the action-selection             I would like to thank Fons Verbeek and Walter Kosters for
process such that a good backup action is chosen when a              helpful comments and ideas.
part of the model has been forgotten. ALL and BEST50
provide a more balanced heuristic to select the next                                          References
interaction. This is supported by the more hockey stick              Anderson M. L. (2003). Embodied cognition: A field guide.
shaped learning curve of (ALL, invest, r=0.5 and q=0.01)               Artificial Intelligence, 149. 91-130.
(Figure 3d) compared to NON (Figure 3c). It seems that               Bickhard. M.H. (1998). Levels of representationality.
NON forgets knowledge at such a rate that performance can              JETAI, 10, pp. 179-215.
actually get worse (around t=60), while the performance of           Bickhard. M.H. (2000) Motivation and emotion: An
ALL first increases quickly after which it keeps increasing            interactive process model. In: R.D. Ellis and N. Newton
slowly.                                                                (eds.), The Caldron of Consciousness. New York: J.
   Third, learning rate seems to affect NON the most (not              Benjamins.
shown in figures). This is due to the fact that learning             Broekens J. and DeGroot D. (2004). Emergent
depends on how quickly the model propagates back the                   representations and reasoning in adaptive agents. In Proc.
positive reinforcement of the food. Since all simulation               3rd International Conference on Machine Learning and
strategies simulate 1 step ahead, the positive reinforcement           Applications (pp. 207-214).
is visible earlier, thus affecting simulation strategies less        Cohen J.D. and Blum K.I. (2002). Reward and decision.
than NON. This is also the reason why simulation is                    Neuron, 36. 193-198.
dramatically better than NON on the invest task (c.f., Figure        Cruse H. (2002). The evolution of cognition£a hypothesis.
3cd). The roadblock investment is a problem for NON, but               Cognitive Science, 27. 135-155
simulation can overlook the investment to the food reward.           Damasio. A.R. (1994). Descartes’ error: Emotion, reason,
   Last, at this point it is unclear why BEST50 performs               and the human brain. New York: G.P. Putnam.
better than ALL at the invest task, r=0.8 and q=0.05.                Hesslow, G. (2002). Conscious thought as simulation of
                                                                       behaviour and perception. TICS, 6, 2002, 242-247.
Cognition, Planning, and Simulation of Behavior                      Sutton R. S. and Barto A. G. (1998). Reinforcement
We currently connect nodes£and encode and compare                      learning: An introduction. Cambridge, Massachusetts:
information in these nodes£in a way that works in simple               The MIT Press.
gridworlds but introduces problems for real-world                    Svensson H. and Ziemke, T (2004). Making sense of
navigation. Specifically, the number of nodes exponentially            embodiment: Simulation theories and the sharing of
increases if the complexity of the world (and actions of the           neural circuitry between sensorimotor and cognitive
agent) increases. In light of the hypothesis that cognitive            processes. Proc 26th Ann. Conf. of the Cogn. Sci. Soc.
systems are those that have the ability to plan (in contrast to        Mahwah, NJ: Lawrence Erlbaum.
                                                                 347

