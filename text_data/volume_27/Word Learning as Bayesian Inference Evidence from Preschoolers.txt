UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Word Learning as Bayesian Inference: Evidence from Preschoolers
Permalink
https://escholarship.org/uc/item/38m7m5mm
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 27(27)
Authors
Tenenbaums, Joshua B.
Xu, FEi
Publication Date
2005-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

               Word Learning as Bayesian Inference: Evidence from Preschoolers
                            Fei Xu                                                       Joshua B. Tenenbaum
                      fei@psych.ubc.ca                                                         jbt@mit.edu
                  Department of Psychology                                      Department of Brain and Cognitive Sciences
                University of British Columbia                                     Massachusetts Institute of Technology
                            Abstract                                      Settling on one hypothesis by eliminating all others as
                                                                    incorrect amounts to taking a deductive approach to the
     Most theories of word learning fall into one of two            logical problem of word learning. The learner essentially
   classes: hypothesis elimination or associationist. We            deduces the word’s meaning from a set of premises, which
   propose a new approach to word learning within a                 include the assumption that the word maps onto one of the
   Bayesian framework. Tenenbaum and Xu (2000)                      learner’s hypotheses and the a priori knowledge and
   presented a Bayesian model of adults learning words              observational data that rule out all but one hypothesis. The
   for hierarchically structured categories. We report two          success of word learning is then explained by the deductive
   experiments with 3- and 4-year-old children, providing           validity of this inference. Thus we will sometimes refer to
   evidence that the basic principles of Bayesian                   hypothesis elimination approaches as deductive approaches to
   inference are employed when children acquire new                 word learning.
   words at different hierarchical levels. Implications for               To illustrate how word learning has traditionally been
   theories of word learning are discussed.                         explained within a deductive framework, let us return to our
                                                                    opening question of how a child could possibly infer the
                         Introduction                               meaning of the word “dog” from a typical labeling event.
     The problem of word learning has been a well-cited             One influential proposal has been that children come to the
example of the general problem of induction. With each              task of word learning equipped with strong prior constraints
referential act, e.g., “Look, a dog!” there are an infinite         on viable word meanings (e.g., Markman, 1989), allowing
number of hypotheses for the meaning of the word “dog” that         them to rule out a priori many logically possible but
is consistent with the data (Quine, 1960). The word could           psychologically unnatural extensions of a word. Often it is
refer to all (and only) dogs, all mammals, all animals, all         most natural to view these constraints as giving structure to
Dalmatians, this individual Max, all dogs plus the Lone             the learner’s hypothesis space, but they could also be seen as
Ranger’s horse, all dogs except Labradors, all spotted things,      eliminating the implausible hypotheses from a larger space of
the front half of a dog, undetached dog parts, things which are     all logically possible concepts.
dogs if first observed before next Monday but cats if first               Two classic constraints on the meanings of common
observed thereafter, and on and on. Yet despite this severe         nouns include the whole object constraint and the taxonomic
under-determination, even 2- or 3-year-old children seem to         constraint (Markman, 1989). The whole object constraint
be remarkably successful at learning the meanings of words          requires words to refer to whole objects, as opposed to parts
from just one or a few positive examples (Bloom, 2000;              of objects or attributes of objects, thus ruling out word
Carey, 1978; Markman, 1989; Regier, 1996; among others).            meanings such as the front half of a dog, or undetached dog
How do they do it?                                                  parts. The taxonomic constraint requires words refer to
     Most theories of word learning fall under what we call         taxonomic classes, typically in a tree-structured hierarchy of
the hypothesis elimination approach to learning. The learner        natural kind categories (Keil, 1979), thus ruling out word
effectively considers a hypothesis space of possible concepts       meanings such as all dogs plus the Lone Ranger’s horse, all
onto which words will map, and assumes that each word               spotted things, all running things or all dogs except
maps onto exactly one of these concepts. The act of learning        Labradors. Whether these constraints are specific to the
consists of eliminating incorrect hypotheses about word             domain of word meaning or reflect more general restrictions
meaning, based on a combination of a priori knowledge and           on the structure of natural kind concepts is controversial (e.g.,
observations of how words are used to refer to aspects of           Tomasello, 2001), but their importance in guiding the process
experience, until the learner converges on a single consistent      of word learning is fairly well accepted.
hypothesis. More precisely, some logically possible                       In most cases, such as our example of a child learning the
hypotheses may be ruled out a priori because they do not            word “dog”, these constraints are useful but not sufficient to
correspond to any psychologically natural concepts, e.g., the       solve the inference problem. Even after ruling out all
hypothesis that “dog” refers to things which are dogs if            hypotheses that are inconsistent with a typical labeled
observed before Tuesday but cats if observed thereafter.            example (e.g., Max the Dalmatian), a learner will still be left
Other hypotheses may be ruled out because they are                  with many consistent hypotheses that also correspond to
inconsistent with examples of how the word is used, e.g., the       possible meanings of common nouns. How are we to infer
hypotheses that “dog” refers to all and only terriers, can be       whether a word that has been perceived to refer to Max
ruled out upon seeing the example of Max the Dalmatian.             applies to all and only Dalmatians, all and only dogs, all
                                                                    canines, all mammals, or all animals, and so on? This
                                                                    problem of inference in a hierarchy is interesting in its own
                                                               2381

right, but more importantly as a special case of the                inductive leaps based only on suspicious coincidences in the
fundamental challenge faced by deductive approaches to              observed data. This allows the Bayesian framework to
word learning. In most interesting semantic domains, the            explain crucial fast mapping phenomena and other word
natural concepts that can be named in the lexicon are not           learning behaviors that neither previous framework can make
mutually exclusive, but overlap in some more or less                sense of.
structured way. In other words, most objects can be                       We will study a phenomenon in the context of learning
described with more than one word. Thus no example can              words for taxonomic categories, which strongly suggest that a
ever rule out all but one of the a priori plausible hypotheses,     statistical inference mechanism is at work. Suppose that after
as the hypothesis elimination framework requires for                observing Max the Dalmatian labeled a “fep”, and inferring
successful inductive inference.                                     (based on a basic-level preference within a taxonomic
     While deduction or hypothesis elimination may be the           hypothesis space) that “fep” refers to all and only dogs, we
dominant framework in which researchers have sought to              then see three more objects labeled as feps, each of which is
understand the inferential processes underlying word                also a Dalmatian. These additional examples are consistent
learning, it is not the only standing candidate. Connectionist      with exactly the same set of taxonomic hypotheses that were
or neural network models (e.g., Regier, 1996; Gasser &              consistent with the first example; no potential meanings can
Smith, 1998) treat word learning as a kind of associative           be ruled out as inconsistent that were not already inconsistent
learning process. Similarity-based models treat word learning       after seeing one Dalmatian called a “fep”. Yet after seeing
as a process of exemplar memorization and generalization by         these additional examples, the word “fep” seems relatively
graded matching (Landau, Smith, & Jones, 1988). By using            more likely to refer to just Dalmatians than to all dogs.
internal layers of “hidden” units and appropriately designed        Intuitively, this inference appears to be based on a suspicious
input and output representations, or appropriately tuned            coincidence: it would be quite surprising to observe only
similarity metrics, these models are able to produce abstract       Dalmatians called “fep” if in fact the word referred to all dogs
generalizations of word meaning that go beyond what one             (and if the first four examples were a random sample of “fep”
might expect from their roots as models of the simplest             in the world).
animal learning or memory processes. They also have the                   In previous research we presented evidence that adults’
potential to capture some aspects of word learning that are not     performance in a word learning task accords with these
easily explained within the deductive framework, such as the        intuitions and we presented a Bayesian model of adults’
graded nature of many generalizations, the noise tolerance of       generalization behavior (Tenenbaum & Xu, 2000). Given a
learning, or the varying degrees of confidence that word            hypothesis space and one or more examples of a novel word’s
learners may have in their inferences. However, associative         referents, the learner evaluates all hypotheses for candidate
or similarity-based models have not replaced hypothesis             word meanings by computing their posterior probabilities,
elimination as the dominant way of thinking about word              proportional to the product of prior probabilities and
learning (Bloom, 2000). In large part, this is probably             likelihoods. The prior probabilities embody all of the
because they have not yet exhibited the essential capacities of     learner’s beliefs about which hypotheses are more or less
fast mapping. The term “fast mapping” has come to mean              plausible, independent of the observed examples. Constraints
different things for different researchers. We emphasize            on word meaning (e.g., the whole object assumption or the
children’s ability not only to form a link between a                taxonomic assumption) are part of the prior.                Prior
phonological form and a meaning, but also to generalize to          probabilities may be innate or learned, and they may change
novel instances based on one or just a few positive examples        over time as the lexicon grows. The likelihood captures the
(see Carey & Bartlett, 1978; Markman, 1989; Markson &               statistical information in the examples. It reflects the
Bloom, 1997; Waxman & Booth; 2001 for other definitions).           learner’s expectations about which examples are likely to be
     While both deductive and associationist models offer           observed given a particular hypothesis about word meaning,
certain insights into the processes of word learning, we            e.g., the learner assumes that the observed examples are a
believe that neither approach provides an adequate                  representative sample of the word/concept. The posterior
framework in which to explain how people actually learn the         captures the learner’s degree of belief that the hypothesized
meanings of words. Here we propose a novel approach based           word meaning is the true meaning of the word given the
on principles of rational statistical inference. Our framework      examples.
combines the principal advantages of both deductive and                   Tenenbaum and Xu (2000) specified how these various
associationist frameworks: it supports the rational inferences      terms could be instantiated in the Bayesian model. For the
underlying generalization in fast mapping, but it also exhibits     case of learning words for kinds, hypotheses were assumed to
a graded sensitivity to uncertainty in prior knowledge and the      correspond to classes in a hierarchical taxonomy of kinds. A
input. It can be viewed as a natural extension of the               representation of subjects’ taxonomies was obtained by
hypothesis elimination approach, in which hypotheses are            hierarchical clustering of similarity judgments. The clusters
evaluated not by deductive logic but by the machinery of            included subordinate, basic-level, and superordinate
Bayesian probability theory. Thus hypotheses are not simply         categories, as well as others (see T&X for details). The more
ruled in or out, but scored according to their probability of       distinctive a cluster was in terms of similarity, the higher the
being correct. The result is a much wider spectrum of               prior probability was that a word would map onto that cluster.
inferences, ranging from complete certainty to complete             The likelihood reflects a size principle: Assuming that the
uncertainty, and including both logical deductions and true         examples are randomly sampled from the word’s extension,
                                                                2382

hypotheses corresponding to smaller extensions are preferred        says “yes.”] Then the experimenter says, “Good! Now, Mr.
relative to larger extensions, and the preference increases         Frog speaks a different language and he has different names
exponentially as the number of consistent examples increases.       than we do for his toys. He is going to pick out some of them
This captures the intuition of “suspicious coincidence": If the     and he would like you to help him pick out the others like the
first example of "fep" is a Dalmatian, either all Dalmatians or     ones he has picked out, okay?” [Child says ‘ok.’] Three
all dogs may be plausible hypotheses. But if the first three        novel words were used: blick, fep, and dax.
examples of “fep” are all Dalmatians, the word seems more           One-example Condition.
likely to refer to just the Dalmatians than to all dogs. Lastly,          On each trial, the experimenter picked out an object from
generalization to new objects is determined by averaging the        the array, e.g., a green pepper, and labeled it, “See? A blick.”
predictions of all hypotheses weighted by their posterior           Then the child was told that Mr. Frog is very picky. The
probabilities.                                                      experimenter said to the child, “Now, Mr. Frog wants you to
      Previous studies showed that adults’ data fit well with       pick out all the blicks from his toys, but he doesn’t want
this model. What about children who are in the midst of             anything that is not a blick. Remember that Mr. Frog wants
rapidly learning new words?                                         all the blicks and nothing else. Can you pick out the other
                                                                    blicks from his toys?” The child was then allowed to choose
                        Experiment 1                                among the 24 test objects to find the blicks and put them in
     Experiment 1 investigated how 4-year-old children learn        front of Mr. Frog. If a child only picks out one toy, the
words for subordinate, basic-level and superordinate                experimenter reminded him/her, “Remember Mr. Frog wants
categories. Children were taught novel words for object             all the blicks. Are there more blicks?” If a child picks out
categories and were asked to generalize these words to new          more than one object, nothing more was said to encourage
objects. As in T&X (2000), two factors were manipulated:            him/her to pick out more toys. At the end of each trial, the
the number of examples labeled (1 vs. 3) and the range              experimenter said to the child, “Now, let’s put all the blicks
spanned by the examples (e.g., three Dalmatians, three kinds        back and play the game again. Mr. Frog is going to pick out
of dogs, or three kinds of animals).                                some more toys and he would like you to help him pick out
Method                                                              others like the ones he picks, okay?” Then another novel
Participants                                                        word was introduced as before.
     Participants were thirty-six 4-year-old children (mean               Each child received three trials, one from each of the
age 4 years 1 month, ranged from 3 years 6 months to 5 years        three superordinate categories, e.g., a Dalmatian (animal), a
0 months). All participants were recruited from the Greater         green pepper (vegetable), and a yellow truck (vehicle). The
Boston area by mail and subsequent phone calls. English was         order of the trials and the novel words used (blick, fep, and
the primary language for all children.                              dax) were counterbalanced across participants.
Materials                                                           Three-example Condition.
     The stimuli were 45 objects. They were distributed                   On each trial, the procedure was the same as in the one-
across three different superordinate categories (animals,           example trial with one important difference: The
vegetables, and vehicles) and within those, many different          experimenter picked out one object, labeled it for the child,
basic-level and         subordinate-level categories (e.g.,         e.g., “See? A fep.” Then she picked out two more objects, one
Dalmatians/terriers/hush puppies, pelicans, cats, etc.). These      at a time, and labeled each one for the child, e.g., “Look,
stimuli were divided into a training set of 21 stimuli and a test   another fep!” The order of the superordinate category
set of 24 stimuli. The test stimuli included subordinate            (animal, vegetable, and vehicle), the range spanned by the
matches (e.g., other terriers), basic-level matches (e.g., other    examples (subordinate, e.g., three very similar Dalmatians;
dogs), and superordinate matches (e.g., other animals).             basic, e.g., three different dogs; superordinate, e.g., three
Design and Procedure                                                different animals), and the novel words were counterbalanced
     Each child was randomly assigned to one of two                 across participants.
conditions: the One-example Condition and the Three-                Results
example Condition. Each child received a total of 3 trials. In            Since no child chose any of the distracters in this
the One-example Condition, every child received 3 trials, one       experiment, all analyses excluded the distracter scores.
from each domain. In the Three-example Condition, because           Figure 1 shows the percentage of responses at the various
it might be too demanding to ask children for generalizations       hierarchical levels.
at all three levels within a single domain, each child received                            1
one trial in each of the three domains.
                                                                           Choice prob.
     Children were introduced to a puppet, Mr. Frog, and
were told that they were helping the puppet who speaks a                                  0.5
different language to pick out the objects he wants. The test                                                                             sub
                                                                                                                                          basic
array of 24 objects was randomly laid out in front of the child        Examples:                                                          super
                                                                                                 1       3 sub.    3 basic   3 super.
and the experimenter. The experimenter held the puppet and
said to the child, “This is my friend Mr. Frog. Can you say                                 Figure 1: Generalization data from Experiment 1
“hello” to Mr. Frog?” [Child says “Hello.”] These are all of
Mr. Frog’s toys, and he would like you to play a game with                 Two questions are addressed with planned t-tests. First,
him. Would you like to play a game with Mr. Frog?” [Child              did children behave differently in the 1-example trials
                                                                2383

compared with the 3-example subordinate trials when they             when labeling a single example vs. three independent
were given 1 vs. 3 virtually identical exemplars? More               examples. However, given that each object was labeled once,
specifically, did they show a falling off at the basic level in      the three-example trials contained three times as many
the 1-example trials and did they restrict their generalization      labeling events as the one-example trials. Thus we are not
to the subordinate level in the 3-example trials? Second, did        able to tell if the learner kept track of the number of examples
the 3-example trials differ from each other depending on the         labeled or simply the number of labeling events (i.e., word-
range spanned by the examples? More specifically, did                object pairings). This is particularly important since some
children modify their generalization to the most specific level      associative word-learning models (e.g., Colunga & Smith,
that was consistent with the set of exemplars?                       2001) claim that keeping track of word-object pairings is the
     To investigate the first question, we compared the              very mechanism of children’s word learning. To distinguish
percentages of responses that matched the example(s) at the          the Bayesian approach from associative approaches, it is
subordinate, basic-level, or the superordinate level. On the         important to tease apart these possibilities. In Experiment 2
one-example trials, participants chose more subordinate              we equated the number of labeling events between the 1- and
(85%) than basic-level matches (31%), and more basic-level           3-example trials by labeling the single object three times.
than superordinate matches (3%) (p < .0001 for both
comparisons). When presented with three very similar                                                   Experiment 2
exemplars from the same subordinate category, participants        Method
chose more subordinate matches (83%) than both basic-level        Participants
(13%) and superordinate matches (3%) (p < .0001 for both               Participants were thirty-six 4-year-old children (mean
comparisons). Furthermore there was a reliable difference in      age 4 years 0 months, ranged from 3 years 6 months to 5
basic-level matches (31% vs. 13%, p < .01).                       years 0 months). Participants were recruited as in Experiment
     To investigate the second question, we tested a series of    1.
predictions based on our model.           A set of planned        Materials
comparisons address this question by comparing the                     The stimuli were the same 45 objects as in Experiment 1,
percentages of response at each level. Given 3 examples from      except that the five Dalmatians were replaced by five slightly
the same subordinate level category, the model predicts a         different terriers.
sharp drop between subordinate level generalization and           Design and Procedure
basic-level generalization (83% vs. 13%, p < .0001). Given 3           The procedure was identical to that of Experiment 1,
examples from the same basic-level category, the model            except for the following. In the One-example Condition, each
predicts a sharp drop between basic-level generalization and      object was labeled three times.             For example, the
superordinate level generalization (47% vs. 15%, p < .0001).      experimenter may pick out a green pepper, show it to the
Given 3 examples from the same superordinate category, the        child, and say, “See? A fep.” She put the pepper down on the
model predicts that generalization should include all             floor, then picked it up again, and said, “Look, a fep.” She
exemplars from that superordinate category (86%, 53%, and         put down and picked up the pepper the third time and said,
43%). Children's performance is in broad agreement with the       “It’s a fep.” The experimenter made sure that the child was
predictions. With three examples, children generalized to the     following her actions so it was clear that the same pepper had
appropriate level consistent with the examples and their          been labeled three times.
generalizations were sharper than with one example.                    In the Three-example Condition, each object was labeled
Discussion                                                        exactly once. Again, the experimenter monitored the child’s
     Four-year-old children’s performance was in broad            attention to ensure that joint attention was established before
agreement with our predictions. On the 1-example trials, they     the labeling event for each object.
showed graded generalization. Interestingly, they did not              Although all 24 objects were laid out in front of the
show a strong basic-level bias. On the 3-example trials, the      child, the experimenter chose 10 of these objects as target
children modified their generalizations depending on the span     objects. The experimenter picked up each of the 10 objects
of the examples. Their generalizations were consistent with       and asked the child, “Is this a fep?” The target set included 2
the most specific category that included all the examples.        subordinate matches, 2 basic-level matches, 4 superordinate-
However, the children’s data were much noisier than those of      level matches, and 2 distracters.
the adults in T&X. Several methodological reasons may             Results
account for these differences. The overall level of response           Figure 2 shows the percentage of responses at the various
was much lower for children. Perhaps the task of freely           hierarchical levels.
choosing among 24 objects was too demanding for children                                 1
of this age and some of them may be reluctant to choose more
                                                                         Choice prob.
than a few objects.
                                                                                        0.5
     In the next experiment, we presented children with each
                                                                                                                                            sub
of 10 objects and ask for a yes/no response for each. This                                                                                  basic
modification ensured that all children provide us with               Examples:                 1       3 sub.    3 basic   3 super.         super
judgment on each of the test objects.
                                                                                          Figure 2: Generalization data from Experiment 2
     The critical prediction made by our Bayesian framework
was whether the learner’s generalization function differed
                                                              2384

      The same two questions are addressed as in Experiment          three independent, randomly sampled examples warrants a
1. First, did children behave differently in the 1-example trials    sharp increase in confidence for particular hypotheses. These
compared with the 3-example subordinate trials when they             results suggest that both adult and child learners are very
were given 1 vs. 3 virtually identical exemplars? Second, did        sensitive to the “suspicious coincidence” in the input.
the 3-example trials differ from each other depending on the               By varying the span of examples, we found that labels for
span of the examples?                                                subordinate and superordinate categories may not be as
      To investigate the first question, we compared the             difficult to learn as suggested by previous studies. When given
percentages of responses that matched the example(s) at the          multiple examples, preschool children are able to learn words
subordinate, basic-level, or superordinate level. On the one-        that refer to different levels of the taxonomic hierarchy, at least
example trials, participants chose more subordinate (96%)            in the domains of animal, vehicle, and vegetable. Special
than basic-level matches (40%), and more basic-level than            linguistics cues or negative examples are not necessary for
superordinate matches (17%) (p < .001 for both                       learning these words.
comparisons). In contrast, when presented with three very                  By varying the number of labeling events independent of
similar exemplars from the same subordinate category,                the number of examples, we were able to explore the
participants chose more subordinate matches (94%) than both          ontological underpinning of children’s word learning. We
basic-level (6%) and superordinate matches (0%) (p < .0001           found evidence that preschool children are keeping track of the
for both comparisons). Furthermore, there was a reliable             number of instances labeled and not simply the number of co-
difference between the basic-level matches (40% vs. 6%, p <           occurrences between object-percepts and labels. Word learning
.01).                                                                 appears to be fundamentally a statistical inference, but unlike
      To investigate the second question, we tested a series of       standard associative models, the statistics are computed over
predictions based on our model. With the modifications on             an ontology of objects and classes, rather than over surface
methodology, children's performance is very consistent with           perceptual features.
our predictions. Given 3 examples from the same subordinate                Any theory of word learning needs three components:
level category, the model predicts a sharp drop between               First, what is the body of prior knowledge the learner has
subordinate level and basic-level generalization (94% vs. 5%,         coming into the task of learning new words at any given point
p < .0001). Given 3 examples from the same basic-level                in time? Second, what are the data required by the learner and
category, the model predicts a sharp drop between basic-level         what are the data actually available to the learner in order to
and superordinate level generalization (75% vs. 8%, p <               succeed in acquiring word meanings? Third, what engine of
.0001). Given 3 examples from the same superordinate                  inference is employed by the learner? Furthermore, how these
category, the model predicts that generalization should               three components interact is crucial for the success of any
include all exemplars from that superordinate category (94%,          theory of word learning or inductive inference in general
88%, and 62%).                                                        (Tenenbaum & Griffiths, 2001).
Discussion                                                                 We make two main points in adopting a Bayesian
      With the modifications on the experimental design,              inference framework in studying word learning. First,
preschool children showed evidence of computing over the              previous theories of word learning (both from the hypothesis
number of examples labeled (not just the number of word-              elimination tradition and the associative learning tradition)
object pairings) and computing over the span of the examples.         have not endowed the learner with a powerful enough
These results replicated and extended those of Experiment 1,          inference engine. Second, some researchers may suggest that
providing stronger evidence for our model.                            what is most important is to characterize the first and the
                                                                      second components above, prior knowledge and input. We
                      General Discussion                              argue otherwise. If the inference engine were incorrectly
      In order to test specific predictions of the Bayesian           characterized, one would necessarily err in charactering either
framework, our experiments investigated the effects of number         prior knowledge or the data necessary for successful learning.
of examples (1 vs. 3), span of examples presented to our              If the inference engine were too weak, one would need to posit
participants (subordinate, basic, vs. superordinate levels), and      either a great deal of prior knowledge or a lot of input data. By
number of labeling events (one object labeled three times vs.         adopting a stronger inference engine than other approaches to
three objects labeled once each). Each of these experimental          word learning, we are able to place stronger constraints on
design features sheds new light onto the process of word              prior knowledge and also on the necessary input data.
learning.                                                                  The research presented here sheds light on both of these
      By varying the number of examples, we were able to              points. What is the right inference engine? Previous literature
examine the effects of multiple examples on generalization.           suggests two candidates: associative learning or hypothesis
We found that word learning displays the characteristics of a         elimination, neither of which can easily explain our findings
statistical inference, with both adult and child learners             here. One major issue with typical associative learning rules is
becoming more accurate and more confident in their                    that they are sensitive only to the statistical relations between
generalizations as the number of examples increased. This             features, regardless of the nature of those features (e.g., Regier,
effect was not the typical gradual learning curve that is             1996; Gasser & Smith, 1998). Given multiple features that are
typically associated with “statistical learning”; rather, there       all present in all examples of a new word to be learned,
was a strong shift in generalization behavior from one to three       standard associative models raise the weights of all these
examples, reflecting the statistical intuition that the span of       features equally; they do not recognize that some highly
                                                                 2385

correlated features are more diagnostic than others. A priori         of generalization are qualitatively and quantitatively
biases can be incorporated by adopting different initial values       consistent with the Bayesian model's behavior, but not with
for the weights of different features, but unless one introduces      standard models based on hypothesis elimination, or
an attentional mechanism, it is difficult to develop a posteriori     associative or correlational learning. Bayesian inference may
preferences that discriminate among two features which are            thus offer the most promising framework in which to explain
equally natural a priori and equally well-correlated with the         the speed and success of fast mapping in word learning.
observation of the word to be learned. In the context of                    The field of cognitive and language development has
learning words for nested categories, associative learning            often been polarized over debates about of whether nature or
approaches have difficulty explaining why generalization              nurture is key in development. The Bayesian framework we
sharpens up from 1 to 3 virtually identical examples since both       advocate here can perhaps take us beyond this classic “either-
sets of examples are consistent with multiple hypotheses              or” dichotomy, by showing how both prior knowledge
represented in terms of correlated features; it also has difficulty   (probabilistic versions of constraints) and observed data (the
choosing the right level of generalization for the same reason.       statistical structure of the examples) can be combined in a
      In contrast, hypothesis elimination approaches run into a       rational inference process.
different sort of problem. In order to explain the sharpening
from 1 to 3 examples, one would have to posit a basic-level            Acknowledgments. JBT was supported by the Paul E.
bias just for the 1-example case and some version of a bias            Newton Career Development Chair and FX was supported by
towards “the smallest category consistent with the examples”           a Canada Research Chair. We thank the children for their
just for the 3-example case. Presumably we do not want to              participation.
have to posit a specific selection principle for each particular
case.     In addition, positing a basic-level bias makes                                        References
subordinate and superordinate nouns difficult to learn. Since          Bloom, P. (2000) How children learn the meanings of words.
children do eventually learn these nouns, hypothesis                     Cambridge, MA: MIT Press.
elimination approaches have to posit further constraints to           Carey, S. & Bartlett, E. (1978) Acquiring a single new word.
override the basic-level bias.                                           Papers and reports on child language development, 15, 17-
     Although it may be possible to modify existing models to            29.
account for these results, the advantage of the Bayesian              Gasser, M. & Smith, L.B. (1998) Learning nouns and
inference framework is that it explains both the transition              adjectives: A connectionist approach. Language and
from 1 to 3 examples and the appropriate level of                        Cognitive Processes, 13, 269-306.
generalization without having to posit somewhat post hoc               Keil, F.C. (1979) Semantic and Conceptual Development: An
constraints. The graded generalization with 1 example                    ontological perspective.       Cambridge, MA: Harvard
follows straightforwardly from the mechanism of hypothesis               University Press.
averaging. The sharpening from 1 to 3 examples follows                 Markman, E.S. (1989) Categorization and naming in children.
straightforwardly from the size principle. For the problem of            Cambridge, MA: MIT Press.
learning words for kinds, at least, the Bayesian framework             Markson, L. & Bloom, P. (1997) Evidence against a
provides the most principled and parsimonious account.                   dedicated word learning system in children. Nature, 385,
     Choosing the right inference engine also has implications           813-815.
for the information needed for successful learning, both in           Quine, W.V.O. (1960) Word and object. Cambridge, MA:
terms of prior knowledge and input data. Associative                     MIT Press.
learning requires many examples and sometimes both                    Regier, T. (1996) The human semantic potential: Spatial
positive and negative examples. At least in certain domains              language and constrained connectionism. Cambridge,
of word learning (e.g., count nouns for kinds), a small number           MA: MIT Press.
of examples are sufficient for generalization in both children        Tenenbaum, J.B. & Griffiths, T. (2001) Generalization,
and adults, and vast majority of the words are learned through           similarity, and Bayesian inference. Behavioral and Brain
positive examples alone (Bloom, 2000).                 Similarly,        Sciences, 24, 629-641.
hypothesis elimination approaches have to posit specific prior         Tenenbaum, J.B. & Xu, F. (2000) Word learning as Bayesian
constraints (e.g., the basic-level bias) in order to explain fast        inference. In L. Gleitman and A. Joshi (eds.), Proceedings
mapping. These constraints often have to be overridden by                of the 22nd Annual Conference of the Cognitive Science
other constraints (e.g., mutual exclusivity so that each                 Society (pp. 517-522). Hillsdale, NJ: Erlbaum.
category can only have one basic-level label). The advantage           Tomasello, M. (2001) Perceiving intentions and learning
of the Bayesian framework is that it arrives at the right                words in the second year of life. In Bowerman & Levinson
generalization pattern from just a few positive examples, and            (eds.), Language acquisition and conceptual development.
it does not need special linguistic cues or constraints that have        Cambridge University Press.
to be overridden later on.                                             Waxman, S.R. & Booth, A.E. (2001) Principles that are
     In sum, our inductive models may be seen as                         invoked in the acquisition of words, but not facts.
probabilistic generalizations of the classic deductive approach          Cognition, 77, B33-B43.
to word learning based on hypothesis elimination. Our
experiments in the domain of words for object categories,
with both adults and children, showed that people's patterns
                                                                  2386

