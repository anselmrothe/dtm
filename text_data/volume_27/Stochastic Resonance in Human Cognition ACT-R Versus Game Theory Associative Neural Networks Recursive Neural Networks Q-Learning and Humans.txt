UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Stochastic Resonance in Human Cognition: ACT-R Versus Game Theory, Associative Neural
Networks, Recursive Neural Networks, Q-Learning, and Humans
Permalink
https://escholarship.org/uc/item/4qg2t6v7
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 27(27)
Authors
Chandrasekharan, Sanjay
Lebiere, Christian
Stewart, Terrence C.
et al.
Publication Date
2005-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

          Stochastic Resonance in Human Cognition: ACT-R Versus Game Theory,
       Associative Neural Networks, Recursive Neural Networks, Q-Learning, and
                                                              Humans
           Robert L. West (robert_west@carleton.ca)1, Terrence C. Stewart, (tcstewar@connect.carleton.ca)1,
         Christian Lebiere (clebiere@maad.com)2, Sanjay Chandrasekharan (schandr2@connect.carleton.ca)1
                     1
                       Institute of Cognitive Science, Carleton University, Ottawa, Ontario, Canada, K1S 5B6
                         2
                           Micro Analysis and Design, Inc., 6800 Thomas Blvd, Pittsburgh, PA 15208 USA
                              Abstract                                decreases the ability of the opponent to predict these moves.
                                                                      If we assume that increasing noise in a cognitive model will
  We examined the effect of cognitive noise on human game             always increase the level of randomness in its behaviour
  playing abilities. Human subjects played a guessing game            then there is a direct link between cognitive noise levels and
  against an ACT-R model set at different noise levels. Counter       the level of randomness in a game. However, if adding noise
  to the normal effect for noise (i.e., to increase randomness)       can, under certain conditions, reduce the level of
  increasing noise over certain ranges increased the win rate in
                                                                      randomness, then the relationship between cognitive noise
  both the ACT-R model and in the humans. We then attempted
  to model the human results using ACT-R, Q-Learning, neural          and randomness is not so straightforward.
  networks, and Simple Recursive Neural Networks. Overall,               We investigated this by looking at the relationship
  ACT-R produced the best match to the data. However, none            between cognitive noise and the ability to predict your
  of these models were able to reproduce a secondary counter          opponent in the game of Paper, Rock, Scissors (henceforth
  intuitive human win rate effect.                                    PRS). PRS was chosen for this study because the game
                                                                      theory solution is very simple; just play randomly, 1/3
Noise, or randomness, plays an important role in cognitive            paper, 1/3 rock, 1/3 scissors. The reason for this is that any
modelling. In problem solving it is often necessary to add            deviation from this strategy would leave the player open to
noise to a model to get it to explore possible solutions rather       exploitation from an opponent who could detect the
than freezing onto a single approach. In memory models,               deviation. The expected outcome for this strategy over time
noise often plays a role in modelling errors of omission and          is for both players to play at chance; 1/3 wins, 1/3 losses,
commission (e.g. Anderson & Lebiere, 1998). Noise is also             and 1/3 ties. If adding noise to the cognitive system of a
used to model the ability of humans to purposefully behave            player increases the randomness of their playing then adding
stochastically (e.g., Treisman & Faulkner, 1987). In these            noise should cause the rate of win, losses and ties to move
cases, the role of noise is to create and/or increase                 towards the chance rate. In contrast, an SR effect would
randomness in behaviour. However, adding noise to a                   cause one or both players to move away from the chance
component within a system can also have the opposite                  rate as more noise is added. Typically, such an effect would
effect. That is, adding noise can, under the right conditions,        occur over only a limited range of the noise parameter.
decrease randomness (i.e. the system’s behaviour moves                   Another reason that PRS is a good choice is that the
away from chance).                                                    cognitive processes underlying PRS play have been
   The best-known example of this is stochastic resonance             previously studied. Human PRS play has been successfully
(SR). SR refers to a class of models that produces the effect         modelled using neural networks (West & Lebiere, 2001) and
of reducing randomness by adding noise. Importantly, SR               ACT-R (Lebiere & West, 1999). In both cases the basic
has been implicated in neural functioning (see chapter 22 of          strategy was the same: to attempt to win through the
                                                                      detection of sequential dependencies. Specifically, each
Ward, 2002 for a review) and has also been shown to
                                                                      player tries to predict what their opponent will play next by
influence decision making in perceptually based tasks (see
                                                                      detecting sequential dependencies in past moves. Both the
chapter 21 of Ward, 2002, for a review). However, there is            neural network model and the ACT-R model, when
no agreed upon, precise definition of when a system should            compared to human data, indicated that people use their
be classed as SR. For experimental results it is often the            opponent's last two moves to predict the current move. We
case that a result is assumed to be SR if adding noise to a           refer to this as a lag 2 model. Simpler models, which use
system reduced the level of randomness of the system in               only the last move, were termed lag 1 models.
some way. This is the sense in which we use the term SR.                 The effect of cognitive noise on this strategy seems
However, the important point is not the technical definition          straightforward: as noise is added to the sequential
but whether or not noise can function in this way for the             dependency mechanism the player should become less able
cognitive system, as it is known to do for the neural and             to predict their opponent's moves. Also, as their moves are
perceptual systems.                                                   increasingly determined by the noise they should become
                                                                      increasingly hard to predict. Eventually the cognitive
    Games, Randomness, and Cognitive Noise                            system will become completely swamped with noise and all
In game theory, the ability to behave randomly or pseudo-             the moves will be random. That is, the win/loss/tie rates for
randomly often plays a central role. This is because                  both players will converge towards the chance rates. With
increasing the level of randomness in a player’s moves                sufficient noise this outcome is unavoidable. However, if an
                                                                 2353

SR effect exists then the relationship between the level of          squared regression to examine the human scores across
noise and the level of randomness will not be monotonic.             trials and the ACT-R score across trials. The slope of the
That is, for some regions of noise, increasing noise will            regression produced an estimate of the win and loss rates for
cause the win/loss/tie rates to move away from chance.               each noise level.
                                                                        The thick lines in Figure 1 and Figure 2 show the results
               Humans Versus ACT-R                                   for our first experiment. In Figure 1, we see that as the
Testing for stochastic resonance in humans is difficult              model noise increased from 0 to 0.5, the human win rate
because it is problematic to add precise amounts of noise to         decreased towards chance. At the same time, Figure 2
their mental processes. We approached this issue by having           shows that the model’s win rate (which is the same as the
human subjects play against an ACT-R model with different            human’s loss rate) increased dramatically. Importantly,
amounts of noise. Previous experiments (Lebiere & West,              between a noise level of 0.2 and 0.5, increasing the amount
1999; West & Lebiere, 2001) have shown that humans tend              of noise caused the win rate to significantly increase in a
to beat the ACT-R model when it is set to detect sequential          direction away from chance (p<0.001). This is a clear
dependencies at lag 1, while they tend to tie or lose if the         example of an SR effect within the model. The model was
model uses the last two moves (lag 2) to predict the next            able to predict the human players better as noise was added,
move. They also found that human players were much more              within this noise range.
motivated when they were winning, so we chose the lag 1                 The data also revealed an SR effect within the human
version for this experiment.                                         players. When the model’s noise was increased from 0.5 to
   The ACT-R model learns sequential dependencies by                 0.75, the humans significantly increased their win rate away
observing the relationship between what happened and what            from chance (p<0.001). The increased noise in the ACT-R
came before on each trial. After each round, a record of this        model caused increased noise in the information being
is stored in the ACT-R declarative memory system. Each               received by the human players, which in turn caused an
time the same sequence of events is observed it strengthens          increase in their ability to predict the model’s performance.
the activation of that sequence in memory. Thus, activation             To replicate the human SR effect, we repeated the
levels reflect the past likelihood of a sequence occurring.          experiment focusing in on the noise range that produced this
Noise is added (via the standard ACT-R activation noise              effect. As the thin line in Figure 1 illustrates, the effect for
parameter) when the model attempts to retrieve the                   the humans was even stronger in this experiment, possibly
sequence with the highest activation level in order to predict       because we came closer to capturing the peak of it. Again,
the opponent’s next move. For example, if the opponent’s             the movements away from chance were significant
last move was P and the model was set to use information             (p<0.001).
from just the previous move (lag 1), the model would                    Finally, to be sure of the effect, we re-ran the experiment
choose from PR, PS, and PP based on activation levels, then          using only experienced players (n = 8) who had been able to
use the retrieved sequence to predict the opponent’s next            win in the previous experiment, and focused in on two noise
move. Thus, if PS had the highest activation this would              levels that would maximize the effect. Again we found a
predict that the opponent will play S next, and so the model         significant increase in the human hit rate (p<0.001). In this
would play R (which beats S). The model would then see               case, probably due to the use of experienced players, every
what the opponent actually did and store a record of it (e.g.,       subject individually produced the effect. As far as we
assume the opponent played S, the model would then store             know, this is the first direct demonstration of SR effects at
PS), which would strengthen the activation of that sequence.         the cognitive level in human subjects.
   Previous research has shown that when two players use
the strategy of detecting sequential dependencies against
each other, the result is that both players produce a series of                    0.41
short-lived sequential dependencies (West & Lebiere,
2001). Adding noise to the ACT-R model increases the                               0.39
likelihood that a mistake will be made, in that it increases
the chance that the most active sequence may not be chosen.                        0.37
                                                                        win rate
Thus, if the model ‘knows’ the right answer, adding noise
increases the chance that it will fail to retrieve it. When                        0.35
noise causes a failure to retrieve the most active sequence, it
also introduces noise (i.e., false information) into the signal                    0.33
                                                                                               Exp.1
sent to the opponent. This causes the opponent to store false                                  Exp.2
information that is not predictive of the player’s sequential                      0.31
                                                                                               Exp.3
dependencies (which are embodied in the activation levels),
                                                                                   0.29
and so introduces noise into the opponent’s learning
process. Thus we tested for two sources of noise: internally                              0   0.2      0.4     0.6    0.8        1
                                                                                                       model noise
generated noise in the ACT-R model, and noise in the signal
provided to the human players.
   For all the experiments, each human subject played
against the ACT-R model at different noise levels. Each                 Figure 1: Human win rate (model loss rate) at different
game was 150 trials and the order was randomized. To test               levels of model noise for three different experiments.
the significance of the noise manipulation we used a least
                                                              2354

            0.39                                                             This is not expected to be a good match to the human
                                                                             results, but is included so as to have a baseline for
            0.37
                                                                             comparison. It has no parameters.
                                                                             The ACT-R Model
            0.35
loss rate
                                                                             This model is as previously described. It has a single
                                                                             parameter (the level of noise), and the general
            0.33                                                             recommendation is to set this value to 0.25. This gives us a
                                 Exp.1                                       prediction that the model should be optimal at or near that
            0.31                 Exp.2                                       setting.
                                 Exp.3                                          We also examined a number of variations on the basic
            0.29                                                             ACT-R model. The original ACT-R PRS model was
                   0       0.2     0.4     0.6      0.8       1              created in ACT-R 4. An important aspect of this model was
                                   model noise                               that it used the architecture in a very direct way to detect
                                                                             sequential dependencies. ACT-R 5 introduced a change in
            Figure 2: Human loss rate (model win rate) at different          the architecture such that implementing the version 4 model
            levels of model noise for three different experiments.           in ACT-R 5 could not be achieved in a simple and direct
                                                                             way. So we created a version 5 model that used the ACT-R
                       Modeling Human Performance                            5 architecture in the most direct way. The difference
We developed a number of fundamentally different models                      amounts to this: in version 4 only the chunk describing what
of the human performance versus ACT-R, and tested them                       actually happened is strengthened, while in version 5 the
at various parameter settings. In practice, this approach can                chunk describing what the model thought was going to
have a number of different outcomes. First, a model may                      happen is also strengthened. This makes sense, as both of
simply not match well with the human data, no matter what                    these chunks play an important role and are focused on.
changes are made to its parameter settings. This falsifies the                  In both ACT-R 4 and 5, there is the option to enable
model. Second, a model may match well over a wide range                      ‘partial matching’, allowing for memory retrieval errors.
of plausible parameters settings (or, indeed, over all                       We varied this and found that it caused either no significant
parameter settings). It is our experience that this happens                  effect or a deleterious effect. These results are not otherwise
surprisingly often (see Stewart, West, & Coplan, 2004 for                    reported.
an example). Third, a model may match the human data                            We also tried making use of ACT-R’s ‘optimized
well, but only over a particular narrow range of settings. If                learning’ system. This approximation of the learning
there is no way to explain or justify those parameter settings               system is used in ACT-R models to save computing time,
then there is a possibility that the fit is due to capitalizing on           but, similar to Sims and Gray (2004), we found that it
chance and it is difficult to draw any conclusions about the                 significantly altered the results. Because there is no
validity of the model. Fourth, a model may inherit                           theoretical story behind the optimized version we only
recommended settings for its parameters that have been                       report on the results from the full, non-optimized version.
found to work in a wide range of situations. In this case,
there is a prediction that the standard settings should work                 Associative Network
well in the new situation, and as the parameters are moved                   This model has been used previously to model PRS playing
away from that norm, the accuracy should decrease.                           (West & Lebiere, 2001). Here, a network is used whose
   We compared the human data shown in the previous                          weights form a payoff matrix for performing a given action
section to models falling into five major classes: Game                      given the previous moves by the opponent. The weights are
Theory, ACT-R, Associative Neural Net, Simple Recursive                      then modified based on whether or not this choice results in
Neural Net, and Q-Learning. To do the comparisons, we ran                    a win. The rewards and punishments were set equal to the
the various models against the same opponent that the                        game payoffs (i.e., +1 for winning and -1 for losing) so the
humans played: an ACT-R lag 1 model with varying                             only free parameters are the number of rounds of history to
degrees of noise. For each level of noise in the opponent,                   use (i.e. the 'lag' of the network, in the same sense as the
we ran 100 simulations of the two agents playing 150                         ACT-R model), and whether the system treats ties as neutral
rounds of PRS.                                                               (payoff = 0) or as losses (payoff = -1). West and Lebiere
   For each model class, we created a large number of sub-                   (2001), using different experimental manipulations, found
models by adjusting the internal parameters. A variety of                    that a lag 2 network that treated ties as losses most closely
settings for each parameter were chosen, and the models                      modeled the human data.
were run for each combination of settings. All models were
implemented in Python, and the source code is available at                   Simple Recursive Neural Network
<http://ccmlab.ca/prs>.                                                      An SRNN is a variant of the standard neural network that is
                                                                             specifically designed to predict the next element in a
The Game Theory Model                                                        sequence (Elman, 1990). It does this by adjusting its
This is the simplest model, and inspired by the pure game-                   connection weights via the back-propagation of error
theory solution to PRS. This model chooses its actions                       learning algorithm (Rumelhart et al, 1986), and by having a
randomly, without regard to the actions of the opponent.                     separate set of inputs that are set to the values of its own
                                                                      2355

internal hidden nodes. This allows the network to learn its           values were above 0.05. However, the best version of the
own representation of past events, and thus to find patterns          model came close to significance. It was a Lag-2, treating
that are not artificially limited to being of a certain length.       ties like losses, with learning rate of 0.5, exploration of 0.1,
This is in contrast to the models seen thus far which are set         and future discount rate of 0.95, which achieved a p-value
to be either lag 1 or lag 2.                                          of 0.052. Most settings were significantly worse.
   The important parameters for an SRNN are the learning
rate and number of hidden nodes. We also varied the                   Associative Network
number of times the network was trained on the previously             This model was also unable to match the human data at the
seen data. This allowed the network to adjust more quickly            0.05 significance level. Its best result was also with a Lag
to short-term patterns. Payoffs were set in the same way as           of 2, and treating ties as losses (the same result as found in
the associative network.                                              West & Lebiere, 2001).
Q-Learning                                                            Simple Recursive Neural Network
Here, we made use of the classic reinforcement-learning               For the SRNNs, we found one model that matched the
algorithm as defined in (Watkins, 1989). This is an action-           human data, according to our criteria. With 3 hidden nodes,
selection algorithm that makes decisions based on a current           a learning rate of 0.1, and repeating the training 100 times,
sensory state (in this case, the last 1 or 2 moves by the             the model achieved a p-value of 0.02. This gives us a 98%
opponent) and an experientially learned estimation of the             certainty that the model plays within 2.5% of the human
long-term rewards (as measured by wins and losses) for                performance. However, since there were 50 SRNN models
overall strategies. Importantly, it is capable of learning            investigated, the fact that one was found to match with 98%
strategies that involve short-term loss for long-term wins.           confidence would be expected, even if none of the models
   However, it has three parameters (the learning rate, the           matched. This means that we should be wary of accepting
future-discount rate, and the exploration rate), and these do         the SRNN as a model of human performance on this task.
not have suggested values. Furthermore, we had the option
of treating ties in the game as either losses or neutral (as in       ACT-R
the SRNN and associative networks), and we could set it to            Almost half of the ACT-R models investigated were found
be either lag 1 or lag 2. This resulted in a large number of          to be good matches (p < 0.05) to the human data. This is a
possible parameter settings.                                          remarkable result, indicating that we can model the human
                                                                      data accurately without parameter tweaking. However, the
Modeling Results                                                      best matches were in the range of 0.25-0.28. This compares
To establish that a given model matches with the known                favorably to the recommended noise setting of 0.25.
human data, we performed equivalence testing. This is
similar to a standard t-test, but the null hypothesis is that the                  Table 1: The Top 10 ACT-R Models
two means are different, rather than being equal. In the                            p-value     Noise Lag        Version
results that follow, the p-values indicate the probability that                     <0.01       0.28     2       5
the model data and the human data have means that differ                            <0.01       0.25     2       5
by more than 2.5%.                                                                  <0.05       0.28     2       4
   For each model, we measured the average win and loss                             <0.05       0.28     1       4
rates when that model played against the ACT-R lag 1                                <0.05       0.5      2       4
model with the 9 different levels of noise studied in the                           <0.05       0.3      2       5
human data. This gave us 18 p-values per model. We                                  <0.05       0.3      2       4
combined the p-values using Fisher’s rule, resulting in a                           <0.05       0.25     1       4
single p-value indicating the probability that the model and                        <0.05       0.5      1       5
the humans had different mean scores. This means that a p-                          <0.05       0.7      1       4
value of less than 0.05 indicates 95% certainty that the
model is within 2.5% of the human data.                               The SR Effects
   In total, we investigated 223 separate parameter settings.         All of these results were based on an overall fitting of the
                                                                      model data to the human data. As discussed earlier, in the
The Game Theory Model                                                 human versus ACT-R data there were two SR effects. As
Since it is well known that humans are generally bad at               noise was added, the first effect was a relatively large effect
performing randomly (e.g. Neuringer, 1986), it was                    benefiting the ACT-R player. The second effect was a
expected that this model would act as a benchmark for                 relatively small effect benefiting the human player.
comparison. As expected, the match was not significant                Disappointingly, none of the models we tested produced the
(p>0.05).                                                             second effect.
                                                                         To determine if this effect could be produced with highly
Q-Learning                                                            specific parameter settings, we ran a (1,5)-Evolutionary
No matter what combinations of parameter settings were                Strategy (a relative of a standard Genetic Algorithm, but
tried, we were unable to find a Q-Learning model that                 more suited for parameter optimization) on all the models to
matched the human data, according to our criteria. All p-             try to get this effect. However, this failed to get the effect in
                                                                 2356

any of the models, indicating that none of the models is             rise away from chance in the loss rate is significant
ultimately correct. All of the models became swamped by              (p<0.001), so the model did succeed in replicating the
the noise in the output of the lag 1 ACT-R model and went            effect, although it produced only a muted version of it. The
to chance rates around the point where the human SR effect           problem seems to be that the randomness effect was too
occurred.                                                            large and the SR effect was too small. Interestingly the best
                                                                     performing models for the Associative Networks and Q-
                        Discussion                                   Learning (not shown) also produced a muted version of the
In this study we tested four different classes of cognitive          first SR effect.
models. Qualitatively, all of them (with the right parameter                 0.41
settings) were able to match the human win rate for the first                                                                win
SR effect to some degree. That is, it was possible to produce                0.39                                            loss
a model that initially could beat the lag 1 ACT-R model, but                 0.37                                            tie
lost this ability as noise was added. One way to interpret this
is that the model is able to predict the opponent (the ACT-R                 0.35
lag 1) and so wins above chance; however, the opponent
cannot predict the model's moves and so wins at chance.               rate   0.33
Because one wins above chance and the other at chance, ties                  0.31
occur at a level below chance. As noise is added to the
opponent it becomes more difficult for the model to predict                  0.29
it and its win rate and the tie rate converge towards chance.                0.27
In this situation, everything moves towards chance as noise
is added. This is a standard randomness effect as opposed to                 0.25
an SR effect. However, this is not what happened for the                            0   0.2       0.4      0.6         0.8          1
ACT-R lag 1 model that the human subjects played against.                                        opponent noise
The human win rate went down, but the model’s win rate
(the human loss rate) increased away from chance,                       Figure 3: Performance of the best ACT-R model (lag 2,
producing an SR effect (see Figure 2).                               noise 0.28, version 5) playing against the same opponent the
   The mechanism for this first SR effect may reside in the                         human subjects played against.
fact that the interaction between two players using the
sequential dependencies strategy causes the players to                  Overall, when the models were examined across the entire
generate short lived sequential dependencies (West &                 data set, only ACT-R and SRNN were able to outperform
Lebiere, 2001). If the opponent can detect one of these, it          the random game theory model in terms of matching the
can be exploited, but after it disappears the opponent needs         human data. However, SRNN could do this only for a very
to let it go and find the next one. Thus unlearning is as            limited set of parameter settings that did not have a
important as learning. Under these conditions, during the            theoretical justification, and also failed to reproduce the first
unlearning stage it can actually be an advantage not to select       SR effect. Thus it is questionable whether the SRNN model
the most active chunk, because it now represents a wrong             should be regarded as better than the Associative Network
prediction. Of course it is also a disadvantage not to select        and Q-Learning models. In contrast, ACT-R was able to
the most active chunk once a sequential dependency has               outperform the random model over a wide range of
been learned and is still valid. However, if learning is more        parameter settings, worked best for parameter settings at or
transitory than unlearning, the overall effect would be to           near the value found to work in most ACT-R models, and
increase the win rate.                                               produced the first SR effect.
   Another possibility is that the dynamic interaction that             The fact that the ACT-R model performed well in this
creates the sequential dependencies is affected. It is               study, and also in other studies using different games
important to realize that the sequential dependencies are not        (Lebiere, Wallach, & West 2000; Lebiere, Gray, Salvucci,
generated by the individual players but through the                  & West, 2003), indicates that it is accurately capturing a
interaction between them. Thus it is possible for changes in         significant portion of the cognitive functions involved in
the behaviour of one player to affect the sequential                 human game playing. However, the ACT-R model we used
dependencies outputted by the other player. In most cases            was falsified, along with all the other models, because it
adding noise would increase the chance of choosing the next          could not produce the second SR effect. This is part of the
most active chunk. It is theoretically possible that this could      normal process of developing and refining models. Some
affect the interaction such that the opponent would output           (e.g., Roberts & Pashler, 2000) have suggested that
stronger sequential dependencies and thus be easier to               cognitive models are not falsifiable and therefore not
predict. The dynamic interaction between the players forms           scientific. Our results show that this is not the case, as the
a complex coupled system that is not easily unpacked.                current version of the ACT-R model has clearly been
   With this in mind, Figure 3 shows the results for the best        falsified. However, we hope that with further study we will
performing ACT-R model (version 5, lag 2, noise = .28). It           be able to develop an ACT-R model that will produce both
models the human win rate well, but the loss rate is much            SR effects and thus shed more light on the phenomenon.
flatter than the human loss rate. However, the small initial            The second SR effect requires an explanation. It was
                                                                     interesting that the second effect occurred as the first effect
                                                              2357

was ending. We speculate that this marked a transition from                                  References
the first SR effect to a normal randomness effect in the
                                                                       Anderson, J. R. & Lebiere, C. (1998). The Atomic
ACT-R model. One possibility is that at this point the
                                                                         Components of Thought. Mahwah, NJ: Erlbaum.
increasing randomness component in the signal from the
                                                                       J.L. Elman, (1990). Finding structure in time, Cognitive
ACT-R model caused the human subjects to be less ‘locked
                                                                         Science, 14, 179-211.
in’ when they detected a sequential dependency. We have
                                                                       Kitajo, K., Nozaki, D., Ward, L., & Yamamoto, Y. (2003).
some simulation results indicating that this can produce an
                                                                         Behavioral Stochastic Resonance within the Human
advantage. Another possibility, as with the first SR effect, is
                                                                         Brain. Physical Review Letters, 90:21.
that the dynamic interaction that created the sequential
                                                                       Lebiere, C., Gray, R., Salvucci, D., & West, R. L. (2003).
dependencies was affected, causing the ACT-R model to
                                                                         Choice and Learning under Uncertainty: A Case Study in
output stronger sequential dependencies. Further analysis
                                                                         Baseball Batting. Proceedings of the 25th Annual Meeting
(beyond the scope of this paper) is required to understand
                                                                         of the Cognitive Science Society, 704-709.
the mechanisms for both the first and second SR effects.
                                                                       Lebiere, C., Wallach, D., & West, R. L. (2000). A memory-
   Another interesting question is the extent to which SR
                                                                         based account of the prisoner's dilemma and other 2x2
effects can occur in ACT-R models. The ACT-R lag 1
                                                                         games. Proceedings of International Conference on
model produced a large SR effect when interacting with the
                                                                         Cognitive Modeling, 185-193. NL: Universal Press.
human players, but only a muted effect when playing
                                                                       Lebiere, C., & West, R. L. (1999). Using ACT-R to model
against the ACT-R lag 2. This raises the question of whether
                                                                         the dynamic properties of simple games. Proceedings of
a large effect is possible using ACT-R to model both
                                                                         the Meeting of the Cognitive Science Society. Hillsdale,
players. Figure 4 shows a lag 2 ACT-R model set at
                                                                         NJ: Earlbaum.
different noise levels playing against a lag 1 ACT-R model
                                                                       Neuinger, A. (1986). Can people behave “randomly”? The
fixed at a somewhat low noise level (noise = 0.10). This
                                                                         role of feedback. Journal of Experimental Psychology:
result demonstrates that large SR effects are possible using
                                                                         General, 115, 62-75.
just ACT-R players (this is also demonstrated in Lebiere
                                                                       Roberts, S., & Pashler, H. (2000). How persuasive is a good
and West, 1999).
                                                                         fit? A comment on theory testing. Psychological Review,
           0.6                                                           107, 358-367.
                                                                       Rumelhart, D. E., Hinton, G. E., & Williams, R. J. (1986)
           0.5                                                           Learning representations by back-propagating errors.
                                                                         Nature, 323, 533-536.
           0.4
                                                                       Sims, C. & Gray, W. D. (2004). Episodic versus semantic
                                                                         memory: An exploration of models of memory decay in
    rate   0.3
                                                                         the serial attention paradigm. Proceedings of the 6th
                                                     win
                                                                         international conference on cognitive modeling. 279-284.
           0.2
                                                                         Mahwaj, NJ: Earlbaum.
                                                     loss
           0.1                                                         Stewart, T.C., West, R., and Coplan, R. (2004). A Dynamic,
                                                     tie                 Multi-Agent Model of Peer Group Formation.
            0                                                            Proceedings of the 6th international conference on
                 0   0.2      0.4     0.6      0.8          1            cognitive modeling 290-295. Mahwaj, NJ: Earlbaum.
                                                                       Treisman, M. & Faulkner, A. (1987). Generation of random
                              model noise
                                                                         sequences by human subjects: Cognitive operations or
                                                                         psychophysical process?        Journal of Experimental
  Figure 4: Performance of an ACT-R lag 2 model against
                                                                         Psychology: General, 116, 337-355.
the same opponent the humans played against. Unlike the
                                                                       Ward, L. (2002). Dynamical Cognitive Science. MIT Press.
other graphs, here we vary the amount of noise in the lag 2
                                                                       Watkins, C. J. C. H. (1989). Learning from Delayed
                model, not in the opponent.
                                                                         Rewards. Doctoral dissertation, Cambridge University,
                                                                         Cambridge, England.
                           Conclusions                                 West, R. L., & Lebiere, C. (2001). Simple games as
As far as we can ascertain, this is the first conclusive                 dynamic, coupled systems: Randomness and other
demonstration that adding noise can produce an SR-like                   emergent properties. Cognitive Systems Research, 1:4,
effect in the human cognitive system. Although we could                  221-239.
not model both SR effects found in the human data, three
out of the five cognitive models we tested did produce the
first SR effect. This indicates that SR effects are a property
of current models of cognition. This means that adding
noise to a cognitive system should not automatically be
assumed to increase the randomness of that system’s
behaviour. This is particularly true for systems involved in
dynamic interactions with competition and feedback.
                                                                2358

