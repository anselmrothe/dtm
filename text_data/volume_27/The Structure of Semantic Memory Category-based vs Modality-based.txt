UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
The Structure of Semantic Memory: Category-based vs. Modality-based
Permalink
https://escholarship.org/uc/item/0380q8f5
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 27(27)
Author
Gottlieb, Jeremy F.
Publication Date
2005-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

            The Structure of Semantic Memory: Category-based vs. Modality-based
                                          Jeremy F. Gottlieb (jgottlieb@carthage.edu)
                                               Carthage College, 2001 Alford Park Drive
                                                        Kenosha, WI 53140 USA
                              Abstract                                 that are related to each other have strong links with each
                                                                       other, regardless of the subsystem they are stored in. Thus,
   A source of significant debate in psychology is the issue of        while the associations between pieces of information are
   how information is stored in semantic memory. The two               still category-based, the physical layout of information in
   primary frameworks are the Unitary Content Hypothesis,
                                                                       semantic memory is based on modality. Furthermore, a
   which holds that information is stored based on categories,
   and the Multiple Semantics Hypothesis, which holds that
                                                                       given input modality can only directly access semantic
   information is stored based on sensory modality. In a series of     memory through its corresponding semantic subsystem, and
   three experiments, I attempt to shed some light on which of         only accesses the other subsystems indirectly through the
   these two frameworks is the most probable explanation of a          links between them. Damage to the semantic system should
   number of phenomena associated with semantic memory. The            result in selective impairments in processing semantic
   results indicate that the Multiple Semantics Hypothesis is the      information associated with particular input modalities,
   most likely explanation.                                            instead of particular categories.
   Keywords: semantic memory                                              There are a number of reports of patients demonstrating
                                                                       modality-specific semantic deficits, such as optic aphasia
                           Introduction                                (Beauvois, 1982; Riddoch & Humphreys, 1987). In this
   A significant debate in cognitive science centers around            deficit, patients are unable to name objects that are
the question of the underlying organizational structure of the         presented visually, but are not impaired at naming them
information stored in semantic memory. Specifically, the               from any other sensory domain, such as touch. They are also
question that has been argued is whether information is                unable to point to the proper object when given its spoken
organized categorically (e.g., Caramazza, Hillis, Rapp &               name. However, these patients still exhibit semantic
Romani, 1990; Caramazza & Shelton, 1998) or based on the               knowledge of the objects they were unable to name, such as
sensory modality of the information (e.g., Shallice, 1987;             being able to correctly mime the use of the objects and
McCarthy & Warrington, 1988; Beauvois, 1982).                          being able to draw objects and complex scenes upon a
   The prototypical theory that posits a category-based                verbal request. Beauvois (1982) also reports similar cases of
organization is the Unitary Content Hypothesis (UCH) of                tactile and auditory aphasia. According to proponents of the
Caramazza et al. (1990). The UCH posits that semantic                  MSH, the most parsimonious explanation of optic aphasia is
memory is an abstract, amodal storage system where                     to theorize that the connection between visual semantics and
information is organized categorically. Every input modality           verbal semantics has been severed.
can access all of the information about a given category in               Much of the rest of the evidence brought to bear in this
semantic memory, and semantic memory provides                          particular debate is also relatively ambiguous. One
information to every output modality. Thus, damage to                  frequently reported finding is that words are named faster
semantic memory should affect semantic processing in a                 than pictures, but pictures are categorized faster than words
category-based fashion.                                                (e.g., Potter & Faulconer, 1975; Guenther, Klatzky, &
   There are several reports of patients who exhibit category-         Putnam, 1980; Seifert, 1997; Theios & Amrhein, 1989).
specific deficits (see Forde & Humphreys, 1999, and                    However, word “naming” is really just word reading, which
Caramazza & Shelton, 1998, for review). Patients suffering             arguably involves simply mapping graphemes to phonemes
from these deficits are generally impaired at processing               and can bypass semantic memory entirely, as in many dual-
semantic information about living things vis a vis man-made            route models of word reading (Coltheart, Curtis, Atkins, &
objects (e.g., Warrington & Shallice, 1984; Gainotti and               Haller, 1993).
Silveri, 1996) or vice versa (e.g., Warrington & McCarthy                 PET scanning has showed that areas of the brain related to
1983, 1987).                                                           visual processing are more active when living things are
   The representative theory of a modality-based semantic              being named, while those related to action are more active
organization is Shallice’s (1987) Multiple Semantics                   when tools are being named (Martin, Wiggs, Ungerleider, &
Hypothesis (MSH). The MSH posits that semantic memory                  Haxby, 1996), indicating that information in semantic
is actually divided into different semantic subsystems based           memory is organized by sensory modality. However, there
on the various input modalities. There is a visual-semantic            is no true indication that this is the result of a modality-
system, a tactile-semantic system, a verbal-semantic system,           based organization instead of a category-based one. It could
etc. These systems are linked together, allowing for                   simply be the case that living things are processed in one
information to pass between them. Categories are a result of           part of the brain and man-made objects in another.
how these systems are linked together. Pieces of information
                                                                   797

                                   Experiments                                                Thus, we can postulate that the reaction times1 for
The experiments described below were designed to better                                       participants across the three stimulus conditions should take
examine how information is organized in semantic memory.                                      the form of:
All three experiments shared a common methodology.                                               Picture-picture: (2*ERp) + (2*CR) + C + R
Participants were simultaneously presented with two                                              Picture-word: ERp + ERw + (2*CR) + C + R
stimuli. The stimuli were either both pictures, both words,                                      Word-word: (2*ERw) + (2*CR) + C + R
or there was one of each type of stimulus. The two things                                        In other words, participants’ reaction times across the
that were varied across the experiments were the exact                                        three conditions should be essentially linear with a slope of
nature of the stimuli (what they depicted or described) and                                   |ERp – ERw| if the UCH is correct.
the question that participants were supposed to answer upon                                                                     VISUAL SEMANTICS
being presented with the stimuli.                                                                                                                            Prototypical
                                                                                                                                                             4-legged animal
Experiment 1 – Natural categorization                                                                                              Category
                                                                                                                                                  Prototypical
                                                                                                                                                  dog
                                                                                                                                                                        Prototypical
                                                                                                                                                                        cat
                                                                                                                                   retrieval
The primary purpose of Experiment 1 was to give a better                                                          Encoding
                                                                                                          Picture
overview of the time-course for making judgments about                                                                     Recognition
category membership for both words and pictures.                                                                                                           “animals”
                                                                                                                              Category            “dog”               “cat”
                                                                                                                              retrieval                                                   Conversion?
                                                                                                                  Encoding           “beagle”         “dachshund”       “siamese”
                                                                                                           Words
                                                                                                                           Recognition       “collie”             “manx”       “jaguar”
                     Encoding
            Pictures                                                                                                               VERBAL SEMANTICS
                              Recognition                                                                                                                           Response            Comparison
                                           COLLIE           Category
                                                            retrieval
                                           “collie”                           visual
                                                                              prototype
                                                                                                             Figure 2: MSH model of Experiment 1
                                                                      “dog”
                                                                            DOG
                                                          Category
                                                                        “woof” smell
                                                                                prototype
                                                                                                 Under the MSH the category retrieval stage becomes
                                                                                              more complicated. In particular, the issue arises of whether
                                             BEAGLE
                                                          retrieval
             Words
                     Encoding
                                             “beagle”
                                                                                              or not the category information being retrieved is the same
                               Recognition
                                                                                              for both pictures and words. If it is not, then some sort of
                                                      Response              Comparison        conversion step becomes necessary in the picture-word
                                                                                              condition (see Figure 2) to allow for a comparison of the
                Figure 1: UCH model of Experiment 1                                           category retrieved by the picture to the category retrieved by
                                                                                              the word. If this is the case, then we would postulate that the
Predictions The task in Experiment 1 requires four basic                                      reaction times across the three stimulus conditions would
processing stages:                                                                            take the form of:
   1. Perceptual encoding and recognition – converting the                                       Picture-picture: (2*ERp) + (2*CR) + C + R
        input stimulus into a usable representation and                                          Picture-word: ERp + ERw + (2*CR) + Conversion + C + R
        finding the semantic representation of the input.                                        Word-word: (2*ERw) + (2*CR) + C + R
   2. Category retrieval – retrieving the superordinate                                          Thus, in this case, we would expect to see a non-linear
        category of the input                                                                 pattern of reaction times, with the picture-word condition
   3. Comparison – comparing the two retrieved categories                                     showing slower reaction times than it would if the
        to each other.                                                                        distribution were linear.2
   4. Response preparation and generation
   There is no difference between the models for steps 1, 3,                                  Stimuli The picture stimuli were 138 colored pictures of
and 4. Step 1 happens independently for the two stimuli, and                                  living things that fell into nine different categories – dogs,
steps 3 and 4 occur independently of how semantic memory                                      cats, horses, fish, trees, birds, fruits, vegetables, and flowers.
is structured. Thus, the crux of the differences between the                                  The word stimuli were the names of the objects used as the
two models occurs at the point of category retrieval. To see                                  pictures. Stimuli were presented side by side on a Macintosh
how, we will examine what each model has to say about the
cognitive processing involved in this task.                                                   1
                                                                                                ERp is encouding and recognition for pictures; ERw is the same
   If the UCH is correct, then both the picture and word                                      for words; CR is category retrieval; C is comparison; R is response
representations of objects should access the same semantic                                    preparation and generation.
                                                                                              2
representation of the concept, as in Figure 1. Thus, both                                       The predictions presented here are predicate on the idea that the
pictures and words ought to be equivalent with regards to                                     processing stages are linear, as per the traditional models of
retrieving the superordinate category of that representation.                                 memory. However, even in a system where one or more of these
                                                                                              stages are performed in parallel, the MSH would still presume
                                                                                              separate semantic representations for visual and verbal
                                                                                              information, and thus there would still be a cost associated with
                                                                                              having to transfer information from one semantic subsystem to the
                                                                                              other.
                                                                                          798

computer using the PsyScope experiment software (Cohen,                  condition, this test was highly significant, F(1) = 30.78, p <
MacWhinney, Flatt & Provost, 1993). The pictures were                    .001.
normalized to all be roughly 1.5 inches square and the
words were presented so that they were, on average, roughly              Discussion The results from Experiment 1 seem to provide
as wide as the pictures. There was, on average, a one inch               contradictory evidence, based on the predictions made
horizontal separation between the two stimuli.                           above. The results from the no-match condition would seem
                                                                         to confirm the UCH, while the results from the match
Procedures As noted above, on any given trial, participants              condition would seem to confirm the MSH.
were presented with two stimuli simultaneously – two                        One likely explanation of this discrepancy involves a
pictures, two words, or one of each. Within a block of trials,           possibility that I failed to consider when generating my
no picture or word was repeated, and only one member of                  predictions. When a participant is presented with two
the picture/name pair would be presented. Thus, if a                     pictures from the same category – such as a beagle and a
participant saw the word “beagle” at some point in a given               collie – the first picture will be processed, which will
block of trials, they would not see the picture of the beagle            activate the DOG category node in visual semantic memory.
within that same block. Each participant was given three                 When the second picture is processed, it will need to access
blocks of 69 trials each.                                                the same category node in order for the task to be performed
   For individual trials, there was an equal probability for             properly. In this case, one could argue that a priming effect
which type of stimulus pair participants would see. In the               would cause the retrieval of the category for the second
picture-word trials, there was an even chance that the                   picture to be faster than it was for the first picture. Similarly,
stimuli would appear with the picture on the left and the                we would anticipate seeing a priming effect when both
word on the right or vice versa.                                         stimuli are words belonging to the same category.
   Participants were asked to judge whether the two stimuli                 However, when the stimuli are of different modalities (a
“depict or describe” objects that belong to the same                     word and a picture), they would access different category
category. They were not told what the possible categories                nodes – one in visual semantics and one in verbal semantics.
were, nor were they told any criteria to use to categorize the           Thus, there would be no priming effect in the picture-word
objects. The stimuli were left on the screen until the                   condition, and so it should be slower than it would be if the
participant responded.                                                   distribution across the three conditions were linear.
                                                                         Likewise, for the no-match condition, none of the stimuli
Results Trials that participants answered incorrectly or that            benefit from any sort of priming effect, and thus we would
took longer than 3000ms were excluded from this analysis.                expect those reaction times to be linear.
These accounted for 11.6% of the total trials (9.3% and                     Put another way, my predictions presumed that the match
2.3% respectively).                                                      and no-match conditions would operate in roughly the same
                                                                         fashion, and thus be subject to essentially the same rules.
                1500                                                     However, this does not appear to be the case. The no-match
                1400                                                     conditions would be governed by the equations given above
                1300
                                                                         for the UCH. The match conditions would be governed by
                                                                         these same equations, except that the picture-picture and
                1200
                                                                         word-word conditions should subtract a priming effect. To
                1100
                                                          MATCH
                                                                         wit:
                1000                                                        Picture-picture: (2*ERp) + (2*CR - P) + C + R
                 900
                                                            no-match        Picture-word: ERp + ERw + (2*CR) + C + R
                 800                                        match
                                                                            Word-word: (2*ERw) + (2*CR - P) + C + R
                    N = 646
                            p-p
                               1029 574
                                        p-w
                                            766 492
                                                   w-w
                                                      726
                                                                            Thus, it would appear that what Experiment 1 has found
                                                                         is not a conversion parameter, but a priming effect.
                      COND
                                                                            The UCH has a difficult time explaining this discrepancy
                                                                         between the match and no-match conditions. Since the
   Figure 3: Mean reaction time by condition by response for
                                                                         underlying feature of the UCH framework is that inputs of
                                 Experiment 1
                                                                         any modality activate the same category information in
                                                                         semantic memory, then presumably any semantic priming
   As can be seen from Figure 3, there was a deviation from
                                                                         effect observed in the same-modality trials in the match
linearity by the picture-word condition when the two stimuli
                                                                         condition should also be present in the cross-modal trials,
belonged to the same category (the match condition), but
                                                                         since the picture and the word are accessing the same
not when they belonged to different categories (the no-
                                                                         category node in semantic memory. As a result, the pattern
match condition). To confirm the statistical significance of
                                                                         of reaction times should still be linear across the three types
this deviation, a contrast-coded regression analysis was
                                                                         of stimulus presentation in the match condition. Clearly, this
performed. In the no-match condition, the analysis was not
                                                                         is not what is happening.
significant, F(1) = 1.05, p > .05. However, for the match
                                                                     799

   There is, however, one possible explanation for these            The results from the 1-yes and 2-yes conditions were
results that would allow the UCH to explain this deviation          collapsed as there was no statistical significance between
from linearity in the match condition. Specifically, there is       the groups.
the possibility that the deviation is due to a task-switching
effect that results from having to mentally “switch gears,”
so to speak, from processing pictures to processing words
(or vice versa). This effect would only be present in the
picture-word case, causing the non-linearity I found.
Experiment 2 – Object/lexical decision task
The purpose of Experiment 2 is to determine whether the
effect found in Experiment 1 was due to a simple task-
switching effect.
Predictions This experiment uses a lexical/object decision                  Figure 4: Reaction time by condition by response
task, where participants are asked to judge whether the
pictures and letter strings they see are real or not. This task        As can be seen from Figure 4, there is no apparent
eliminates the category retrieval and comparison stages             deviation from linearity in either yes or no response
from the task in Experiment 1. In doing so, I am hoping to          conditions. A contrast-coded regression confirms the
isolate any task switching effect in the perceptual encoding        probable linearity of the results in both the yes condition
and recognition stage (since it clearly cannot be in the            (F(1) = 0.57, p > .10) and the no condition ( F(1) = 3.06, p >
response stage). Thus, if we see a deviation from linearity in      .05).
the pattern of reaction times similar to the one we saw in the
match condition of Experiment 1, arguably that effect is due        Discussion The results from Experiment 2 demonstrate that
to a task switching effect, and not to a semantic effect.           the non-linearity observed in the match condition of
                                                                    Experiment 1 is not, in fact, due to a task-switching effect in
Stimuli The stimuli for this experiment were the same               the perceptual encoding and recognition stage. This was
pictures and words used in Experiment 1 (the real stimuli),         expected since a task-switching effect presumably should
as well as modified versions that were divided into two             have also been present in the non-match condition of
groups:                                                             Experiment 1, but the results from Experiment 2 provide a
   1. Plausibly unreal stimuli – pictures of objects with           more concrete demonstration of the absence of a task
        analogous parts interchanged (e.g. a beagle with a          switching effect in this experimental design.
        collie’s head); pronouncable non-words (e.g.                   As a result, it is reasonable to conclude that a MSH-based
        “mave”).                                                    model of Experiment 1 in conjunction with a semantic
   2. Unreal stimuli – pictures of objects with non-                priming effect in the like-modal, match condition trials is
        analogous parts interchanged (e.g., the bottom of a         the most likely explanation for the data gathered in
        beagle with the top of a tree); random,                     Experiment 1.
        unpronounceable letter strings.
                                                                    Experiment 3 – Functional categorization
Procedures The procedures were exactly the same as those            Experiment 3 was designed to provide a direct test of one of
in Experiment 1 except for how participants were directed to        the key components of the MSH model. Certain
respond. Half of the participants were instructed to decide         neurological patients have category-specific deficits, where
whether both items they saw were real or not (2-yes) while          they show severe impairments when trying to semantically
the other half were instructed to decide whether at least one       process living things vis a vis man-made objects, or vice
item was real (1-yes). This was to control for the fact that in     versa. This syndrome is difficult to explain under the MSH
the 2-yes condition, participants could also just respond           framework – how can there be category-specific deficits
“no” as soon as they saw an unreal item, meaning that rather        resulting from a modality-based organizational structure?
than always having to look at both stimuli and make a                  The solution was the sensory/functional hypothesis
decision, they could just be making their decisions as soon         (SFH). The SFH is a theory about what exactly separates
as they saw a stimulus that determined the answer (an unreal        living things from man-made objects within semantic
one). Thus, the 1-yes condition was used to balance this by         memory. The main idea is that there are significant
having a set of trials where the deciding stimulus was the          differences in the composition of the representations for the
real one.                                                           different classes of objects. We interact with natural objects
                                                                    primarily in a visual fashion – we observe trees and flowers
Results As before, incorrect trials and those trials over           and animals and what-not and classify them based on what
3000ms were excluded from the analysis. This accounted              they look like. Thus, the bulk of the features used to
for 11.6% of the total trials (8.8% and 2.8% respectively).
                                                                800

represent these objects in semantic memory are going to be             Procedures The procedures used were exactly the same as
visual in nature, and thus stored in visual semantic memory.           those used in Experiment 1, except that participants were
   Man-made objects, on the other hand, are interacted with            asked to judge whether or not the two stimuli “depict or
in a primarily functional fashion. We use man-made objects,            describe” objects that belong to the same functional
we build them to serve particular functions, and we classify           category.
them based on what they are designed to be used for.
Therefore, the representations of man-made objects are                 Results Once again, trials with errors or that took more
going to contain proportionally more functional features as            than 3000ms were excluded. This accounted for 10.4% of
opposed to visual features. Arguably, these functional                 total trials (8.8% and 1.6% respectively).
features would be stored in a semantic subsystem other than               As can be seen from Figure 5, the reaction times seem to
visual semantics.                                                      be relatively linear in both the match and no-match
   Category-specific deficits, in this hypothesis, are simply          conditions. Coded-contrast regressions show that, in fact,
the result of damage to either the visual semantic system or           the no-match condition does not deviate from linearity, F(1)
to whatever system contains functional information.                    = 2.64, p > .10. However, the match condition does show a
Experiment 3 was thus designed to test whether or not                  slight deviation from linearity, F(1) = 4.71, p < .05.
functional semantic information is stored differentially from
at least visual semantic information, and possibly from                                  1500
verbal semantic information.                                                             1400
                                                                                         1300
Predictions Experiment 3 was designed to be as analogous                                 1200
as possible to Experiment 1, with the exception that the                                 1100
category retrieval stage in Experiment 1 needs to be                                                                               MATCH
                                                                                         1000
replaced with two stages – a function retrieval stage and a                                                                          no-match
functional category retrieval stage.                                                      900
   Under both the UCH and the MSH we would expect a                                       800
                                                                                             N = 321     344 340     334 300   357
                                                                                                                                     match
                                                                                                     p-p         p-w        w-w
linear distribution of reaction times across the three
                                                                                               COND
modality conditions. Under the UCH, both pictures and
words would be accessing the same semantic representation
of each object, including functional information3. Not only                     Figure 5: Reaction time by condition by response
should the distribution be linear, but the reaction times
across the three modality conditions should be                            Table 1 shows the reaction times from this experiment
commensurate with the corresponding reaction times from                and Experiment 1 side-by-side. Of particular interest is the
Experiment 1. If anything, they should be a little bit slower          fact that the distribution of reaction times in Experiment 3 is
due to the added step of needing to retrieve the functional            much flatter than the distribution in Experiment 1. This
category after retrieving the function.                                difference is entirely the result of the picture-picture
   In the case of the MSH, however, the argument is that               conditions being slower in Experiment 3 while the word-
functional information is stored in a separate semantic                word conditions are faster. The picture-word conditions are
subsystem from visual information. Thus, how the reaction              virtually identical in Experiments 1 and 3.
times change will depend on precisely where this functional
information is stored. If it is stored in close association with             Table 1: RT across conditions in Experiments 1 and 3
verbal semantics, then we would expect the picture-picture
                                                                                                                 Experiment 1           Experiment 3
condition to be significantly slower than the same condition
                                                                                    Picture-picture              874.09                 1071.04
in Experiment 1 and the word-word condition to be
                                                                         match      Picture-word                 1187.26                1161.91
significantly faster. On the other hand, if functional
                                                                                    Word-word                    1290.69                1186.97
information is stored in a completely separate subsystem,
                                                                                    Picture-picture              1090.12                1192.53
we would expect reaction times to be significantly slower
                                                                         no-        Picture-word                 1265.32                1192.93
across all three conditions, to reflect the extra time needed
                                                                         match      Word-word                    1403.02                1275.72
to access the functional semantic system.
Stimuli The picture stimuli were 63 colored pictures of                Discussion The results from Experiment 3, when compared
man-made objects. The words used were the names of the                 with those from Experiment 1, match the predictions based
objects used as the pictures.                                          on functional information being stored either in the verbal
                                                                       system, or in a system more closely tied to the verbal than to
                                                                       the visual semantic system (see Figure 6). In this scenario, it
3
                                                                       is clearly the case that pictures would have slower access to
  Please note that when I refer to “functional information” what I     functional information than to categorical information as
am referring to is the semantic representations of functional
                                                                       they would need to take an extra step to retrieve that
information that we have stored, not to usage information that may
be directly derived from visual input.                                 functional information from the verbal/abstract subsystem.
                                                                   801

                                                                                Cohen, J. D., MacWhinney, B., Flatt, M. & Provost, J.
                                                                                  (1993).PsyScope: An interactive graphic system for
                                    VISUAL
                           Encoding
                                    SEMANTIC MEMORY                               designing and controlling experiments in the psychology
                  Pictures                                                        laboratory using Macintosh computers. Behavior
                                    Recognition
                                                                                  Research Methods, Instruments, & Computers, 25(2),
                                    VERBAL/ABSTRACT                               257-271.
                   Words   Encoding
                                    SEMANTIC MEMORY
                                                   “stool”         “chair”
                                                                                Coltheart, M., Curtis, B., Atkins, P., & Haller, M. (1993).
                                     Recognition
                                                                                  Models of reading aloud: Dual-route and parallel-
                                                sitting            sitting
                                                                                  distributed-processing approaches. Psychological Review,
                                                                                  100(4), 589-608.
                                                        THINGS YOU
                                                        SIT ON
                                                                                Forde, E. M. E., & Humphreys, G. W. (1999). Category-
                                                                                  specific recognition impairments: A review of important
                                         Response                Comparison
                                                                                  case studies and influential theories. Aphasiology. 13(3),
           Figure 6: MSH explanation of Experiment 3
                                                                                  169-193.
                                                                                Gainotti, G. & Silveri, M. C. (1996). Cognitive and
    The fact that verbal input seems to have faster access to
                                                                                  anatomical locus of lesion in a patient with a category-
functional information than to categorical information can
                                                                                  specific semantic impairment for living beings. Cognitive
be explained by the fact that the functional information is
                                                                                  Neuropsychology, 13(3), 357-389.
possibly more strongly and/or directly connected to a                           Guenther, R. K., Klatzky, R. L., and Putnam, W. (1980).
particular concept node in verbal/abstract semantics than                         Commonalities and Differences in Semantic Decisions
superordinate category information. This could potentially                        about Pictures and Words. Journal of Verbal Learning and
be the case for the man-made objects used in Experiment 3,
                                                                                  Verbal Behavior. 19, 54-74.
as it would fit with the sensory/functional hypothesis
                                                                                Martin, A., Haxby, J. V., Lalonde, F. M., Wiggs, C. L., &
discussed above.
                                                                                  Ungerleider, L. G. (1995). Discrete cortical regions
                                                                                  associated with knowledge of color and knowledge of
                                                                                  action. Science, 270(5233), 102-105.
                            Conclusion                                          Martin, A., Wiggs, C. L., Ungerleider, L. G., & Haxby, J. V.
   The experiments presented in this paper evidence in favor                      (1996). Neural correlates of category-specific knowledge.
of the multiple-semantics hypothesis explanation of how                           Nature, 379, 649-652
information is organized in semantic memory. The results of                     McCarthy, R. A. & Warrington, E. K. (1988). Evidence for
Experiment 1 are most consistent with the MSH view if one                         modality-specific meaning systems in the brain. Nature.
simply presumes, not without merit, that the like-modal                           334, 428-430.
match condition trials benefit from a semantic priming                          Potter, M. C., and Faulconer, B. A. (1975). Time to
effect. Experiment 2 rules out the possibility of a task-                         understand pictures and words. Nature. 253, 437-438.
switching effect being responsible for the results of                           Riddoch, M. J., & Humphreys, G. W. (1987). Visual object
Experiment 1, and the results from Experiment 3, when                             processing in optic aphasia: A case of semantic access
viewed in conjunction with the results from Experiment 1,                         agnosia. Cognitive Neuropsychology, 4(2), 131-185.
are best explained by the MSH. Functional information                           Seifert, L. (1997). Activating Representations in Permanent
would be more strongly related than category information to                       Memory: Different Benefits for Pictures and Words.
verbal semantics, but category information would be more                          Journal of Experimental Psychology: Learning, Memory,
strongly associated with visual semantics than functional                         and Cognition. 23, 1106-1121
information. Future work will explore categorization of                         Shallice, T. (1987). Impairments of semantic processing:
man-made objects and functional categorization of natural                         Multiple dissociations. In M. Coltheart, R. Job, & G.
objects to confirm these findings.                                                Sartori (Eds.) The cognitive neuropsychology of
                                                                                  language. London: Lawrence Earlbaum Associates Ltd.
                            References                                          Theios, J., and Amrhein, P.C. (1989). Theoretical Analysis
Beauvois, M. (1982). Optic aphasia: a process of interaction                      of the Cognitive Processing of Lexical and Pictorial
   between vision and language. Philosophical Transactions                        Stimuli: Reading, Naming, and Visual and Conceptual
   of the Royal Society of London. 298, 35-47.                                    Comparisons. Psychological Review. 96, 5-24.
Caramazza, A., Hillis, A. E., Rapp, B. C., & Romani, C.                         Warrington, E. K., & McCarthy, R. A. (1983). Category
   (1990). The multiple semantics hypothesis: Multiple                            specific access dysphasia. Brain, 106, 859-878.
   confusions? Cognitive Neuropsychology. 7, 161-189.                           Warrington, E. K., & McCarthy, R. A. (1987). Categories of
Caramazza, A. & Shelton, J. R. (1998). Domain-specific                            knowledge: further fractionation and an attempted
   knowledge systems in the brain: The animate-inanimate                          integration. Brain, 110, 829-854.
   distinction. Neurocase Special Issue: Category-specific                      Warrington, E.K., & Shallice, T. (1984). Category-specific
   deficits, 4(4-5), 339-351.                                                     semantic impairments. Brain, 107, 829-853.
                                                                            802

