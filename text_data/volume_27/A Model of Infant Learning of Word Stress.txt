UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
A Model of Infant Learning of Word Stress
Permalink
https://escholarship.org/uc/item/6h942407
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 27(27)
Authors
Gerken, LouAnn
Shultz, Thomas R.
Publication Date
2005-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

                                  A Model of Infant Learning of Word Stress
                                        Thomas R. Shultz (thomas.shultz@mcgill.ca)
             Department of Psychology and School of Computer Science, McGill University, 1205 Penfield Avenue
                                                    Montreal, QC H3A 1B1 Canada
                                           LouAnn Gerken (gerken@u.arizona.edu)
                                 Departments of Psychology and Linguistics, University of Arizona
                                                     Tucson, AZ 85721-0068 USA
                             Abstract                                one ranking was not attested in the initial input, but it could
                                                                     be inferred from attested rankings, based on transitivity. For
   Nine-month-old infants can distinguish the word-stress            example, if learners have evidence that constraint A
   patterns of two artificial languages after a few minutes of       outranks (>>) constraint B and B >> C, they should be able
   exposure to words from one of the languages, apparently by
                                                                     to infer that A >> C. Adults showed evidence of accepting
   making transitive inferences from known word-stress
   constraints to unknown constraints. We report on a neural-        test words reflecting the unattested ranking from their
   network simulation of these data using the sibling-descendant     training grammar, while rejecting very similar words that
   cascade-correlation algorithm. The simulations cover the          reflected a different grammar (Guest et al., 2000).
   infant data and generate some predictions for further infant         To determine if infants would behave similarly, 18 nine-
   research.                                                         month-olds were familiarized for two minutes to three- and
                                                                     five-syllable words (Gerken, 2004). The stress patterns in
                         Introduction                                these spoken words conformed to either Language 1 (n = 9;
                                                                     Table 1) or Language 2 (n = 9; Table 2). In these tables,
Learning how to stress the syllables in words is an important
                                                                     syllables in upper case are stressed, whereas those in lower
part of acquiring a language, easily distinguishing skilled          case are unstressed. Seven variants of each familiarization
from novice speakers. Word stress is an interesting problem          and test word were created using the seven solfège syllables
to study because it has been given a fairly complete                 (do, re, mi, fa, so, la, ti) and substituting re for do, mi for re,
linguistic description, it can be studied independently of           etc., in the example words of Tables 1 and 2. No
other aspects of language, and it has attracted competing            substitutions were made for the syllable ton.
computational models using principles and parameters on
the one hand (Dresher & Kaye, 1990) and neural networks                Table 1: Example words and constraint rankings for infants
on the other (Gupta & Touretzky, 1994).                                                      familiarized to L1.
   Remarkably, infants as young as 9 months can learn                             Familiarization         Attested ranking
something about novel word-stress patterns with a few
                                                                                  TON ton do RE mi A >> B
minutes of exposure to artificial words (Gerken, 2004).
Such infant research, perhaps coupled with computational                          TON do re               B >> C1
modeling, has the potential to uncover some of the                                DO re TON               B >> C1
fundamental properties of the human language-learning                             DO re TON mi fa         B >> C1
system (Gómez & Gerken, 2000; Shultz, 2003). This paper                           DO re mi FA so          C1 >> D1
reports on artificial-neural-network simulations of these                         L1 test                 Inferred ranking
infant data. We first review the infant research, then discuss                    do TON re MI fa         A >> D1
the properties of our computational model, and finally
present three simulations.                                             Table 2: Example words and constraint rankings for infants
                                                                                             familiarized to L2.
                   Evidence with Infants                                          Familiarization         Attested ranking
Gerken (2004) asked whether 9-month-olds could learn an                           do RE mi ton TON A >> B
optimality-theoretic stress system that had been used in an                       do re TON               B >> C2
adult production study by Guest, Dell, and Cole (2000). The                       TON do RE               B >> C2
logic of the adult experiment was to expose learners to                           do re TON mi FA         B >> C2
multisyllabic nonwords whose stress patterns provided                             do RE mi fa SO          C2 >> D2
evidence for several rankings of stress constraints. In                           L2 test                 Inferred ranking
optimality theory (Prince & Smolensky, 1997), different                           do RE mi TON fa         A >> D2
stress-assignment constraints can conflict in their
application to a particular word. When two constraints do               The following four word-stress constraints, all of which
conflict, only the more highly ranked applies. Importantly,           are typical of constraints in natural languages, were used in
in the Guest et al. (2000) and Gerken (2004) experiments,             the study:
                                                                 2015

A. Two stressed syllables cannot be adjacent.                      obvious adaptive value in promoting cognitive and
B. Heavy syllables (i.e., those ending in a consonant) are         linguistic development. The building of categories for
     stressed.                                                     familiar stimuli is typically discussed in terms of
C. Syllables are stressed if they are second to last (C1 in        recognition memory. If a stimulus is recognized as a
     L1) or second (C2 in L2).                                     member of a familiar category, then it often elicits less
D. Alternating syllables are stressed, starting from the left      attention than a stimulus not recognized as familiar.
     (D1 in L1) or right (D2 in L2).                                  Recently it has become possible to simulate such
   The familiarized words provide evidence for three               processes with artificial neural networks. One of the most
rankings of stress principles. In L1, A >> B, B >> C1, and         effective techniques employs feed-forward encoder
C1 >> D. For example, the word TON do re in L1 attests that        networks (Mareschal & French, 2000; Mareschal, French, &
constraint B (heavy syllables are stressed) outranks               Quinn, 2000; Shultz & Bale, 2001; Shultz & Cohen, 2004).
constraint C1 (syllables are stressed if they are second to        In such networks, stimulus features are represented as real
last). Likewise, in L2, A >> B, B >> C2, and C2 >> D. No           numbers in an input vector, then encoded in representations
direct evidence was provided that A >> D1 (for L1) or A >>         on a relatively small number of hidden units, and finally
D2 (for L2). However, these unattested rankings could be           decoded on an output vector. Discrepancy between output
inferred using transitivity from the attested rankings.            and input representations is computed as network error.
   During testing, infants heard on different trials words with    Familiar stimuli produce less error than novel stimuli, and
new stress patterns that were consistent with A >> D1 or A         this extra error indicates that the novel stimuli merit further
>> D2 inferences. Note that L1 and L2 test items had the           processing and perhaps learning. Such encoder networks are
same stress pattern (second and fourth syllables stressed)         able to generalize, abstract prototypes, and complete input
and differed only in the location of the heavy syllable TON.       patterns that are missing components. After training, error
Therefore if infants can discriminate the two types of test        on familiar stimuli is typically less than that on novel
items, they presumably did so by making inferences across          stimuli. Various learning algorithms have been employed
words in their familiarization language.                           for this purpose, including back-propagation (BP) with
   Infants were tested in a head-turn preference procedure,        static networks and cascade-correlation (CC) with
and their looking times revealed a significant                     constructive networks (Shultz, 2003).
familiarization-language x test-language interaction, F(1,            Here we report on our efforts to simulate Gerken’s (2004)
16) = 7.78, p < .02. Infants familiarized with L1 listened         experiments on word stress with a variant of CC called
longer to L2 test words and vice versa, as shown in Figure         sibling-descendant cascade-correlation (SDCC; Baluja &
1. These data, which have been replicated at least once, F(1,      Fahlman, 1994). Like CC, SDCC recruits new hidden units
16) = 9.97, p < .01 (Gerken, 2004), suggest that infants were      as it needs them to learn. But unlike CC, which installs each
able to make inferences across multiple words in their             new hidden unit on a separate layer, SDCC determines
familiarization language, and on that basis could distinguish      whether it is better to install each new recruit on the current
the stress patterns of the two languages.                          highest layer of hidden units (as a sibling) or on its own new
                                                                   layer (as a descendant). Algorithms that construct their own
                                                                   networks, like CC and SDCC, have been found better at
                                   L1 test   L2 test
                                                                   simulating a variety of phenomena in psychological
                                                                   development than algorithms like BP that merely learn to
                         10                                        adjust connection weights in a static, pre-designed network
    Mean looking (sec)
                         8                                         topology (Shultz, 2005). The mathematics underlying CC
                                                                   and SDCC can be found elsewhere (Shultz, 2003).
                         6
                         4                                             Simulation 1: Familiarization x Test-pattern
                         2                                                             Interaction
                                                                    The principal aim of the present simulations was to capture
                         0
                                                                    Gerken’s (2004) familiarization-language x test-language
                              L1                       L2
                                                                    interaction with the tests discussed earlier. To this end, we
                                   Familiarization                  created seven examples of each of five training-word types,
                                                                    totaling 35 training patterns and seven examples of test
                                                                    words, just as in the infant experiments.
 Figure 1: Infant interest in hearing word-stress patterns in
                       two languages.
                                                                    Coding of Words
         Properties of Our Computational Model                      We coded the words used in Gerken’s (2004) experiments
                                                                    on a sonority scale, shown in Table 3. This scale is based on
In familiarization experiments like those of Gerken (2004),         phonological research (Vroomen, van den Bosch, & de
it is assumed that infants build categories for repeated            Gelder, 1998) and has been used to simulate other infant
stimuli and then subsequently ignore stimuli corresponding          experiments with artificial-language stimuli (Shultz & Bale,
to their categories and concentrate instead on stimuli that are     2001). Sonority is the quality of vowel likeness and it has
relatively novel. Such shifts of attention would be of              both acoustic and articulatory aspects. As can be seen in
                                                                2016

Table 3, the sonority scale ranges from -6 to 6 in steps of 1,        In input phase, weights from inputs and existing hidden
with a gap and change of sign between the consonants and           units to candidate recruits are adjusted in order to increase
vowels.                                                            the size of a correlation between candidate-unit activation
                                                                   and network error. When those correlations stagnate, the
              Table 3: Phoneme sonority scale.                     unit with the largest absolute correlation is recruited, and the
      Phoneme category          Examples        Sonority           other candidates are discarded. The candidate pool contains
      low vowels                /a/ /æ/                  6         equal numbers (4) of siblings and descendants, each with
      mid vowels                                         5         initially random connection weights from input units and
                                /Є/ /e/ /o/ /ɔ/                    any existing hidden units. As is customary with SDCC,
      high vowels               /I/ /i/ /U/ /u/          4         recruitment of descendant candidate units was penalized by
      semi-vowels, laterals     /w/ /y/ /l/             -1         multiplying their correlations with error by a factor of 0.8
      nasals                    /n/ /m/ / ŋ /           -2         (Baluja & Fahlman, 1994).
      voiced fricatives         /z/ /ʒ/ /v/             -3            We tested each network’s performance every 25 output
      voiceless fricatives      /s/ /ʃ/ /f/             -4         epochs. Training was stopped after 280 total epochs because
      voiced stops              /b/ /d/ /g/             -5         pilot simulations suggested that this amount of training
      voiceless stops           /p/ /t/ /k/             -6         provided a good match to the size of the key interaction F
                                                                   ratio in Gerken’s (2004) infant experiments. The simulation
Note. Example phonemes are represented in International
                                                                   results reported here are quite robust in that they can be
Phonetic Alphabet. From “Infant familiarization to artificial
                                                                   replicated over a wide range of maximum training epochs.
sentences: Rule-like behavior without explicit rules and
                                                                   Infant trials cannot be precisely equated to network epochs
variables.” By T. R. Shultz and A. C. Bale. In L. R.
                                                                   because it is unknown how much processing occurs during
Gleitman & A. K. Joshi (Eds.), Proceedings of the Twenty-
                                                                   an infant trial.
Second Annual Conference of the Cognitive Science Society
(p. 461), 2000. Mahwah, NJ: Erlbaum. Copyright 2000 by
the Cognitive Science Society, Inc. Adapted by permission.
                                                                    Results
                                                                    After 280 epochs, these networks recruited a mean of 6.2
   Because words could have up to five syllables, with up to        hidden units on a mean of 1.3 layers. The final topology for
three phonemes per syllable, 15 units were required to code         a typical network with four hidden units on one layer and
each word. The stress given to each of the potential five           two hidden units on the next layer is shown in Figure 2. The
syllables was coded as -0.5 for unstressed and 0.5 for              arrows indicate full connectivity, with all of the units in one
stressed. Twenty-five input units coded each word as                layer being connected to all of the units in the next layer.
follows, with subscripts 1-5 indicating the slots for cv or cvc     The bias unit always has an input value of 1 and is
syllables and the stress (s) each syllable received: cvc1 cvc2      connected to all downstream units by trainable connection
cvc3 cvc4 cvc5 s1 s2 s3 s4 s5. Five-syllable words required all     weights, thus establishing a learnable resting level of
five syllable slots for both sonority and stress, but three-        activation for each hidden and output unit. As is customary
syllable words were coded only in slots 2-4. Missing final          with encoder networks, there are no direct input-to-output
consonants and missing syllables were coded as 0.0.                 connections here. This is to prevent trivial solutions in
Twenty-five output units had this same structure. Codes for         which a network might learn weights of about 1 from each
the phonemes of each syllable are shown in Table 4.                 input unit to its corresponding output unit. Such solutions
                                                                    would memorize the training patterns quickly but would not
             Table 4: Sonority codes for syllables.                 generalize well to untrained test patterns.
        Syllable Consonant1 Vowel Consonant2
        do                  -5.0        5.0          0.0
        re                  -1.0        5.0          0.0
        mi                  -2.0        4.0          0.0                     20 output units
        fa                  -4.0        6.0          0.0
        so                  -4.0        5.0          0.0
        la                  -1.0        6.0          0.0
        ti                  -6.0        4.0          0.0                                                     2 hidden units
        ton                 -6.0        5.0         -2.0
Procedures and Parameters
We ran nine networks in each familiarization condition in                                                    4 hidden units
order to match the statistical power of the infant
experiments. A training epoch is a single pass through all of
the training patterns. SDCC networks alternate between two
phases: output phase and input phase. In output phase,
weights entering output units are adjusted in order to reduce             Bias unit           20 input units
network error. When error reduction stagnates, SDCC
switches to input phase in order to recruit a new hidden unit.              Figure 2: Final topology of an SDCC network.
                                                               2017

                                                                 might matter a lot because they would break the transitive
   As is typical of both infant attention to a repeated          inference chain required to make the A >> D inference.
stimulus category and CC and SDCC simulations of infants           To test this idea, we ran a simulation in which we deleted
in such experiments, network error decreased exponentially       the third of the B >> C word types from each language for
over time. Error reduction in a typical network is plotted in    18 networks in one condition, and the C >> D word type
Figure 3.                                                        from each language for 18 networks in another condition.
                                                                 See Tables 1 and 2 for the particular word types deleted.
                40                                                 Network error was subjected to a mixed ANOVA in
                                                                 which deletion condition and familiarization language
                30                                               served as between-network factors and test language served
                                                                 as a within-network factor. With a significant deletion x
   Error        20                                               familiarization-language x test-language interaction, F(1,
                                                                 32) = 17, p < .001, we then analyzed the familiarization-
                10                                               language x test-language interaction for each deletion
                                                                 condition. The familiarization-language x test-language
                0                                                interaction was significant for the C >> D deletions, F(1,
                     1   2    3      4       5      6   7        16) = 6.7, p < .03, as well as the B >> C deletions, F(1, 16)
                                                                 = 27, p < .001. Interaction means are plotted in Figure 5 for
                              25th output epoch
                                                                 the B >> C deletions, and in Figure 6 for the C >> D
                                                                 deletions. It is noteworthy that the variance associated with
   Figure 3: Error reduction in a representative network.        the familiarization-language x test-language interaction was
                                                                 36 times larger for the B >> C deletions than the
  Network error after 280 epochs was subjected to a mixed        corresponding variance for the C >> D deletions, showing
ANOVA in which familiarization language served as a              that the C >> D deletions were more disruptive to this key
between-network factor and test language served as a             interaction than were the B >> C deletions, as expected.
within-network factor. The key interaction between the two
                                                                                              L1 test    L2 test
was significant, F(1, 16) = 15, p < .001. The associated
means are plotted in Figure 4. As with infants, there was
more interest in the test language with novel stress patterns,                     40
signaled here by network error.
                                                                                   30
                                                                      Mean error
                               L1 test   L2 test
                                                                                   20
                30
                                                                                   10
   Mean error
                20                                                                 0
                                                                                        L1                         L2
                10                                                                             Familiarization
                0                                                    Figure 5: Network interest in word-stress patterns in two
                         L1                        L2                    languages when a B >> C word type is deleted.
                               Familiarization
                                                                                              L1 test    L2 test
  Figure 4: Network interest in word-stress patterns in two
                        languages.                                                 40
                                                                                   30
  Simulation 2: Effects of Constraint Deletion
                                                                      Mean error
If the results of the infant experiments and Simulation 1 are                      20
due to transitive inferences about constraints, then it should
be possible to produce different results depending on which                        10
constraints are deleted from the familiarization words. For
                                                                                   0
example, deletion of one of the three B >> C familiarization
word types should disrupt transitive inferences less than                               L1                         L2
deletion of the only C >> D familiarization word type. The                                     Familiarization
former deletions should not matter much because there are
still two other word types present that attest to the B >> C
                                                                     Figure 6: Network interest in word-stress patterns in two
constraint ranking. In contrast, the latter C >> D deletions
                                                                        languages when the C >> D word type is deleted.
                                                              2018

                                                                                          Discussion
  Thus, omitting a unique link in the transitive-inference
                                                                  The infant results simulated here are quite remarkable when
chain disrupts the key familiarization-language x test-
                                                                  one considers the nature of the test words. The two test
language interaction more than omitting a redundant link
                                                                  words have entirely different stress patterns than do the
does.
                                                                  familiarization words in either L1 or L2. Moreover, the two
                                                                  test words exhibit the same stress pattern, differing only in
       Simulation 3: Position of a Heavy Syllable                 the location of the heavy syllable TON. Therefore, the
A reviewer of the paper reporting the infant experiments          obtained familiarization x test interaction suggests that
(Gerken, 2004) suggested an alternative explanation for           infants are able to generalize beyond stress patterns
successful generalization to the test sentences. This reviewer    encountered during familiarization to the abstract system
noted that the heavy syllable TON occurs earlier in L1            underlying these patterns. That a simple neural-network
familiarization words than in L2 familiarization words.           model, after being familiarized to words in the same fashion
These average positions happen to correlate with the              as the infants were, can also generalize in this manner is
positions of TON in the test sentences. TON is in second          perhaps equally remarkable. How do they (infants or
position in the test sentence for L1 and fourth position in the   networks) do that, particularly on the basis of rather limited
test sentence for L2. Perhaps infants generalized to the test     exposure to a few example words? Poverty-of-the-stimulus
sentences, not by performing transitive inference on              arguments might well be employed to support some sort of
constraint rankings, but rather by using the relative positions   innate knowledge of word-stress rules (Dresher & Kay,
of the heavy syllable.                                            1990). We are still quite far from understanding how infants
   We tested this idea in neural networks by equating the         execute this skill, but it is somewhat easier to examine
position of TON in the two languages. We omitted the first        computational models to determine how they perform.
B >> C familiarization word type from both languages and             Although we have described the inferences on which this
doubled the frequency of the second B >> C familiarization        generalization is based in terms of transitive reasoning, it is
word types. As the reader can verify from Tables 1 and 2,         noteworthy that SDCC networks do not perform any explicit
this yields a mean serial position of 3.0 for TON in the          logical reasoning on symbolic propositions. A simulation of
familiarization words of each language (assuming that three-      transitive inference in older children and adults shows that
syllable words are coded on the middle-three banks of input       CC networks, even without hidden units, can simulate such
and output units). With these changes to the familiarization      inferences in an entirely neural manner (Shultz & Vogel,
languages, nine networks were run in each familiarization         2004). An important computational trick used by those
condition as in Simulation 1.                                     networks is to learn connection-weight strengths that
   Network error was analyzed as in Simulation 1. The             effectively represent the linear order of the training stimuli.
familiarization-language x test-language interaction was still    Such weights can then be used to make accurate
present, F(1, 16) = 38, p < .001. Associated means are            comparisons of unattested stimulus pairs. In several ways,
plotted in Figure 7. As far as networks are concerned, the        the word stimuli and training regime used in the present
serial position of the heavy syllable TON is irrelevant to the    simulations are more complicated, but the ability of the
expected interaction between familiarization language and         learning algorithm to construct an essentially neural solution
test language. Coupled with the results of Simulation 2, this     to a transitivity problem is comparable. We plan to explore
suggests that, at least for networks, differential                the exact nature of this solution to word-stress assignments
generalization to the familiar and novel languages arises, not    in a longer paper in which we perform detailed analyses of
from the position of the heavy syllable, but rather from          network knowledge representations.
transitive inferences across known constraints.                      As far as we are aware, no other computational models
                                                                  have yet been applied to these infant data. Of the two most
                                                                  prominent models in the area of word-stress learning, it is
                                                                  interesting that one is a symbolic model based on principles-
                               L1 test   L2 test                  and-parameters theory (Dresher & Kaye, 1990) and the
                                                                  other is a connectionist model using static BP networks
               150                                                (Gupta & Touretzky, 1994). One thing that these two, very
                                                                  different models share is that each learning algorithm is
                                                                  presented with stress-pattern information abstracted away
  Mean error
               100
                                                                  from the actual phonemes in a word. As might be expected,
               50                                                 such pre-processing of the input simplifies the learning task
                                                                  immensely. Speech sounds can be ignored in these models,
                0                                                 enabling the learning algorithm to focus only on the pre-
                      L1                           L2             abstracted stress patterns, which are identical for all words
                               Familiarization                    of the same number of syllables in the language being
                                                                  learned. Although these models have a number of
  Figure 7. Network interest in word-stress patterns in two       interesting features, we believe that our model is more
    languages with equated serial positions of the heavy          realistic in view of the fact that both numerically-coded
                         syllable.                                speech sounds and syllabic-stress information are included.
                                                                  This might enable our model to deal with anticipated effects
                                                              2019

of phonological content on the learnability of stress patterns.    Gómez, R. L., & Gerken, L. A. (2000). Infant artificial
Humans, whether infants or adults, are never presented with          language learning and language acquisition. Trends in
abstract stress patterns, only with streams of speech in             Cognitive Sciences, 4, 178-186.
which word syllables are stressed according to the stress          Gupta, P., & Touretzky, D. S. (1994). Connectionist models
syntax of the language.                                              and linguistic theory: Investigations of stress systems in
   We plan to implement and compare alternate models to              language. Cognitive Science, 18, 1-50.
our SDCC model in future work. The principles-and-                  Guest, D. J., Dell, G. S., & Cole, J. S. (2000). Violable
parameters model (Dresher & Kaye, 1990) will be                      constraints in language production: Testing the transitivity
particularly interesting to study in this context because            assumption of Optimal Theory. Journal of Memory &
many of its predictions would be quite different from those          Language, 42, 272-299.
that our model would make.                                          Mareschal, D., & French, R. M. (2000). Mechanisms of
   One current prediction of our model concerns the                  categorization in infancy. Infancy, 1, 59-76.
differential effects on test-word performance of deleting          Mareschal, D., French, R. M., & Quinn, P. (2000). A
certain word-stress constraints from the familiarization             connectionist account of asymmetric category learning in
words. Simulation 2 showed that deleting one of the three            infancy. Developmental Psychology, 36, 635-645.
familiarization word types attesting to the B >> C constraint      Prince, A., & Smolensky, P. (1997). Optimality: From
disrupted performance on the test words less than did                neural networks to universal grammar. Science, 275
deleting the familiarization word type containing the only           (5306), 1604-1610.
evidence of the C >> D constraint. Although it is likely that      Shultz, T. R. (2003). Computational developmental
a model using explicit transitive inference would predict            psychology. Cambridge, MA: MIT Press.
something similar, its prediction would be more extreme            Shultz, T. R. (2005, in press). Constructive learning in the
than the one made by our model. This is because our model,           modeling of psychological development. In Y. Munakata
even without the important C >> D familiarization                    & M. H. Johnson (Eds.), Processes of change in brain
information, still retained some capacity to make a correct          and cognitive development: Attention and performance
transitive inference regarding the A >> D constraint. An             XXI. Oxford: Oxford University Press.
explicit-reasoning model would likely generate no A >> D           Shultz, T. R., & Bale, A. C. (2001). Neural network
inference at all without some C >> D evidence. Like other            simulation of infant familiarization to artificial sentences:
neural-network models, ours predicts a graceful degradation          Rule-like behavior without explicit rules and variables.
of performance in the face of important evidential gaps, in          Infancy, 2, 501-536.
contrast to the more brittle performance of an explicit-           Shultz, T. R., & Cohen, L. B. (2004). Modeling age
reasoning model.                                                     differences in infant category learning. Infancy, 5, 153-
   Another prediction of our model ruled out an alternate            171.
hypothesis concerning the relative position of a heavy             Shultz, T. R., & Vogel, A. (2004). A connectionist model of
syllable in both familiarization words and test words. Even          the development of transitivity. Proceedings of the
when mean serial positions of the heavy syllable were                Twenty-sixth Annual Conference of the Cognitive Science
equated across the two languages, networks were still                Society (pp. 1243-1248). Mahwah, NJ: Erlbaum.
relatively more interested in the novel language, as revealed      Vroomen, J., van den Bosch, A., & de Gelder, B. (1998). A
by a difference in mean network error. Predictions such as           connectionist model for bootstrap learning of syllabic
these should be interesting to test with infants.                    structure. Language and Cognitive Processes, 13, 193-
                                                                     220.
                     Acknowledgments
This work was supported by a grant from the Natural
Sciences and Engineering Research Council of Canada to T.
R. Shultz and by NIH grant R01HD42170 to R. Gómez, L.
A. Gerken, and E. Plante. Thanks to Gabrielle Pagé, J.-P.
Thivierge, and Frederic Dandurand for helpful comments
on an earlier draft.
                         References
Baluja, S., & Fahlman, S. E. (1994). Reducing network
   depth in the cascade-correlation learning architecture.
   Technical Report CMU-CS-94-209, School of Computer
   Science, Carnegie Mellon University.
Dresher, B., & Kaye, J. (1990). A computational learning
   model for metrical phonology. Cognition, 34, 137-195.
Gerken, L. A. (2004). Nine-month-olds extract structural
   principles required for natural language. Cognition, 93,
   B89-B96.
                                                               2020

