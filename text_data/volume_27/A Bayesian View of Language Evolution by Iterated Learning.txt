UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
A Bayesian View of Language Evolution by Iterated Learning
Permalink
https://escholarship.org/uc/item/0vb7c896
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 27(27)
Authors
Griffiths, Thomas L.
Kalish, Michael L.
Publication Date
2005-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

        A Bayesian View of Language Evolution by Iterated Learning
                             Thomas L. Griffiths (tom griffiths@brown.edu)
             Department of Cognitive and Linguistic Sciences, Brown University, Providence, RI 02912
                                   Michael L. Kalish (kalish@louisiana.edu)
              Institute of Cognitive Science, University of Louisiana at Lafayette, Lafayette, LA 70504
                        Abstract                                    Kirby, & Brighton, 2003), and minimum description
                                                                    length (Brighton, 2002). Iterated learning with these
  Models of language evolution have demonstrated how                algorithms produces languages that possess one of the
  aspects of human language, such as compositionality,
  can arise in populations of interacting agents. This pa-          most compelling properties of human languages: compo-
  per analyzes how languages change as the result of a              sitionality. In a compositional language, the meaning of
  particular form of interaction: agents learning from one          an utterance is a function of the meaning of its parts.
  another. We show that, when the learners are rational             The intuitive explanation for these results is that the
  Bayesian agents, this process of iterated learning con-
  verges to the prior distribution over languages assumed           regular structure of compositional languages means that
  by those learners. The rate of convergence is set by              they can be learned from less data, and are thus more
  the amount of information conveyed by the data seen               likely to pass through the information bottleneck.
  by each generation; the less informative the data, the               These instances of compositionality emerging from it-
  faster the process converges to the prior.
                                                                    erated learning raise an important question: what lan-
                                                                    guages will survive many generations of iterated learn-
   Human languages form a subset of all logically pos-
                                                                    ing? While the circumstances under which composi-
sible communication schemes, with universal properties
                                                                    tionality will emerge from iterated learning with specific
shared by all languages (Comrie, 1981; Greenberg, 1963;
                                                                    learning algorithms have been investigated (Brighton,
Hawkins, 1988). A traditional explanation for these lin-
                                                                    2002; Smith et al., 2003), there are no general results
guistic universals is that they are the consequence of
                                                                    for arbitrary properties of languages or broad classes
constraints on the set of learnable languages imposed by
                                                                    of learning algorithms. In this paper, we analyze iter-
an innate, language-specific, genetic endowment (e.g.,
                                                                    ated learning for the case where the learners are rational
Chomsky, 1965). Recent research has explored an alter-
                                                                    Bayesian agents. A variety of learning algorithms can be
native explanation: that universals emerge from evolu-
                                                                    formulated in terms of Bayesian inference, and Bayesian
tionary processes produced by the transmission of lan-
                                                                    methods underlie many approaches in computational lin-
guages across generations (e.g., Kirby, 2001; Nowak,
                                                                    guistics (Manning & Schütze, 1999). The assumption
Plotkin, & Jansen, 2000). Languages change as each gen-
                                                                    that the learners are Bayesian agents makes it possible to
eration learns from that which preceded it. This process
                                                                    derive analytic results indicating which languages will be
of iterated learning implicitly selects for languages that
                                                                    favored by iterated learning. In particular, we prove the
are more learnable. This suggests a tantalizing hypoth-
                                                                    surprising result that the probability distribution over
esis: that iterated learning might be sufficient to explain
                                                                    languages resulting from iterated Bayesian learning con-
the emergence of linguistic universals (Briscoe, 2002).
                                                                    verges to the prior probability distribution assumed by
   Kirby (2001) introduced a framework for exploring
                                                                    the learners. This implies that the asymptotic probabil-
this hypothesis, called the iterated learning model (ILM).
                                                                    ity that a language is used does not depend at all upon
In the ILM, each generation consists of one or more
                                                                    the properties of the language, being determined entirely
learners. Each learner sees some data, forms a hypothe-
                                                                    by the assumptions of the learner.
sis about the process that produced that data, and then
produces the data which will be supplied to the next
generation of learners, as shown in Figure 1 (a). The                          le                ge
                                                                                                       on
                                                                                                              le                ge
                                                                                                                                      on
                                                                                                                                             le
                                                                                 ar                             ar                             ar
languages that succeed in being transmitted across gen-                            nin
                                                                                      g
                                                                                                   ner
                                                                                                      at          nin
                                                                                                                     g
                                                                                                                                  ner
                                                                                                                                     at          nin
                                                                                                                                                    g
                                                                                                        i                              i
erations are those that pass through the “information               (a) data        hypothesis         data        hypothesis         data              ···
bottleneck” imposed by iterated learning. If particular
properties of languages make it easier to pass through              (b) x0                              x1                             x2
that bottleneck, then many generations of iterated learn-
ing might allow those properties to become universal.
                                                                                                                                                        ···
   The ILM can be used to explore how different as-                     y0                h1            y1               h2            y2
sumptions about language learning influence language
evolution. A variety of learning algorithms have been
examined using the ILM, including a heuristic gram-                 Figure 1: (a) Iterated learning. (b) Dependencies among
mar inducer (Kirby, 2001), associative networks (Smith,             variables in iterated iterated Bayesian learning.
                                                              827

             Iterated Bayesian learning                             defining a sequence of random variables in which hn+1
Following most of the work applying iterated learning               is independent of all previous hypotheses given hn . This
to language evolution, we will assume that our learners             is a Markov chain, with state space H and transition
are faced with a function learning task: given a set of             matrix T (hn , hn+1 ) = p(hn+1 |hn ).2
m inputs, x = {x1 , . . . , xm }, and m corresponding out-             Markov chains are a common form of stochastic pro-
puts, y = {y1 , . . . , ym }, the learner has to estimate the       cess with well-understood properties (see Norris, 1997).
probability distribution over y for each x. In a language           In particular, identifying a process as a Markov chain im-
learning setting, x is usually taken to be a set of “mean-          mediately provides insight into its asymptotic behavior.
ings” or events in the world, and y is taken to be the set          If a Markov chain with transition matrix T (hn , hn+1 ) is
of utterances associated with those events. We will use             ergodic, then it will converge to a stationary distribution
X and Y to denote the set of values that x and y can                π(h) satisfying the equation
take on.                                                                                         X
   Iterated learning begins with some initial data,                              π(hn+1 ) =            T (hn , hn+1 )π(hn )              (2)
(x0 , y0 ), presented to the first learner, who then gener-                                     hn ∈H
ates outputs y1 in response to some new inputs x1 . The
second learner sees (x1 , y1 ), and generates y2 in response        for all hn+1 . Intuitively, convergence to the stationary
to x2 . This process continues for each successive gener-           distribution means that regardless of the initial state h1
ation, with learner n + 1 seeing (xn , yn ) and generating          (or, in our setting, the data on which that hypothesis is
yn+1 in response to xn+1 . The result of this process               based), the probability distribution over hn approaches
depends upon the algorithm used by the learners.                    π(hn ) as n → ∞. Equation 2 indicates why π(hn ) is the
   We will assume that our learners are Bayesian agents,            “stationary” distribution: if hn follows this distribution,
supplied with a finite discrete1 hypothesis space H and             then so will hn+1 , and likewise for every hn+k for k > 0.
a prior probability distribution p(h) for each hypothe-                The conditions for ergodicity are given in Norris
sis h ∈ H. In a function learning task, each hypothesis             (1997). The most important condition in our setting is
h corresponds to a conditional probability distribution             irreducibility: for every pair of hypotheses h and h′ , there
p(y|x, h), specifing the distribution over all sets of out-         must be some k such that the probability of going from
puts for any set of inputs. In the learning step of the pro-        h to h′ in k steps is greater than zero. If this condition is
cess illustrated in Figure 1 (a), learner n+1 sees (xn , yn ),      violated, it is possible to enter sets of states from which
and computes a posterior distribution over hn+1 using               there is no departure, preventing the chain from visiting
Bayes’ rule                                                         all of the states which have some probability under the
                                                                    stationary distribution.
                               p(yn |xn , hn+1 )p(hn+1 )
          p(hn+1 |xn , yn ) =                                          The stationary distribution, π(h), for the Markov
                                      p(yn |xn )                    chain with transition matrix T (hn , hn+1 ) = p(hn+1 |hn )
where                        X                                      satisfies the equation
              p(yn |xn ) =       p(yn |xn , h)p(h).                                              X
                             h∈H                                                 π(hn+1 ) =            p(hn+1 |hn )π(hn ).
We will assume that the learners consider     Q each yi inde-
                                                                                                hn ∈H
pendent given xi and h, so p(y|x, h) = i p(yi |xi , h). In
the production step, learner n + 1 sees xn+1 , generated            If we take π(h) = p(h), we obtain
from a distribution q(xn ) that is independent of all other                                2                                      3
variables. The change in notation is a reminder that un-             p(hn+1 )   =
                                                                                      X
                                                                                           4
                                                                                             X X
                                                                                                      p(hn+1 |x, y)p(y|x, hn )q(x)5 p(hn )
like all of the other distributions we have mentioned,                               hn ∈H  x∈X y∈Y
q(x) expresses the objective probability of an event in                              X X
                                                                                                            2
                                                                                                               X
                                                                                                                                     3
the world, rather than a subjective probability assessed                        =             p(hn+1 |x, y) 4       p(y|x, hn )p(hn )5 q(x)
by the learner. Given xn+1 , the learner samples a hy-                               x∈X y∈Y                  hn ∈H
                                                                                     X X p(y|x, hn+1 )p(hn+1 )
pothesis, hn+1 , from p(hn+1 |xn , yn ), and generates yn+1                     =                                    p(y|x)q(x)
from the distribution p(yn+1 |xn+1 , hn+1 ).                                         x∈X y∈Y
                                                                                                     p(y|x)
                                                                                                  2                   3
         A Markov chain on hypotheses
                                                                                              X X
                                                                                =    p(hn+1 )     4     p(y|x, hn+1 )5 q(x).
                                                                                              x∈X   y∈Y
The stochastic process that iterated Bayesian learning
defines on x, y, and h has the dependency structure
shown in Figure 1 (b). It is straightforward to analyze             The two sums on the last line evaluate to 1, showing
the properties of this stochastic process. In particular, if        that p(h) is the stationary distribution of this chain.
we sum over the data (xn , yn ) we obtain the distribution          Consequently, provided the underlying Markov chain is
                   XX                                               ergodic, the distribution over hypotheses entertained by
  p(hn+1 |hn ) =             p(hn+1 |x, y)p(y|x, hn )q(x), (1)      Bayesian learners engaged in iterated learning will con-
                   x∈X y∈Y                                          verge to the prior over hypotheses held by the learners.
    1                                                                   2
      This assumption is not necessary for our results to hold.           The process can also be reduced to a Markov chain on
Similar results can be obtained with continuous hypothesis          data, (xn , yn ), by summing out the hypotheses, hn . Similar
spaces, but the proofs are more involved.                           results hold for this chain, but we omit them due to space.
                                                                828

  An example: evolving compositionality                         smaller, it becomes less likely that one would see a com-
The results in the previous section imply that the asymp-       positional language at all, and a holistic language that
totic probability with which a language is spoken de-           just happened to produce a compositional mapping be-
pends only upon its prior probability, and is not affected      comes more plausible. However, the number of holistic
by any of the properties of the language. This result           languages grows much faster than the number of com-
is counter-intuitive, particularly in the light of previous     positional languages as the space of meanings and utter-
results indicating that iterated learning seems to favor        ances becomes larger, so the value of α needed to over-
particular properties of languages, such as composition-        whelm the advantage of compositional languages rapidly
ality. To gain a deeper understanding of our results, we        becomes extremely small.
examined the consequences of iterated Bayesian learn-              The matrix of transition probabilities p(hn+1 |hn ) can
ing in a simplified version of the scenario used by Kirby       be obtained by summing over all (x, y) pairs, as shown
(2001), Smith et al. (2003), and Brighton (2002) for ex-        in Equation 1. Since there are (22 22 )m such pairs, this
ploring the evolution of compositionality.                      is intractable for large m. Consequently, matrices for
   In our scenario, meanings and utterances each vary           m > 4 were computed approximately using a Monte
along two binary dimensions. This yields a total of four        Carlo method with 1000 samples for each hypothesis.
meanings and four utterances, each corresponding to the         We computed transition matrices for α ∈ {0.01, 0.5},
set {00, 01, 10, 11}. In a holistic language, the mapping       ǫ ∈ {0.01, 0.05}, and m ∈ {1, 2, . . . , 10}. The first col-
between meanings and utterances is arbitrary, and a sin-        umn of Figure 2 shows a portion of some of these tran-
gle word is chosen to represent each meaning without any        sition matrices.
constraints. There are 44 = 256 such languages. In a            The effect of priors
compositional language, the mapping between meanings
                                                                The second column of Figure 2 shows 1000 iterations
and utterances depends upon their parts: the two dimen-
                                                                sampled from four Markov chains (initialized by choosing
sions of meanings are mapped onto the two dimensions
                                                                h1 at random), while the third and fourth columns (la-
of utterances (for simplicity, we assume that the order is
                                                                belled “Chain” and “Prior”) show the distribution over
preserved), and the only uncertainty is in which values
                                                                hypotheses from a single sample of 10000 iterations from
map to one another. There are 22 = 4 such languages.
                                                                those chains and the prior p(h) respectively. The results
   The hypothesis space H thus contains 260 hypotheses,         in (a)-(c) all use α = 0.5 and ǫ = 0.05, giving equal prior
each a mapping between meanings and utterances. For             probability to compositional and holistic languages and
each h ∈ H, we defined the probability distribution over        allowing a reasonable amount of error. The number of
outputs y given the input x to be                               datapoints seen by the learners varies, with m = 1 in
                                                               (a), m = 3 in (b), and m = 10 in (c). While the Markov
                         1 − ǫ x maps to y in h                 chain develops a greater tendency to remain in the same
         p(y|x, h) =       ǫ                            (3)
                           3          otherwise                 state as m increases, indicated by the strong diagonal
                                                                in the transition matrices and the length of the streaks
where ǫ is the “error rate” of production. The prior            in the samples, it maintains the same distribution over
probability of each hypothesis was                              hypotheses, as shown in the “Chain” column. This dis-
                    α                                          tribution matches the prior over hypotheses, consistent
                       4     h is compositional                 with our mathematical analysis.
           p(h) =     1−α                        .      (4)
                      256        h is holistic                     The results shown in Figure 2 (d) illustrate how chang-
                                                                ing the prior changes the stationary distribution. Keep-
This is a hierarchical prior, allocating a probability of α     ing ǫ and m at the same values as in (b), α is set to
to the set of compositional languages and 1−α to the set        0.01, giving extremely low prior probability to the set
of holistic languages, and then spreading this probability      of compositional languages. Consequently, each holis-
uniformly over the hypotheses within those sets.                tic language has a slightly higher probability than any
   Since every language is simply a mapping from mean-          compositional language. The distribution over hypothe-
ings to utterances, our hypothesis space includes four          ses produced by the Markov chain is markedly different
holistic languages that each give the same mapping as           from that in (b), and matches the prior. If compositional
one of the four compositional languages. These lan-             languages have low prior probability, then they do not
guages make the same predictions about inputs and out-          emerge as a result of iterated learning.
puts, as determined by Equation 3, and thus cannot
be discriminated by any data. The advantage of the              Convergence rates
compositional languages over their holistic counterparts        The rate at which a discrete Markov chain converges to
results from the prior defined in Equation 4. If com-           its stationary distribution is determined by the second
positional and holistic languages are equally probable          eigenvalue of the transition matrix, λ2 , with smaller val-
a priori (α = 0.5), then the relatively small number of         ues of λ2 resulting in faster convergence (Norris, 1997).
compositional languages means that any particular com-          Figure 3 (a) shows how λ2 is affected by α, ǫ, and m.
positional language is more probable than any particu-          As α brings p(h) away from uniformity, it increases the
lar holistic language. Consequently, it would be very           probability that learners n and n + 1 will share the same
unlikely to see a holistic language that just happened          hypothesis. Thus, increasing α increases λ2 . Changing
to produce a compositional mapping. As α becomes                ǫ and m decreases the rate of convergence as the data
                                                            829

                                                α = 0.5, ε = 0.05, m = 1                             Chain       Prior
   (a)                     C                                                                    C           C
         C
                           H                                                                    H           H
         H
              C     H                           α = 0.5, ε = 0.05, m = 3                          0 0.1 0.2    0 0.1 0.2
   (b)                     C                                                                    C           C
         C
                           H                                                                    H           H
         H
              C     H                           α = 0.5, ε = 0.05, m = 10                         0 0.1 0.2    0 0.1 0.2
   (c)                     C                                                                    C           C
         C
                           H                                                                    H           H
         H
              C     H                           α = 0.01, ε = 0.05, m = 3                         0 0.1 0.2    0 0.1 0.2
   (d)                     C                                                                    C           C
         C
                           H                                                                    H           H
         H
              C     H        0          200         400            600          800         1000 0      0.02 0      0.02
                                                         Iteration
Figure 2: Markov chains on hypotheses for the evolution of compositionality. Different rows correspond to different
parameter values. For each set of parameters, the first column shows a portion of the transition matrix, with
four compositional languages (C) and four holistic languages (H). Columns are hn , rows are hn+1 , and darker grey
indicates a higher value of p(hn+1 |hn ). The second column shows a sample of 1000 iterations from this matrix, the
third shows the relative frequency of hypotheses across 10000 iterations, and the fourth shows the prior, p(h).
received by each learner become more informative. De-           are more learnable. For example, Kirby (2001) explained
creasing ǫ increases the fidelity with which information        the emergence of compositionality in terms of the rela-
is transferred between generations, increasing the corre-       tive ease of transmitting a compositional language: lan-
spondence between the hypotheses of successive learn-           guages that contain more generalizations are more com-
ers and thus increasing λ2 . Increasing m increases the         pressible, and can thus be learned from smaller amounts
amount of information language production provides for          of data. In support of this claim, Smith et al. (2003)
language learning, and thus the probability that learner        showed that tightening the information bottleneck, by
n + 1 will acquire the language of learner n, increasing        reducing the amount of data a learner saw, increased
λ2 . With large m, it is likely that a single hypothesis        the advantage in stability for compositional languages
will be maintained across several generations. The effect       over holistic languages with their learning algorithms.
of large m is demonstrated in Figure 2 (c), where some             Figure 3 (b) shows how the relative stability of compo-
compositional hypotheses are dominant over hundreds of          sitional and holistic languages changes as a function of α,
iterations.                                                     ǫ, and m. The relative stability was assessed by comput-
                                                                ing the ratio of the mean probability that a particular
The information bottleneck                                      compositional language would appear as both hn and
In applications of the ILM, it is common to explain             hn+1 to the mean probability that a particular holistic
the emergence of languages with particular properties           language would appear as both hn and hn+1 . The fig-
in terms of the “information bottleneck” imposed by             ure shows that this stability ratio is strongly affected by
transmission of a language across generations. This bot-        α: if the prior probability of a compositional language
tleneck provides a selection pressure for languages which       is high, it is more likely that a learner will acquire that
                                                            830

                                 1
                                                                                                2
                                                                                               10                         α = 0.5, ε = 0.05
      (a)                                                                   (b)
                                                                                                                          α = 0.01, ε = 0.05
                                                                                                                          α = 0.5, ε = 0.001
       Second eigenvalue (λ2)
                                0.8                                                                                       α = 0.01, ε = 0.001
                                                                                                1
                                                                                               10
                                                                             Stability ratio
                                0.6
                                                                                                0
                                                                                               10
                                0.4
                                                                                                −1
                                0.2                                                            10
                                      0   2      4        6       8   10                             0   2      4        6       8       10
                                           Number of datapoints (m)                                       Number of datapoints (m)
Figure 3: Quantities derived from Markov chains on hypotheses as a function of number of datapoints, m, prior on
composite languages, α, and error rate, ǫ. (a) Second eigenvalue of transition matrix, λ2 . (b) Stability ratio. The
dotted line shows the stability ratio as m → ∞. Lower values of m constitute a tighter “information bottleneck”.
language, and consequently that language is more sta-                              Previous work on iterated learning
ble. The magnitude of this effect is modulated by the
                                                                                   We view our results as broadly consistent with previ-
number of datapoints, m, with α having the greatest ef-
                                                                                   ous work on iterated learning, but suggesting a differ-
fect when m is small. As m increases, the data begin to
                                                                                   ent approach to understanding its consequences. With
overcome the influence of the prior.3
                                                                                   Bayesian learners, iterated learning results in conver-
   The results shown in Figure 3(b) are evocative of those                         gence to the prior distribution over hypotheses. Conse-
of Smith et al. (2003): tightening the information bot-                            quently, the iterated learning process is only the engine
tleneck produces a greater advantage for compositional                             by which languages with particular properties emerge:
languages when each of those languages has higher prior                            the real object of analysis should be the assumptions
probability than any holistic language. The explanation                            behind the algorithms used by the learners. This con-
is the same: a tight bottleneck favors more learnable lan-                         clusion is quite different from that of previous work, in
guages. For a Bayesian, an important aspect of learn-                              which the emphasis is placed upon the learning process
ability is consistency with the prior.                                             itself as the source of linguistic universals.
                                                                                      Interpreting previous results in terms of our frame-
                                          Discussion                               work is not straightforward, as the learning algorithms
                                                                                   used in most previous analyses do not have a clean
Our results provide simple conditions for determining                              Bayesian interpretation. It is also not clear whether
when a particular property of languages will emerge                                these algorithms satisfy the requirements for the under-
through iterated Bayesian learning: languages will ap-                             lying Markov chain to be ergodic: in many cases, the
pear in proportion to their prior probability, provided                            criterion for ending simulations was reaching a steady
the Markov chain defined by the learners is ergodic. In                            state, which is not something that should happen with
closing, we will consider connections between these re-                            an irreducible Markov chain. To the extent that our
sults and previous work on iterated learning, how they                             results are applicable, they suggest that the algorithms
bear upon the issue of linguistic universals, and their im-                        that Kirby (2001), Brighton (2002), and Smith et al.
plications for understanding the diversity and dynamics                            (2003) used to demonstrate the emergence of compo-
of human languages.                                                                sitionality through iterated learning implicitly define a
                                                                                   prior distribution over hypotheses that favors composi-
   3
                                                                                   tional languages. Making these connections explicit is
     A small influence of the prior remains even at asymptote                      an important direction for future work.
due to the presence of holistic hypotheses that are equivalent
to compositional hypotheses. These hypotheses cannot be
separated by any amount of data, so the stability ratio ap-                        Prior probabilities and linguistic universals
            64
proaches 62+1/α   as m → ∞. If H did not include hypotheses                        By tying the probability of a language emerging through
that make equivalent predictions about the data, the stabil-                       iterated learning to its prior probability, our analysis lo-
ity ratio would approach 1 as m → ∞. Consequently, the
decrease in the stability ratio as a function of m for the cases                   cates the stability of languages firmly in the algorithm
where α = 0.01 is due to the specific structure of H, rather                       applied by the learners. The structure of languages has
than being a general trend.                                                        no effect on their stability, except insofar as it determines
                                                                           831

prior probability. From this perspective, linguistic uni-       in a state with very low probability under the stationary
versals simply manifest the prior probability distribution      distribution will rapidly move towards states with higher
over languages entertained by the learner. Explaining           probability. By studying how new languages develop, we
linguistic universals thus requires explaining why partic-      have the opportunity to map out languages with low and
ular properties of language have high prior probability.        high prior probability, and to estimate the rate at which
   The statement that linguistic universals result from         the Markov chain converges.
the priors of learners initially seems consistent with tra-
ditional explanations, with innate, language-specific, ge-                              Conclusion
netic endowment providing these priors. This need not           We have presented a novel mathematical framework for
be the case. The priors that a Bayesian agent brings to         exploring the consequences of iterated learning, based
a learning task reflect their general cognitive capacities,     upon the assumption that learners are rational Bayesian
and the expectations yielded by their experience with           agents. Making this assumption allows us to obtain pre-
all other independent sources of evidence. Furthermore,         cise results that characterize the circumstances under
priors can be motivated by a priori symmetry arguments          which iterated learning will produce languages with par-
– such as the belief that holistic and compositional lan-       ticular properties. These results have the potential to
guages should be equally likely – or information theoretic      provide connections between the formal and functional
constraints – such as the relative difficulty of encoding       approaches to explaining the existence of linguistic uni-
languages (c.f. Brighton, 2002). Either of these latter         versals, showing that the results of iterated learning –
considerations would be sufficient to explain the evolu-        a process that would seem to emphasize the functional
tion of compositionality. Framing the explanation of lin-       properties of languages – do not depend upon the struc-
guistic universals in terms of accounting for the prior         ture of the languages involved, except insofar as that
probability distribution entertained by language learn-         structure determines their prior probability. We antici-
ers provides a well-defined formal setting in which to          pate that this framework will prove useful in the further
explore these issues.                                           study of language evolution by iterated learning, as well
                                                                as in other settings. Iterated learning provides a natural
Language change and regularization                              model of cultural evolution, suggesting that our frame-
                                                                work could be applied to a range of cultural phenomena
If we take the idea that language evolves through iterated
                                                                other than language. Our results demonstrate that it-
Bayesian learning seriously, it has several interesting im-
                                                                erated learning also provides a means of assessing the
plications for understanding the diversity and dynam-
                                                                priors of learners, which could be exploited in a labora-
ics of language. First, if iterated learning is the only
                                                                tory setting as a means of determining learning biases.
force influencing language change, then all languages
used by human beings should be considered samples from                                  References
the prior probability distribution over languages. Even         Bickerton, D. (1981). Roots of language. Karoma, Ann Arbor,
though the assumption that no other selective pressures          MI.
are at work in language evolution – such as the relative        Brighton, H. (2002). Compositional syntax from cultural
communicative utility of speaking one language rather            transmission. Artificial Life, pages 25–54.
than another – is probably false, this result provides a        Chomsky, N. (1965). Aspects of the theory of syntax. MIT
direct connection between the mind and world that is             Press, Cambridge, MA.
provocative. It provides a formal justification for the         Comrie, B. (1981). Language universals and linguistic typol-
idea that examining the diversity of human languages             ogy. University of Chicago Press, Chicago.
has the potential to reveal interesting properties of the       Greenberg, J., editor (1963). Universals of language. MIT
human mind.                                                      Press, Cambridge, MA.
                                                                Hawkins, J., editor (1988). Explaining language universals.
   A second implication of this view of language evolution       Blackwell, Oxford.
is that language change should be viewed as a random            Kirby, S. (2001). Spontaneous evolution of linguistic struc-
walk through this prior probability distribution. Once           ture: An iterated learning model of the emergence of regu-
a Markov chain has converged, it will move through its           larity and irregularity. IEEE Journal of Evolutionary Com-
state space in a fashion determined by its dynamics, vis-        putation, 5:102–110.
iting each state with probability determined by the sta-        Manning, C. and Schütze, H. (1999). Foundations of statis-
tionary distribution. If a language is well-determined by        tical natural language processing. MIT Press, Cambridge,
                                                                 MA.
the evidence available to a learner (together with their
                                                                Newmeyer, F. J. (1998). Language form and language func-
prior probabilities), the rate at which this random walk         tion. MIT Press, Cambridge, MA.
moves between states should be very low, as in Figure           Norris, J. R. (1997). Markov Chains. Cambridge University
2 (c). However, examining the dynamics of language               Press, Cambridge, UK.
change can shed further light on the structure of the           Nowak, M. A., Plotkin, J. B., and Jansen, V. A. A. (2000).
prior, and the transition probabilities.                         The evolution of syntactic communication. Nature, 404:495–
   Finally, the development of new languages can be un-          498.
derstood in terms of convergence to this prior. Cases like      Smith, K., Kirby, S., and Brighton, H. (2003). Iterated learn-
                                                                 ing: A framework for the emergence of language. Artificial
creolization (e.g., Bickerton, 1981) are striking in terms       Life, 9:371–386.
of the sudden regularization introduced by a single gen-
eration of learners. Similarly, a Markov chain initialized
                                                            832

