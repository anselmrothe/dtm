UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Lexical Processing Drives Motor Simulation

Permalink
https://escholarship.org/uc/item/28t0v2fc

Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 27(27)

Authors
Bergen, Benjamin K.
Tseng, Meylysa J.

Publication Date
2005-01-01
Peer reviewed

eScholarship.org

Powered by the California Digital Library
University of California

Lexical Processing Drives Motor Simulation
Meylysa J. Tseng (meylysa@hawaii.edu)
Department of Linguistics, 569 Moore Hall, 1890 East West Road,
Honolulu, HI 96822 USA

Benjamin K. Bergen (bergen@hawaii.edu)
Department of Linguistics, 569 Moore Hall, 1890 East West Road,
Honolulu, HI 96822 USA

Abstract
While growing evidence suggests that sentence understanding
engages perceptual and motor systems for the purpose of
mentally imagining or simulating the content of utterances
(Barsalou 1999) it is not known whether processing words
alone does the same. We investigated whether making a
decision about the form of a word would lead to activation of
motor mechanisms, using a modified version of the Actionsentence Compatibility Effect (Glenberg and Kaschak 2002).
Fluent signers of American Sign Language (ASL) were
shown pairs of ASL signs which were either identical or not.
Critical signs involved hand motion forward or backward,
relative to the body. Subjects indicated whether the two signs
were the same or different with a manual response requiring
their hand to move either forward or backward - thus in a
direction either compatible or incompatible with the direction
of motion denoted by the sign. Results demonstrated a
compatibility effect - literal and metaphorical motion signs
facilitated response motion in the same direction, suggesting
that mere phonological processing of a lexical item with
motion meaning engages the motor system. The same
experiment, performed with non-signers yielded no such
effect, demonstrating that the effect was not simply the result
of perceptual processing of the form of the sign. These results
support an embodied view of linguistic processing where the
content of language about motor actions is simulated using
parts of the cognitive system responsible for actually
performing the described actions.
Keywords: mental simulation; American Sign Language;
motor control; lexical processing, Action-sentence
Compatibility Effect

Introduction
If one were charged with the duty of building a human
being from scratch, one would have to give it a motor
system with which to act upon the world. To facilitate
thinking about acting, one might configure this motor
system such that, when necessary, it could be run without
sending sufficient output to the relevant effectors for them
to actually move - thus allowing the same system used for
action execution to internally simulate acting for the
purpose of reasoning. Finally, if one wanted to bestow upon
one's creation the capacity to communicate about acting on
the world, a particularly parsimonious solution would be to
hook the communication system into the motor system. In
such a system, acts of communication would begin with the
2206

speaker internally recreating a motor experience, followed
by the selection and production of language appropriate to
evoke a similar experience in the mind of the understander.
Remarkably, the demands of effective communication
about action seem to be met in humans by this very solution
(Barsalou 1999, Zwaan 1999, Glenberg and Robertson
2000, Bergen et al. 2004, Bergen and Chang 2005).
Mounting evidence demonstrates that finely detailed
perceptual and motor processing is automatically and
unconsciously evoked during language processing
(Stanfield and Zwaan 2001, Glenberg and Kaschak 2002,
Zwaan et al. 2002, Richardson et al. 2003, Bergen et al.
2003, Bergen et al. 2004, Kaschak et al. 2004, and Matlock
To Appear). This constitutes broad evidence for the view
that language understanders make use of perceptual and
motor simulation to gain deep understanding of language,
even when performing tasks that do not directly ask them to
perform imagery.
The last decade has witnessed an explosion of evidence
demonstrating that the motor system is critically engaged
during behaviors that are related to motor control, but
involve no actual motion on the part of the relevant
individual. Much of this work has developed from the
discovery of “mirror neurons” in monkeys (Gallese et al.
1996, Rizzolatti et al. 1996) - neural circuits that become
active during either the execution or perception of specific
motor control events, like grasping and reaching. The work
has since been extended to humans, where neural imaging
and behavioral methods have demonstrated that different
overlapping regions in pre-motor cortex become active
when subjects execute and perceive actions produced by the
mouth, leg, and hand, respectively (Buccino et al. 2001),
and similarly that executing actions and recalling those
actions also activate common areas in motor and parietal
cortex (Nyberg et al. 2001).
In addition to execution, perception, and memory of
motor actions, recent work has begun to investigate whether
language about actions is processed using action-specific
areas of the motor system. Pulvermüller et al. (2001) and
Hauk (2004) found that when subjects performed a lexical
decision task, verbs associated with different effectors led to
different paths of processing in motor cortex. These findings
were further corroborated by Tettamanti et al. (m.s.) who
showed that passive listening to sentences describing
mouth, leg and hand motions also activated different parts
of pre-motor cortex, along with BA 6, BA 40 and BA 44.

Two principal lines of behavioral investigation have also
demonstrated a role for the motor system in language
processing. The first (Glenberg and Kaschak 2002) has
demonstrated that processing sentences denoting motion
involves activation of motor programs for hand action in the
same direction as the described motion. The second
(Bergen and Wheeler 2005) has demonstrated that
processing sentences which verbs of hand motion involves
activation of motor programs for hand shape.
In Glenberg and Kaschak (2002), it was demonstrated
that a sentence sensibility task could evoke motor imagery
strong enough to prime actual motor action. In this
paradigm, participants judge the acceptability of sentences
describing movement toward or away from the body (i.e.
“Put your finger under your nose” vs. “Put your finger
under the faucet”). To respond ‘yes’ (sensible) or ‘no’ (notsensible), they are asked to press a button that is either
further away or closer to their body. The critical condition
was measured by comparing the reaction times to the
‘yes’-is-near and ‘yes’-is-far responses. Glenberg and
Kaschak found that participants were slower to respond
when the direction of the action in the sentence was
opposite to the direction their hand had to move in order to
press the ‘yes’ button. They named this the Action-Sentence
Compatibility Effect, or ACE (Glenberg and Kaschak
2002).
In subsequent work, Bergen and Wheeler (2005) have
similarly found that language describing physical actions
made using specific hand shapes, like a fist versus an open
palm, facilitates performing the same action. In their study,
subjects again judge the sensibility of sentences, though the
critical condition is found in the fine motor detail implied in
the verbs, such as ‘punch’ (clenched fist) and ‘pat’ (open
palm). To investigate their hand shape compatibility effect,
subjects responded ‘sensible’ versus ‘not sensible’ by using
a fist or an open palm to hit the large response button.
Bergen and Wheeler found that participants were faster to
respond ‘sensible’ when the hand shape of the action in the
sentence was the same as the hand shape they had to make
when pressing the response button. Thus, Bergen and
Wheeler (2005) show that not only hand motion, but also
fine details of hand shape are also encoded in our language
understanding.
This speeding up of actual motor action by previously
processed
language
about
similar
actions
is
straightforwardly interpreted as indicating that during
sentence processing, language understanders automatically
and unconsciously activate motor representations
corresponding to the described actions. Thus, the current
state of knowledge about motor activity in language
processing is that words about actions interfere with
processing images of similar actions, and at the level of the
sentence, processing language about actions appears to
critically engage the motor system. The next logical step in
the investigation of motor imagery in language
comprehension is thus to apply the methods that have
previously demonstrated a role for the motor system in
sentence processing to the study of lexical processing.
To this end, we begin in a language rich in motor activity,
American Sign Language (ASL), because the pervasive

iconicity of signs expressing motor events, as compared
with spoken languages, could increase the likelihood that
lexical access drives motor imagery when understanding
language.
Signed languages are profoundly iconic,
especially when it comes to representations of space (Taub
2001). In spoken languages, movements of the tongue and
other parts of the vocal tract constitute the speech gestures.
These phonological movements generally do not iconically
represent spatial aspects of events denoted by the words
they occur in, though the sounds produced by these gestures
may be acoustically iconic through sound symbolism or
onomatopoeia (Ohala 1997). Signed languages, on the other
hand, use ‘phonological’ movement consisting of hand, arm
and upper body movements, all of which can be used to
iconically represent motion events. Thus, if there are any
languages in which lexical access directly drives motor
imagery, a spatially iconic one like ASL is likely to be one.
To investigate the extent of motor imagery in lexical
meaning, we adapted the Action-Sentence Compatibility
Effect (ACE) methodology (Glenberg and Kaschak 2002) to
suit word processing. Recall that the ACE methodology
presents subjects with sentences to read and make a
meaningfulness judgment on. In order to explore whether
simply accessing a lexical item also yields motor imagery,
the design was modified somewhat. Rather than direct
subjects' attention to meaning, we instead had subjects
perform a lexical matching task, in which they saw two
signs in sequence and were asked to simply judge as quickly
as possible whether they were the same sign or not. With
this design, any motor interference would arise not from
task-imposed interpretation of sign meaning, or from the
construction of an interpretation for a whole sentence, but
rather would be exclusively an automatic product of the
lexical item presented.

Method
Following Glenberg and Kaschak (2002), signs fell into one
of two conditions - those denoting motion forward (i.e.
away from the body) and those denoting motion backwards
(i.e. towards the body). The subjects were to respond
affirmatively or negatively, by performing a hand
movement that was either in the same or the opposite
direction as the denoted movement. If subjects display
motor priming effects even when performing this task,
which could be successfully performed solely through
visual perceptual means since the two signs in the matching
condition were identical, then this would indicate the
automaticity of the motor imagery mechanism.
The signs denoting forward motion and those denoting
backwards motion were split into two groups. The first
denoted literal motion of the arm and hand, like CATCH
and BOWLING (both of which look iconically like the
denoted actions), and will be referred to as semantic or sem
signs. The second set denoted metaphorical motion, like
TELL (in which the index finger moves forwards from
under the chin to indicate transfer of information in the
form of language) or YESTERDAY (in which the hand
moves backwards to indicate the past), and will be
identified as metaphorical or met signs. Using signs with
forwards or backwards semantics, however, is complicated

2207

by the pervasiveness of spatial iconicity in ASL. Nearly all
forward signs, like BOWLING and GIVE and backward
signs like CATCH and EAT are phonologically encoded
using physical motion of the arm and hand in the
corresponding direction. As a result, any motor priming
effects observed could potentially result from the processing
of the phonology of the signs, and not their semantics.
To deal with this possibility, we included a final set of
signs that were phonologically directional but whose
semantics was totally unrelated - even through metaphor to that same motion. An example is the sign for GIRL, in
which the thumb moves forward away from the face, but its
meaning does not reflect this directional element in any
way. Other examples include ABUNDANT (forward),
HOME (backward) and MISTAKE (backward). These signs
will be referred to as phonological or phon signs. If an
observed compatibility effect obtains for the sem and met
signs as well as for the phonological signs, then this would
confirm the possibility that it is the phonology and not the
semantics that is responsible for the effect. However, if the
effect is present for signs encoding motion but not for those
that only had ‘phonological’ motion, then this would
indicate that the effect results from semantic access.

Subjects
Two groups of subjects participated in the same experiment.
The first group consisted of forty-six ASL signers. The
second group consisted of forty-two non-signers. The
signers were residents of Oahu and were compensated with
ten dollars. The non-signers were students at the University
of Hawai’i at Mnoa and received credit in an
undergraduate linguistics course for their participation.

Materials and Design
Subjects were tested on sixty-six different signs, consisting
of twenty-two phon signs, twenty-two met signs and
twenty-two sem signs. Half of each of these three groups of
signs were phonologically encoded with hand movement
forward, away from the body and half had the hands move
backward, toward the body. The major axis of motion for all
signs was forward or backward, though some were angled
slightly to the right or left. (We chose forward and
backward motion as opposed to rightward and leftward
motion because the direction of the latter set depends on the
signer's handedness.) These signs were obtained with
permission from the MSU American Sign Language
Browser
(http://commtechlab.msu.edu/sites/aslweb/).
During the experiment, signs consisted of a movie
composed of four frames, which was found to suffice to
clearly display each unidirectional sign.
There were two lists, identical except for the order of
their two blocks. In addition, each list had two variations. In
the first variation of each list, the first block had the ‘same’
button in a location that required movement forward, and
the second block had the ‘same’ button in a location that
required backward movement. In the second variation of
each list the location of the ‘same’ button was reversed.
Thus, the first block had the ‘same’ button in a location that

required backward movement, and the second block had the
‘same’ button in a location that required forward movement.
Thus, there were a total of four versions of the
experiment. Each subject was randomly assigned to one of
these versions. The task required subjects to determine
whether two ASL signs, which they saw in sequence, were
the same sign or different signs. Subjects first saw a fixation
cross. At this point they pressed and held down the ‘go’
button, the ‘h’ key on an attachable keyboard that was
rotated 90-degrees clockwise from normal so that the long
axis extended outwards from the subject. After pressing the
'go' button the subject saw, in sequence, a blank screen
(100msec), the first sign (first frame 150msec, second frame
100msec, third frame 100msec, fourth frame 100msec),
followed by a visual mask (1000msec), then the second sign
(first frame 150msec, second frame 100msec, third frame
100msec). Subjects did not release the ‘go’ key until they
were ready to respond. The last frame of the second sign
remained on the screen until the subjects lifted their finger
off the ‘go’ button to hit the ‘same’ key or the ‘different’ key
(either the ‘a’ key or the ‘’’ key, depending on the
condition). The keyboard was positioned such that hitting
the ‘a’ key would require moving the arm forward, whereas
hitting the ‘’’ key would require moving backward. The
‘same’ and ‘different’ keys were switched halfway through
the experiment such that subjects would have a chance to
respond in both directions. For the critical signs, the answer
was always ‘same’.
Thus, independent variables were the direction of the
hand movement - forward or backward (sign direction),
whether the signs had semantic direction of movement or
not (sign type) and whether the response was a forward or
backward movement of the hand (hand motion). The
dependent variables were time to let go of the ‘go’ key
(release time) and time to press the ‘same’ button (response
time).
For the nonmatching sign pairs, phon signs were
randomly matched with met and sem signs which moved in
the same direction, met signs were matched with phon and
sem signs and so on. The reason nonmatching sign pairs
encoded movement in the same direction was to ensure that
subjects couldn't distinguish matching from nonmatching
signs simply on the basis of directional cues.
These twelve groups (6 groups matching and 6
nonmatching) were randomized within their groups, then
halved and split into two lists. Each list was then split into
two blocks, one appearing first and the other appearing last
in the first version and vice versa in the second version.
This gave a total of 66 sign pairs (33 matching and 33
nonmatching) for the first block and 66 sign pairs for the
second, leading to each list, and thus, each version,
containing a total of 132 sign pairs. Thus, each subject saw
each phon, met and sem sign appear as the first sign in a
pair only twice in the experiment, once in a matching sign
pair and once in a nonmatching sign pair. All signs within
blocks were presented in random order.

Procedure
The experimental procedure consisted of a practice session,
which used six signs which did not encode direction

2208

forward away from or backward toward the body, each
presented in a matching and nonmatching condition, as
described above for the main session. During the first half
of the practice session, subjects were given feedback as to
whether they had correctly judged whether the signs were
the same or different. At the end of the practice, subjects
were informed their score. If they scored better than 80%
they were allowed to start the main session; otherwise the
practice session was repeated.1
After the first half of the main session, subjects were
given their accuracy score, and then the screen froze and the
labels on the ‘a’ and ‘’’ buttons were switched. Subjects
were given a second practice session with the new
orientation of ‘same’ and ‘different’ buttons, followed by
the second half of the main experiment.
Subjects in both groups were explicitly told not to lift
their finger off of the ‘go’ button until they were ready to
answer. The experiment took twenty to thirty minutes to
complete.

However, we did find the predicted effect on response
time, the time it took subjects to press the ‘same’ button.
Response times were analyzed with a 3 (sign type) x 2 (sign
dir) x 2 (hand motion) repeated measures ANOVA. The
results for subjects show no significant main effects. In
terms of two way effects, the interaction between sign
direction and hand motion (the ACE) was found to be
significant, F1(1,26) = 4.12, p < 0.05. The three way
interaction was not significant.
In order to investigate whether signs that denoted motion
differed from those that did not, we performed separate 2
(sign dir) x 2 (hand motion) repeated measures ANOVAs for
the combined group of met and sem signs on the one hand
and a separate one for phon signs on the other. The results
showed that the ACE (interaction of sign direction and hand
motion) was significant for sem and met signs combined for
subjects, F1(1,26) = 9.29, p < 0.01, and for items, F2(1,40) =
4.07, p = 0.05. The ACE for phon signs, by contrast, was
not found to be significant, F1(1,26) = 0.53, p = 0.48. This
is illustrated in Figure 1.

Results

1

As pointed out by a reader, this may have been a confounding
factor, since some subjects had more practice time than others.

Phonological signs

Sem and Met signs
490

Reaction time

490
480

480
470

470
460
450

Sign
Direction
Backward

460
450
440
430

440
430
420

Sign
Direction
Forward

420
410

410
400

400
Backward

Forward

Response Direction

Backward

Forward

Response Direction

Figure 1: ASL signers response RTs for sem and met signs
(significant interaction) versus phon signs (no interaction)
Phonological signs

Sem and Met signs

Reaction time

Nineteen ASL signers were eliminated since they were left
handed, had less than five years of ASL experience, had
reaction times (RTs) over three standard deviations from the
mean or performed at less than 80% accuracy (either on
their own account or because of instrument failure). Of the
remaining twenty-seven signers, seventeen were women
and ten were men, with an age range from 19 to 68 years of
age. Sign pairs were eliminated for being over three
standard deviations above the mean response RT. These
were AFFECTION/EMBRACE, BINARY and GOODLUCK.
In the group of nonsigners, three subjects were eliminated
due to either computer malfunction or time constraints on
the subject’s schedule, requiring early termination of the
program. All subjects’ RTs were within three standard
deviations from the mean. Of the remaining thirty-nine
subjects, twenty-one were women and eighteen were men,
with an age range from 18 to 24 years old. Items found to
have average RTs three standard deviations above the mean
(CHOKE and LEISURE) were eliminated from the analysis.
For both groups of subjects, all item RTs per subject over
three standard deviations from the mean for that subject
were replaced by the value three standard deviations above
the mean, resulting in changes in less than 1% of the data.
Glenberg and Kaschak’s original work on the ACE found
significant compatibility effects on release times. The
release RTs for signers were analyzed with a 3 (sign type) x
2 (sign dir) x 2 (hand motion) repeated measures ANOVA.
The results show a main effect on release time of sign
direction, F1(1,26) = 18.0, p < 0.01, with backward signs
having longer release RTs. Unlike Glenberg and Kaschak,
our data showed no significant ACE (sign dir x hand
motion) for release RTs, F1(1,26) = 0, p = 0.99, and no
significant three way effect (sign type x sign dir x hand
motion), F1(2,52) = 1.92, p = 0.16.

390
380
370
360
350
340
330
320
310
300

390
380
370
360
350
340
330
320
310
300
Backward

Forward

Response Direction

Sign
Direction
Backward
Sign
Direction
Forward

Backward

Forward

Response Direction

Figure 2: Response RTs for non-signers show no ACE.
These results show a significant interaction effect
between sign direction and response direction (the ACE),
which is apparently specific to signs that denote literal or
metaphorical motion, but is not shown by signs that only
phonologically encode motion without any motion
semantics. To determine whether these findings might result
from other phonological properties of the signs themselves,
or whether they are in fact due to activation of motor
systems during word processing by fluent signers, we

2209

investigated whether the same ACE would show up in
subjects who had no experience with ASL.
The results of a 3 (sign type) x 2 (sign dir) x 2 (hand
motion) repeated measures ANOVA of response times
showed no significant main effects or interaction effects.
However, a four-way repeated-measures ANOVA in-cluding
language - signer or non-signer - as an independent variable
demonstrated a significant effect of language; signers
responded more slowly than non-signers F1(1,64) = 12.77, p
< 0.01. The results for non-signers are illustrated in Figure
2.

Discussion
Signers took longer to press a button indicating that two
signs were identical when the action they had to perform
was incompatible with the action described by the sign. This
effect appears to hold for signs denoting literal or
metaphorical motion (sem and met signs), but not for signs
with no motion semantics (phon signs), which eliminates
the possibility that the effect results simply from the visual
perception of the phonological motion encoded in the signs
themselves. Further evidence that it is semantic processing,
rather than visual processing that yields this compatibility
effect comes from the absence of an effect in nonsigners. If
it were just observing an action that was priming
performing a similar action in this task, nonsigners should
have displayed a significant interaction between sign
direction and response direction, which they did not.
This observed ACE in ASL within a sign matching task
constitutes two important advances in the study of how
motor systems are engaged by language processing. First, it
provides evidence that sentence sensibility tasks, which
require subjects to pay close attention to sentential meaning,
aren’t required for the motor system to be engaged in
language processing. Even in a lexical matching task, where
subjects can simply look at the form of the two signs to
determine whether they are identical or not, there is
nonetheless motor activity, as evidenced by the priming of
similar physical actions. This speaks to the automaticity of
motor imagery, as it occurs even when superficial visual
processing would suffice.
Second, the fact that this effect was observed in a lexical
matching task, and not one that uses sentences as stimuli
argues for a role for motor activation in lexical processing,
not just sentence processing. This supports the findings
from previous work on lexical semantics (Bergen et al.
2004) arguing that the meanings of action words are at least
in part calculated on the basis of activation of motor
systems.
In terms of the particular method used, it seems that
alongside an Action-Sentence Compatibility Effect there is
an Action-Word Compatibility Effect, at least in ASL. A
notable difference between the two effects lies in the fact
that in Glenberg and Kaschak’s (2002) study, the ACE was
demonstrated in release times, whereas in the current study
it was in evidence in response times. One explanation could
lie in the difference in the two tasks. Since Glenberg and
Kashack's study involved sentence sensibility judgments,
subjects had to perform phonological and relatively deep
semantic processing before they could lift their finger. Thus,

in a sentence sensibility task, the ACE could already be
detected in the time to release the button. In our task
however, where subjects had to decide if two signs were the
same or not, subjects responded much more quickly. It
could be that the semantic processing performed on the
signs in our experiment occurred after the release, and thus
appears in the response times (see Vitevitch and Luce 1999
for discussion of the sublexical and lexical stages of word
recognition; Pylkkanen and Marantz 2003 discuss different
stages in brain activations during word recognition). The
cognitive load of semantic processing could also explain the
relatively shorter response RTs for nonsigners, who did not
perform semantic processing, than signers.
One tangential finding of potential interest was the longer
release RTs for backwards signs. Several possible
explanations present themselves. First, backwards signs
have their critical location information encoded at the end
of the sign, where the movie stops. This could require
longer processing time in sign recognition tasks. In addition,
some of the backwards signs cover shorter distances
compared to forward signs. This leads to less spatial
information, and quicker signs, which also may slow sign
comprehension. It could be that these factors also lead to the
apparently smaller number of backward signs in the lexicon.

Conclusion
This investigation of lexical processing in ASL has provided
evidence supporting an embodied view of language
understanding. In a sign matching task, it was shown that
motor action was affected by the lexical semantics of signs
in citation form. It has been suggested by a number of
authors that simulated motor activity is the actual
mechanism by which language about action is understood
(Barsalou 1999, Zwaan 1999, Glenberg and Robertson
2000, Bergen and Chang 2005). In accordance with this
proposal, ASL signers appeared to mentally access
meanings of literal and metaphorical signs using motor
imagery that relies on the same neural structures responsible
for actual movement. The results seen here for automatic
activation of motor imagery during the processing of words
denoting motor actions supports this embodied view of
word meaning and highlights the importance of low-level
body and brain systems in language use.
The impact of this research lies in its implications for the
importance of automated motor programs in cognitive
development. Language tasks in particular and perhaps
reasoning tasks in general all require activation of motor
systems for understanding. In addition, this motor
understanding isn’t only limited to language about physical
motion events, as can be demonstrated in work by Lakoff
(1987) which shows that our conception of abstract
concepts often depends on motor-based metaphors (i.e.
‘Time passes quickly’). To this extent, recent findings on the
pervasive effects of both visual and motor imagery lead us
to conclude that our conceived realities, specifically as
expressed through language, are inseparable from our
bodily experiences, giving us a neurological explanation for
our uniquely human perception of the world.

2210

Acknowledgments
Thanks for advice from Scott Liddell, James Myers, Amy
Schafer, Victoria Anderson, Jan Fried, Brenda Nicodemus,
James MacFarlane, Nathaniel Smith, Kathryn Wheeler, Avis
Chan, Diana Stojanov, Gerald Bullock and Alfie Lumabas.
Also, thanks to the Communication Technology Laboratory
at MSU for the use of signs from their ASL browser.

References
Barsalou, L. W. (1999). Perceptual symbol systems.
Behavioral and Brain Sciences 22: 577-609.
Bergen, B. and Chang, N. (2005). Embodied construction
grammar in simulation-based language understanding. In
J-O. Östman and M. Fried (Eds.) Construction
Grammar): Cognitive Grounding and Theoretical
Extensions. John Benjamins.
Bergen, B., Chang, N. and Narayan, S. (2004). Simulated
action in an embodied construction grammar.
Proceedings of the Twenty-Sixth Annual Conference of
the Cognitive Science Society.
Bergen, B., Narayan, S. and Feldman, J. (2003). Embodied
verbal semantics: evidence from an image-verb matching
task. Proceedings of the Twenty-Fifth Annual Conference
of the Cognitive Science Society.
Bergen, B. and K. Wheeler. (2005). Sentence understanding
engages motor processes. Proceedings of the TwentySeventh Annual Conference of the Cognitive Science
Society.
Buccino, G., Binkofski, F., Fink, G. R., Fadiga, L., Fogassi,
L., Gallese, V., Seitz, R. J., Zilles, K., Rizzolatti, G. and
Freund, H. J. (2001). Action observation activates premotor and parietal areas in a somatotopic manner: an
fMRI study. European Journal of Neuroscience 13(2):
400-404.
Gallese, V., Fadiga, L., Fogassi, L. and Rizzolatti, G.
(1996). Action recognition in the premotor cortex. Brain
119: 593-609.
Gallese, V. and Lakoff, G. (2005). The brain’s concepts: The
role of the sensory-motor system in reason and language.
Cognitive Neuropsychology.
Glenberg, A. M. (1997). What memory is for. Behavioral
and Brain Sciences 20: 1-55.
Glenberg, A. M. and Kaschak, M. P. (2002). Grounding
language in action. Psychonomic Bulletin and Review.
Glenberg, A. M. and Robertson, D. A. (1999). Indexical
understanding of instructions. Discourse Processes 28: 126.
Glenberg, A. M. and Robertson, D. A. (2000). Symbol
grounding and meaning: A comparison of highdimensional and embodied theories of meaning. Journal
of Memory and Language 43: 379-401.
Hauk, O., Johnsrude, I. and Pulvermüller, F. (2004).
Somatotopic representation of action words in human
motor and premotor cortex. Neuron 41(2): 301-7.
Lakoff, G. (1987). Women, fire and dangerous things: What
categories reveal about the mind. Chicago: University of
Chicago Press.

Langacker, R. (1987). Foundations of cognitive grammar:
Theoretical Prerequisites. Stanford, CA: Stanford
University Press.
Kaschak, M. P, Madden, C. J., Therriault, D. J., Yaxley, R.
H., Aveyard, M., Blanchard, A. and Zwaan, R. A. (2004).
Perception of motion affects language processing.
Manuscript. Florida State University.
Matlock, T. to appear. Fictive motion as cognitive
simulation. Memory and Cognition.
May, R. (1985). Logical Form. Cambridge: MIT Press.
Nyberg, L., Petersson, K. M., Nilsson, L. G., Sandblom, J.,
Åberg, C. and Ingvar, M. (2001). Reactivation of motor
brain areas during explicit memory for actions.
NeuroImage 14: 521-528.
Ohala, J. J. (1997). Sound symbolism. Proceedings, Seoul
International Conference on Linguistics. Seoul: Linguistic
Society of Korea: 98-103.
Pulvermüller, F., Haerle, M. and Hummel, F. (2001).
Walking or talking?: Behavioral and neurophysiological
correlates of action verb processing. Brain and Language
78: 143-168.
Pylkkanen, L. and Marantz, A. (2003). Tracking the time
course of word recognition with MEG. Trends in
Cognitive Science 7 (5): 187-189.
Richardson, D. C., Spivey, M. J., McRae, K. and Barsalou,
L. W. (2003). Spatial representations activated during
real-time comprehension of verbs. Cognitive Science.
Rizzolatti, G., Fadiga, L., Gallese, V. and Fogassi, L.
(1996). Premotor cortex and the recognition of motor
actions. Cognitive Brain Research 3: 131-141.
Stanfield, R. A. and Zwaan, R. A. (2001). The effect of
implied orientation derived from verbal context on picture
recognition. Psychological Science 12 (2): 153-156.
Taub, S. F. (2001). Language from the body: Iconicity and
metaphor in American Sign Language. Cambridge:
Cambridge University Press.
Talmy, L. (2000). Toward a cognitive semantics.
Cambridge, MA: Massachusetts Institute of Technology.
Volumes 1, 2.
Vitevitch, M. S. and Luce, P. A. (1999). Probabilistic
phonotactics and neighborhood activation in spoken word
recognition. Journal of Memory and Language 40: 374408.
Wheeler, M. E., Petersen, S. E. and Buckner, R. L. (2000).
Memory’s echo: Vivid remembering reactivates sensory
specific cortex. Proc. Natl. Acad. Sci. USA 97: 11125–
11129.
Zwaan, R. A. (1999). Embodied cognition, perceptual
symbols, and situation models. Discourse Processes, 28:
81-88.
Zwaan, R. A., Stanfield, R. A. and Yaxley, R. H. (2002). Do
language comprehenders routinely represent the shapes of
objects? Psychological Science 13: 168-171.

2211

