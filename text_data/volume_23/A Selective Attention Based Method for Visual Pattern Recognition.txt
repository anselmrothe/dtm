UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
A Selective Attention Based Method for Visual Pattern Recognition
Permalink
https://escholarship.org/uc/item/43r4j75m
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 23(23)
Authors
Salah, Albert Ali
Aplaydin, Ethem
Akarun, Lale
Publication Date
2001-01-01
Peer reviewed
 eScholarship.org                              Powered by the California Digital Library
                                                                 University of California

         A Selective Attention Based Method for Visual Pattern Recognition
                                         Albert Ali Salah (SALAH@Boun.Edu.Tr)
                                      Ethem Alpaydın (ALPAYDIN@Boun.Edu.Tr)
                                          Lale Akarun (AKARUN@Boun.Edu.Tr)
                                  Department of Computer Engineering; Boğaziçi University,
                                                  80815 Bebek Istanbul, Turkey
                            Abstract                              The ‘where’ pathway, or the dorsal pathway, goes into
                                                                  the posterior parietal areas (PP) (Ungerleider & Mishkin,
   Parallel pattern recognition requires great computational      1982). The ventral pathway is crucial for recognition and
   resources. It is desirable from an engineering point of
   view to achieve good performance with limited resources.       identification of objects, whereas the dorsal pathway me-
   For this purpose, we develop a serial model for visual pat-    diates the location of those objects. We should note that
   tern recognition based on the primate selective attention      although recent findings point towards a distinction be-
   mechanism. The idea in selective attention is that not all     tween perception and guidance of action (Crick & Koch,
   parts of an image give us information. If we can attend to     1990) instead of a distinction between different types of
   only the relevant parts, we can recognize the image more
   quickly and using less resources. We simulate the primi-       perception, the issue is not resolved in favour of a spe-
   tive, bottom-up attentive level of the human visual system     cific theory (Milner & Goodale, 1995).
   with a saliency scheme, and the more complex, top-down,           The serial recognition process gathers two types of
   temporally sequential associative level with observable        information from the image: The contents of the fovea
   Markov models. In between, there is an artificial neu-
   ral network that analyses image parts and generates pos-       window, and the location to which the fovea is directed.
   terior probabilities as observations to the Markov model.      We call these ‘what’ and ‘where’ information, respec-
   We test our model on a well-studied handwritten numeral        tively (Ungerleider & Mishkin, 1982). The object is thus
   recognition problem, and show how various performance          represented as a temporal sequence, where at each time
   related factors can be manipulated. Our results indicate
   the promise of this approach in complicated vision appli-      step, the content of the fovea window and the fovea po-
   cations.                                                       sition are observed.
                                                                     Recurrent multi-layer perceptrons were used to simul-
                                                                  taneously learn both the fovea features and the class
                        Introduction                              sequences (Alpaydın, 1996). Other techniques are ex-
Primates solve the problem of visual object recogni-              plored in the literature to apply the idea of selective at-
tion and scene analysis in a serial fashion with scan-            tention to classification and analysis tasks (Itti, Koch,
paths (Noton & Stark, 1971), which is slower but less             Niebur, 1998; Rimey & Brown, 1990). Our approach
costly than parallel recognition (Tsotsos, Culhane, Wai,          is to combine a feature integration scheme (Treisman &
Lai, Davis, Nuflo, 1995). The idea in selective attention         Gelade, 1980) with a Markov model (Rimey & Brown,
is that not all parts of an image give us information and         1990).
analysing only the relevant parts of the image in detail is          We use handwritten numeral recognition to test our
sufficient for recognition and classification.                    scheme. In our database (UCI Machine Learning Repos-
   The biological structure of the eye is such that a high-       itory, Optdigits Database), there are ten classes (numer-
resolution fovea and its low-resolution periphery pro-            als from zero to nine) with 1934 training, 946 writer-
vide data for recognition purposes. The fovea is not              dependent cross-validation, 943 writer-dependent and
static, but is moved around the visual field in saccades.         1797 writer-independent test cases. Each sample is a
These sharp, directed movements of the fovea are not              32 32 binary image which is normalized to fit the
random. The periphery provides low-resolution informa-            bounding box. There are parallel architectures to solve
tion, which is processed to reveal salient points as targets      this problem in the literature (Le Cun, Boser, Denker,
for the fovea (Koch & Ullman, 1985), and those are in-            Henderson, Howard, Hubbard, Jackel, 1989), and they
spected with the fovea. The eye movements are a part of           have good performance, but our aim is to design a scal-
overt attention, as opposed to covert attention which is          able system which is applicable to problems where the
the process of moving an attentional ‘spotlight’ around           input data is high-dimensional (e.g. face recognition), or
the perceived image without moving the eye.                       not of fixed size (e.g. recognizing words in cursive hand-
   In the primate brain, information from the retina is           writing). Implementing a parallel scheme with good per-
routed through the lateral geniculate nucleus (LGN) to            formance is not trivial in such cases.
the visual area V1 in the occipital lobe. The ‘what’ path-           This paper is organized as follows: We first describe
way, also known as the ventral pathway for anatomical             our model and its three levels. Then we report our simu-
reasons, goes through V4 and inferotemporal cortex (IT).          lation results. In the last section we summarize and indi-

cate future directions.                                          The saliency map indicates the interesting spots on the
                                                              image. We simulate the fovea by moving a 4 4 window
                         The Model                            over the 12 12 downsampled image. The saliency val-
                                                              ues of the visited spots and their periphery are decreased,
                                                              and these spots are not visited again. This process has a
                                                              biological counterpart: Once neurons attuned to detect
                                                              a specific feature fire in the brain, they are temporarily
                                                              inhibited. Subsequently, subjects respond slower to pre-
                                                              viously cued locations (Klein, 2000).
                                                              Intermediate Level
                                                              The simulation of shifts of attention should provide us
                                                              with ‘what’ and ‘where’ information, but we want them
                                                              to be sufficiently quantized to be used in the associative
                                                              level. We divide the image space into uniform regions,
                                                              in effect, performing a quantization on the location infor-
                                                              mation. We use a second set of overlappping windows
                                                              to reduce the effect of window boundaries, as shown in
                                                              Fig. 2. We obtain a time-ordered sequence of visited re-
                                                              gions after the simulation of shifts. This constitutes the
                                                              ‘where’ stream for the particular sample (Fig 3).
Figure 1: The selective attention model for visual recog-
nition.
   The block diagram of the system we propose is given
in Fig. 1. It is composed of the attentive level that decides
on where to look, the intermediate level that analyses the
content of the fovea, and the associative level that inte-
grates the information in time.
                                                                                            a
Attentive Level
In the first step of the model, the bottom-up part of the vi-
sual system is simulated. We work on 12 12 downsam-
pled images to simulate a low-resolution resource. This
slightly decreases the classification accuracy, but speeds
up the computation considerably. Convolving the digit
image with 3 3 line orientation kernels produces four
line orientation maps in 0 , 45 , 90 and 135 angles.
                             Æ      Æ    Æ          Æ
These are combined in a saliency master map, which in-
dicates the presence of aligned lines on the image.
   Line orientations are detected by different primitive
mechanisms in the visual cortex, operating in coarse, in-
termediate and fine scales (Foster & Westland, 1998).                                       b
We can also talk about simple, complex and hypercom-
plex cell structures in the visual cortex, that deal with
increasing levels of complexity and decreasing levels of      Figure 2: Regions of the downsampled image. (a) The
resolution. In constructing the saliency map, we use the      uniform regions. (b) The additional, overlapping regions.
simplest set of features to decrease the computational        Notice how the corner at the intersection of 5 th , 6th , 8th ,
cost. Our experiments showed that adding other feature        and 9th regions are missed in those regions, but captured
detectors like corner maps, Canny edge detector, and fur-     clearly in the 13th region.
ther line orientation maps in higher resolutions increased
the classification accuracy only slightly, whereas the in-       As fovea contents, we extract 64-dimensional real-
crease in the computational cost was significant.             valued vectors. These vectors are produced by concate-

                                                          pert in the final model.
                                                          Associative Level
                                                          In the associative level, the two types of quantized infor-
                                                          mation are combined with a discrete, observable Markov
                                                          model (OMM) (Rabiner 1989). We treat the regions vis-
                                                          ited by the fovea as the states of a Markov model, and
                                                          the quantized output of the local artificial neural network
                                                          experts as the observations from each state. We simulate
                                                          eight shifts for each sample in the training set, obtain the
                                                          ‘where’ and ‘what’ streams, and adjust the probabilities
                                                          of the single Markov chain of the corresponding class to
                                                          maximize the likelihood of the training data. Training an
                                                          observable Markov model is much faster than training a
                                                          Hidden Markov Model.
                                                             In the observable model, the model parameters are
                                                          directly observed from the data. Since we know the
                                                          states, we can count the state transitions, and normal-
                                                          ize the count to find the state transition probabilities a i j ,
                                                          as well as the initial state distribution probabilities π i .
                                                          Similarly, we count the occurrences of the observation
                                                          symbols (quantized outputs of the local neural networks)
                                                          at each state, and normalize them to find the observation
                                                          symbol probability distribution b j k :
                                                                                # of times in Si at time t 1
                                                                       πi                                             (1)
                                                                                 # of observation sequences
Figure 3: The saliency master maps of two examples                              # of transitions from S i to S j
from the Optdigits database. Locations with high inten-                ai j                                           (2)
                                                                                    # of transitions from S i
sity indicate high saliency values. The locations visited
by the fovea are connected with a line, and enumerated                          # of times in S j observing v k
                                                                     bj k                                             (3)
in the order they are visited.                                                              # of times in S j
                                                             Finding the probability of the observation sequence is
                                                          much simpler in the observable Markov model, since the
nating the corresponding 4 4 windows on the line maps.
                                                          states are visible. We just multiply the corresponding
We prefer using the concatenated line maps to inspecting
                                                          state transition probabilities and the observation proba-
the original bitmap image, because the line maps indi-
                                                          bilities:
cate the presence of features more precisely. Further-
more, since they were constructed in the attentive level,                                          n
they come at no additional cost. In any case, we need a             P O Sλ       π S 1 b S 1 v1  ∏ a Si 1 Si b S i vi (4)
                                                                                                 i 2
vector quantization on the fovea contents before passing
them to the associative level.                            where S is the state sequence, O is the observation se-
   In order to efficiently quantize this information, we  quence, and λ      π i ai j b j k stands for the parameters
train artificial neural network experts at each region of of the Markov model. i j 1 N are indices for states,
the image. The experts are single-layer perceptrons       k 1 M is the index for the observation symbols.
(SLP) that are trained in a supervised manner (Bishop,       The Markov model is trained with a limited training
1995). Their input is the 64-dimensional fovea content    set, and if the number of states and observation symbols
vector. The output of the experts are 10-dimensional      is large, there will be connections that are not visited
class posterior probability vectors, which are then clus- at all. Since the model is probabilistic, having a tran-
tered with k-means clustering (Duda & Hart, 1973) to      sition or observation probability of zero is not desired.
obtain the ‘what’ information stream. We select single-   Instead, the transitions that have not occurred within the
layer perceptrons over multi-layer perceptrons for a num- training set should have a low probability in the model.
ber of reasons. Multi-layer perceptrons overlearn the     This is what we do in the post-processing stage. We scan
training data quickly, and perform worse on the cross-    the probabilities of the trained Markov model, and re-
validation set. The number of parameters we need to       place all probabilities lower than a threshold (0 001) with
store for the multi-layer perceptron is larger, and the   the threshold value. Then we normalize the probabili-
training time is significantly higher. These properties   ties once more. This is a simple and fast procedure that
make the single-layer perceptron the better choice of ex- achieves the desired effect.

Figure 4: Dynamic fovea simulation results. These are the histograms of the number of correctly and incorrectly
classified digits after each shift. See also Figs. 5 and 6.
   We have also tried Hidden Markov Models where the             Let τ be the threshold we use as our stopping criterion:
states are not visible and where the concatenated where-
                                                                                       αt c      τ                      (7)
what information is the observation, but this structure
performed worse than the observable Markov model.             where the value of τ is in the range [0,1]. If we assume
                                                              that absolute certainty is not reached anywhere in the
Dynamic Fovea                                                 model and αt i is always below 1 0, selecting τ 1 0 is
One important advantage of using a Markov model is the        equivalent to treating all samples as equally difficult and
ease with which we can control the number of shifts nec-      doing eight shifts. Conversely, selecting τ 0 is equiva-
essary for recognition. In the training period, our model     lent to looking at the first salient spot and classifying the
simulates eight shifts, which is set as the upper bound for   sample.
this particular application. After each shift, the Markov        Selecting a large value for τ trades off speed for accu-
model has enough information to give a posterior prob-        racy. With a well selected value, we devote more time
ability for each class. We may calculate the probability      for difficult samples, but recognize a trivial sample in a
αt c of the partial sequence in the Markov model, which       few shifts.
reflects the probability of the sample belonging to a par-
ticular class, given the ‘where’ and ‘what’ information                                 Results
observed so far. Using Eq. 4, we have                         In this section we present our simulation results. We give
                                                              additional information about the techniques we employ
           αt c     P O1      Ot S1     St λc             (5) in subsections.
where O1        Ot is the observation sequence up to time t,
                                                              Local Experts
S1      St is the state sequence, and λ c are the parameters
of the Markov model for class c.                              Implementing local artificial neural network experts both
   We can use this probability to stop our shifts whenever    increases the classification accuracy and decreases the
we reach a sufficient level of confidence in our decision.    complexity and classification time. The single-layer
Let us define αt c , the posterior probability for class c    perceptron returns a 10-dimensional vector from a 64-
at time t:                                                    dimensional linemap image. Since it is trained in a su-
                                                              pervised manner, it provides more useful information for
                                              αt c            classification to the later Markov model, as our experi-
    p̂ c O1      Ot S1     St    αt c                     (6)
                                           ∑ j 1 αt
                                             K
                                                    j         ments indicated.

Dynamic Fovea Simulation
When we simulate the dynamic fovea with a fixed thresh-
old of τ 0 95, we get 85 67 per cent classification accu-
racy with 5 46 per cent standard deviation on the writer-
dependent test set. The average number of shifts is 3 33,
which corresponds roughly to seeing one thirds of the
image in detail. This justifies our claim that analyzing
only a small part of the image is enough to recognize
it. On the writer independent test set, the classification
accuracy is 84 63 per cent, with a standard deviation of
7 58 per cent, and the average number of shifts is 3 37
(See Fig. 4 for the histograms depicting the distribution
of classifications over the shifts). We are doing less than
half the number of shifts we were doing, but the perfor-
mance decrease is less than a standard deviation.           Figure 6: Average number of shifts vs threshold value in
   The advantages of simulating a dynamic fovea become      dynamic fovea simulation
apparent when we inspect Figs. 5 and 6. The accuracy of
classification increases when we increase the threshold,
because a higher threshold means making more shifts to
                                                            them as a sequence. Soft Voting takes into account the
get a more confident answer. A lower threshold means
                                                            10-dimensional outputs of the experts instead of a sin-
that a quick response is accepted. What happens is
                                                            gle class code. Comparing the results with the OMM re-
that the average number of shifts increase sharply if the
                                                            sults show that the order information which is lost during
threshold is set to a value very close to 1.0. In this
                                                            voting but used in OMM is useful. Another observation
case, the classifier cannot exceed the threshold probabil-
                                                            is that the post-processing method we use increases the
ity with eight shifts, and selects the highest probability
                                                            performance by one standard deviation, which is a sig-
class, without doing any more shifts. This is the reason
                                                            nificant increase.
behind the relatively high number of correct and incor-
                                                               The dynamic fovea simulation has a lower classifica-
rect classifications after eight shifts in Fig. 4.
                                                            tion accuracy, but it only needs 3 2 shifts on the average,
                                                            instead of the previous eight.
                                                               Finally, the last row indicates the accuracy of an all-
                                                            parallel scheme. We use a multi-layer perceptron (MLP)
                                                            with 32 32 binary input and 10 hidden units. Although
                                                            the MLP has a good accuracy in this problem, it is not
                                                            scalable due to the curse of dimensionality.
                                                                      Conclusions and Future Work
                                                            The selective attention mechanism exploits the fact that
                                                            real images often contain vast areas of data that are in-
                                                            significant from the perspective of recognition. A low-
                                                            resolution, downsampled image is scanned in parallel
                                                            to find interesting locations through a saliency map,
                                                            and complex features are detected at those locations by
                                                            means of a high-resolution fovea. Recognition is done
Figure 5: Accuracy vs threshold value in dynamic fovea      serially as the location and feature information is com-
simulation                                                  bined in time. By keeping the parallel part of the method
                                                            simple, we can speed-up the recognition process consid-
                                                            erably.
                                                               Our tests have demonstrated that an observable
Simulation Results                                          Markov model may replace an HMM for the two-
We summarize the results we obtain in Table 1. The          pathway selective attention model. The observable
first column of the table shows the method employed.        scheme is easier to train and use, and performs better.
The successive columns indicate the classification accu-    The dynamic fovea simulation reveals further benefits of
racy and its standard deviation on the training, cross-     serializing the recognition process. We can control the
validation, writer-dependent test and writer-independent    time we spend on an image, and differentiate between
test sets.                                                  simple and confusing images. This is a desirable prop-
   In the first two rows, we do eight shifts, generate the  erty in a classifier, since it allows us to apply more reli-
posterior probabilities of classes by the local artificial  able and costly methods to the confusing samples if we
neural network experts and take a vote without treating     wish. It also reduces the average recognition time, but it

                                                Table 1: Summary of Results
                                                                       Performance
                  Method                  Training        Validation      Writer Dep. Test       Writer Indep. Test
        SLP+Simple Voting               86.74(   9.90)   85.92(  9.39)      64.51( 25.62)           62.66( 25.74)
        SLP+Soft Voting                 93.85(   4.47)   91.25(  7.07)      74.35( 27.66)           73.89( 26.67)
        OMM+SLP                         95.32(   3.72)   83.98( 15.37)      84.42( 14.94)           80.92( 16.24)
        OMM+SLP + post-processing       94.37(   3.33)   90.07(  7.92)      89.73( 8.68)            87.37( 8.73)
        Dynamic fovea                   91.41(   4.56)   88.47(  7.98)      85.67( 5.46)            84.63( 7.58)
        MLP                             99.92(   0.12)   97.45(  0.28)      97.25( 0.42)            94.54( 0.21)
must be remembered that the construction of the saliency        Itti, L., Koch, C., & Niebur, E. (1998).          A Model
map is necessary for all samples. Although we reduce               of Saliency-Based Visual Attention for Rapid Scene
the time complexity of the associative level by half, the          Analysis. IEEE Trans. Pattern Analysis and Machine
overall gain is less than that.                                    Intelligence, 20, 11.
   Our attempt to classify digits may be seen as a toy          Klein, R.M. (2000). Inhibition of Return.         Trends in
problem, since the ratio of the fovea area to the image            Cognitive Science, 4(4), 138-147.
is not high enough to demonstrate the benefits of our
model. Although the accuracy is lower than the state-of-        Koch C., & , Ullman, S. (1985). Shifts in Selective Vi-
the-art parallel approaches in the literature (e.g. the MLP        sual Attention: Towards the Underlying Neural Cir-
result in Table 1), the selective attention mechanism is           cuitry. Human Neurobiology, 4, 219-227.
much more appropriate for applications where parallel           Le Cun, Y., Boser, B., Denker, J.S., Henderson, D.,
processing is too cumbersome to use, and the number of             Howard, R.E., Hubbard, W., & Jackel, L.D. (1989).
input dimensions is high.                                          Backpropagation applied to handwritten zip code
   We are planning to employ our model in a more diffi-            recognition. Neural Computation, 1, 4, 541-551.
cult task, such as face recognition, where an all-parallel      Milner, A. D., & Goodale, M. A. (1995) The Visual Brain
classifier, like the MLP, would be unnecessarily com-              in Action. Oxford University Press.
plex; in a face, small regions of the face like eyes, nose,
mouth give us information. The saliency scheme has to           Noton, D, & Stark, L. (1971). Eye Movements and Vi-
be modified for this purpose, as facial features necessi-          sual Perception. Scientific American, 224, 34-43.
tate different and more complex feature detectors. The          Rabiner, L.R. (1989). A Tutorial on Hidden Markov
fovea size also needs to be adjusted for the specific task.        Models and Selected Applications in Speech Recog-
                                                                   nition. Proc. IEEE, 17, 2.
                  Acknowledgments                               Rimey, R.D., & Brown, C.M. (1990). Selective At-
This work is supported by Boğaziçi University Research           tention as Sequential Behavior: Modeling Eye Move-
Funds 00A101D.                                                     ments with an Augmented Hidden Markov Model.
                                                                   (Tech. Rep. TR-327). Computer Science, University
                       References                                  of Rochester.
                                                                Treisman, A.M., & Gelade, G. (1980). A Feature Inte-
Alpaydın, E. (1996). Selective Attention for Hand-
                                                                   gration Theory of Attention. Cognitive Pscychology,
   written Digit Recognition. In D.S. Touretzky, M.C.
                                                                   12, 1, 97-136.
   Mozer, & M.E. Hasselmo (Eds.), Advances in Neural
   Information Processing Systems 8, 771-777.                   Tsotsos, J.K., Culhane, S.M., Wai, W.Y.K., Lai, Y.,
                                                                   Davis, N., & Nuflo, F. (1995). Modeling Visual Atten-
Bishop, C.M. (1995). Neural Networks for Pattern                   tion via Selective Tuning. Artificial Intelligence, 78,
   Recognition. Oxford University Press.                           507-545.
Crick, F., & Koch, C. (1990). Towards A Neurobio-               UCI Machine Learning Repository,                  Optdigits
   logical Theory Of Consciousness. Seminars in the                Database, prepared by E. Alpaydın and C. Kay-
   Neurosciences, 2, 263-275.                                      nak.        ftp://ftp.ics.uci.edu/pub/machine-learning-
Duda, R., & Hart, P. (1973). Pattern Classification and            databases/optdigits.
   Scene Analysis. New York: John Wiley & Sons.                 Ungerleider, L.G., & Mishkin, M. (1982). Two cortical
                                                                   visual systems. In D.J. Ingle, M.A. Goodale, & R.J.W.
Foster, D.H., & Westland, S. (1998). Multiple Groups of
                                                                   Mansfield (Eds.), Analysis of visual behavior, MIT
   Orientation-selective Visual Mechanisms Underlying
                                                                   Press.
   Rapid Oriented-line Detection. Proc. Royal Society
   London, 265, 1605-1613.

