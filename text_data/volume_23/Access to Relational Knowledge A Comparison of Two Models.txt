UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Access to Relational Knowledge: A Comparison of Two Models
Permalink
https://escholarship.org/uc/item/8nq6s57w
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 23(23)
Authors
Wilson, William H.
Marcus, Nadine
Halford, Graeme S.
Publication Date
2001-01-01
Peer reviewed
 eScholarship.org                              Powered by the California Digital Library
                                                                University of California

              Access to Relational Knowledge: a Comparison of Two Models
                                       William H. Wilson (billw@cse.unsw.edu.au)
                                      Nadine Marcus (nadinem@cse.unsw.edu.au)
                  School of Computer Science and Engineering, University of New South Wales, Sydney,
                                                New South Wales, 2052, Australia
                                         Graeme S. Halford (gsh@psy.uq.edu.au)
                  School of Psychology, University of Queensland, Brisbane, Queensland, 4072, Australia
                            Abstract                             directional access (cf. Halford, Wilson & Phillips,
                                                                 1998). Omni-directional access is the ideal form of
   If a person knows that Fred ate a pizza, then they can        accessibility.
   answer the following questions: Who ate a pizza?, What          Another reason for investigating this lies in the work
   did Fred eat?, What did Fred do to the pizza? and even
   Who ate what? This and related properties we are              of Halford, Wilson, and Phillips (e.g. 1998) which
   terming accessibility properties for the relational fact      seeks in part to define a hierarchy of cognitive
   that Fred ate a pizza. Accessibility in this sense is a       processes or systems and to draw parallels between this
   significant property of human cognitive performance.          hierarchy and a second hierarchy of types of artificial
   Among neural network models, those employing tensor           neural networks. Levels 0 and 1 of this second
   product networks have this accessibility property. While      hierarchy are 2- and 3-layer feedforward nets, and
   feedforward networks trained by error backpropagation
                                                                 levels 2-5 are tensor product nets of increasing rank. It
   have been widely studied, we have found no attempt to
   use them to model accessibility using backpropagation         thus becomes interesting to consider how well
   trained networks. This paper discusses an architecture        feedforward nets can emulate tensor product networks.
   for a backprop net that promises to provide some degree
   of accessibility. However, while limited forms of
   accessibility are achievable, the nature of the                  relation        subject        object     outputs
   representation and the nature of backprop learning both
   entail limitations that prevent full accessibility. Studies
   of the degradation of accessibility with different sets of                                          3-dimensional
   training data lead us to a rough metric for learning                                                    arrayof
   complexity of such data sets.                                                                        binding units
                        Introduction
The purpose of this research is to determine whether a
backpropagation net can be developed that processes                 relation        subject        object     inputs
propositions with the flexibility that is characteristic of
certain classes of symbolic neural net models. This has
arguably been difficult for backpropagation nets in the                 Figure 1 â€“ Tensor product network of rank 3.
past. For example, the model of Rumelhart and Todd
(1993) represents propositions such as "canary can fly".           As tensor product networks are not as well known as
Given the input "canary, can" it produces the output             feedforward networks, we shall describe them and their
"fly". However processing is restricted, so it cannot            accessibility properties briefly here before proceeding.
answer the question "what can fly?" ("canary").                  Tensor product networks are described in more detail,
  There are, however, at least two types of symbolic             and from our point of view, in Halford et al. (1994).
nets that readily meet this requirement. One type of net         Briefly, a rank k tensor product network consists of a k-
model makes roles and fillers oscillate in synchrony             dimensional array of "binding units", together with k
(Hummel & Holyoak, 1997; Shastri & Ajjanagadde,                  input/output vectors. For example, a rank 2 tensor
1993) while another is based on operations such as               product network is a matrix, plus 2 input/output vectors.
circular convolution (Plate, 2000) or tensor products            To teach the network to remember a fact (that is, a k-
(Halford, et al., 1994; 1998; Smolensky, 1990). These            tuple), the input/output vectors are set to be vectors
models appear to have greater flexibility than models            representing the components of the k-tuple, and a
based on backpropagation nets, in that they can be               computation is performed that alters the k-dimensional
queried for any component of a proposition. We will              array. Subsequently that fact can be accessed in a
refer to this property of tensor product nets as omni-           variety of ways. It is common to interpret the first

component of the k-tuple as a predicate symbol, and the          Our specific aim in this paper is to experiment with a
remaining components as argument symbols - e.g. for           feedforward net design that appears to have potential to
rank 3, the components might be vectors representing          provide at least limited accessibility in a rank 3
the concepts likes jane pizza (Jane likes pizza) (see         situation. When we move to feedforward networks
Figure 1). Once this fact has been taught to a rank 3         trained by error backpropagation, we hope to preserve
tensor product network, the following 7 queries can be        the accessibility property that is characteristic of
formulated and answered by a computation involving            symbolic nets. The model resembles an auto-encoder
the tensor product network.                                   but has restricted connectivity.
1) Is likes(jane,pizza) true?
2) Who likes pizza? This we often write as likes(X,           Architecture of the network
  pizza)? The response depends on what else has been          The particular backpropagation network we used to test
  taught to the tensor product network. If the tensor         for accessibility consisted of the following components:
  product network also knows that likes(fred,pizza) and       15 input units, 15 hidden units and 15 output units. The
  likes(mary,pizza) then the response will be the sum of      15 input units were used to represent 3 items or
  the vectors representing Jane, Fred, and Mary - often       patterns, each made up of 5 elements. The hidden and
  written jane + fred + mary.                                 output units also each consisted of three groups of 5
3) What does Jane like? - likes(jane,X)? Similar to 2).       units, connected as shown in Figure 2. The input
4) What relationships hold between Jane and pizza? -          patterns represented relational instances of the form
  X(jane,pizza)? Again, similar to 2).                        RELATION(SUBJECT, OBJECT). The target output
  These four are referred to as limited accessibility.        contained the same information: namely, RELATION,
                                                              SUBJECT and OBJECT.
5) Who likes what? - likes(X,Y)? The response in this
                                                                                              likes
  case would be a rank 2 tensor product network
                                                                                                       hidden   relation
                                                                                              buys
                                                                    input: buys(mary, book)
  storing the pairs (X,Y) for which likes(X,Y) is known                                       hates
                                                                                              has
  to the original rank 3 tensor product network. The
                                                                                                                           target output
                                                                                              sells
  tensor product network approach solves this by                                              jane
                                                                                                                subject
  producing a rank 2 tensor product network, which                                            fred
                                                                                              mary
  stores the pairs (X,Y). (This output possibility, and                                       gina
                                                                                              bob
  corresponding ones for 6) and 7) below, are not
                                                                                              pizza
  shown in Figure 1).
                                                                                                                 object
                                                                                              dog
6) Who does what to pizza? - X(Y,pizza)? Like 5).                                             book
                                                                                              icecrm   units
7) Jane does what to what? - X(jane,Y)? Like 5).                                              hat
  The full set of 7 forms of access are referred to as full        Figure 2 - Connections in our feedforward net
accessibility, or omni-directional access.                                         architecture.
   A rank 4 tensor product network would have 15
access modes, a rank 5 tensor product network would             Notice that this network consists of three functions:
have 31 access modes, and so on. Provided that an             one takes as inputs a relation name and a subject, and
orthonormal set of vectors is used for the set of vectors     produces an object as output, the second takes relation
representing concepts, retrieval is perfect. Facts are        name and object and produces subject as output, and the
learned by a tensor product network one at a time, and        third takes subject and object and produces relation
do not interfere with each other (given orthonormal           name as output. Thus, while it resembles a traditional
representation vectors).                                      auto-association net, note that regular auto-association
   Tensor product networks using orthonormal sets of          nets allow connection paths between corresponding
representation vectors exhibit what has been called full      input and output neurons, typically allowing total
omni-directional access to the facts that have been           interconnection between input and hidden layers, and
taught, as noted above. Humans attempting similar             between output and hidden layers. In essence, the
tasks may find some types of access easier than others.       network architecture can be unraveled into 3 distinct
For example, children who have recently learned sets of       networks that share common inputs. Thus, any weight
multiplication facts such as 9Ã—7=63 are able to use this      in the network is influenced by the output errors in only
knowledge to perform division (9Ã—X= 63, what is X?),          one of the 3 output sets (relation, subject, object). The
but may find this more difficult than multiplication          net makes learning easier by constraining the learning
(9Ã—7=X, what is X?). We use the term accessibility to         algorithm to look for sets of weights that, for example,
refer to imperfect or partial versions of omni-directional    ignore predicate input when trying to infer predicate
access. It turns out that some of the nets discussed in       output from argument input. We also conducted some
this paper also exhibit accessibility rather than full        pilot studies with a fully connected network and the
omni-directional access.                                      networkâ€™s performance was inferior.

   Notice that both the tensor product net architecture      instances and their relationships (or in this particular
and the architecture we are studying here have three         case, their lack of relatedness).
groups of input and three groups of output nodes.               The system was trained for 20 000 epochs. At around
                                                             4000 epochs the error curve smoothed close to zero. In
                Experimental design                          other words, the difference between the target and the
The network was given three different sets of relational     actual output values was negligible.
instances to learn. Each set was made up of five
                                                             Table 1 â€” The instances and patterns in training set 1.
different relational instances. For example, given the
relational instance likes(jane,pizza), l i k e s is the       Relational instance     Action   Subject    Object
RELATION, jane is the SUBJECT and pizza is the
OBJECT. The training sets varied in terms of the             likes(jane, pizza)       10000    10000      00100
amount of overlap or the degree of interaction between       buys(fred, book)         01000    01000      00010
the elements of each of the five relational instances.       hates(mary, dog)         00100    00100      00001
Training set 1 was set up to contain little or no overlap    has(gina, icecrm)        00010    00010      10000
between the different relational instances. Training set 3   sells(bob, hat)          00001    00001      01000
consisted of five relational instances with a large degree
of interaction between the different instances. Training
set 2 contained an intermediate degree of overlap.
   It was hypothesized that relational instances with the
least amount of interaction between the different
instances would be the easiest to learn. This is because
each instance does not have components that overlap
with the other instances. These relations are one-to-one
mappings. Accordingly, the network is most likely to
achieve success in learning such a set of facts. In            Figure 3 - Graphical representation of 3 training sets.
contrast, the set of relations with the highest level of
overlap is expected to be the most difficult for the            The system was then presented with a set of test
network to learn. These relations can be classified as       patterns to assess the degree of accessibility that could
many-to-many mappings, and so cannot be completely           be obtained (refer to Appendix 1, test pattern set 1). For
learned by a feedforward network. Accessibility may be       example, to present the query likes(jane,X) the
easier to obtain if each relational instance can be          RELATION and SUBJECT input units were set to the
represented in isolation, with little or no reference to the patterns for likes and jane respectively, and the
other relations. As the overlap and interaction between      OBJECT input units were set to all zeroes, i.e. the â€™X â€™ is
the relations and their elements increases, so the degree    represented by â€™00000â€™. Then the OBJECT outputs
of accessibility that can be obtained is likely to           were inspected. All of the outputs were checked to see
decrease. This is because information from other related     if they matched the target values. If the correct number
instances is more likely to interfere, when the system is    of output units that should be on is N, then a value of
presented with queries.                                      3/(5N) or greater for an output unit is considered to be
   The software used to run the simulations described in     on, a value of 2/(5N) or less is considered to be off and
this paper is Tlearn v1.01 (Plunkett & Elman, 1997).         any values in the region between 2/(5N) and 3/(5N) can
Other simulators were also used and similar results          be seen to be "partially on". The output thus falls into
were obtained. The settings used included a learning         one of three categories: 1) Either the output is correct
rate of 0.1, momentum set to 0 (the default) and an          and all of the outputs units are correctly on or off (as
initial weight range of -0.5 to 0.5.                         defined above), or 2) The output is incorrect and at
                                                             least one output unit that should be on is clearly off and
Training set 1                                               vice-versa, or lastly, 3) The output is uncertain or
In this simulation, the network was trained on five          partially correct, and output units that should be on are
relations that have no overlap. Each relational instance     only "partially on". For example, if likes(fred,pizza)
consisted of a unique OBJECT, SUBJECT and                    and likes(fred,dog) are facts, then if presented with the
RELATION. Within each relational instance, each field        query likes(fred,X), the answer would be pizza and dog,
or argument was represented by a 1-out-of-5 localist         i.e. N=2. Therefore, the units representing both dog and
encoding. The first five relational instances (and their     pizza need to be on and a value of 0.3 (3/(5N)) or
associated patterns) that were given to the network to       greater for both units is needed for the answer to be
learn are shown in Table 1. Figure 3 (see training set 1)
                                                             accepted as correct. Moreover, output units that need to
contains a graphic representation of the relational

be off should be less than 2/(5N) or in this case less      element were correctly answered by the system and
than 0.2.                                                   some of the queries with two unknown elements were
   With training set 1, correct scores were obtained for    correct. None of the queries were considered incorrect,
all of the test queries. The system was able to handle all  however, four queries obtained uncertain or partially
of the single and double query test patterns. Therefore,    correct scores.       These queries were likes(X,Y),
overall an excellent degree of accessibility was            buys(X,Y), X(gina,Y), and X(Y,pizza). It is interesting to
achieved with the first training set.                       note that all of the responses of questionable
   When there is little or no overlap between the           correctness need to access information that is contained
elements of the relational instances, the system is able    in more than one relational instance, i.e. information
to learn and access elements of the relations with ease.    from the many-to-one and one-to-many mappings. For
This would correspond with human learning, where the        instance, the answer to likes(X,Y) is that both Fred and
less related information is to other information, the       Jane like pizza. This can be clearly expressed in the
easier it is to understand and learn (Marcus, Cooper &      representation available, but the trained system does not
Sweller, 1996; Sweller, 1994).                              do so. Thus although, accessibility is still relatively
                                                            good, the net struggles with the queries that access
Training set 2                                              information that has to be integrated from two relational
The next training set the system was given to learn had     instances.
a higher degree of interaction between the relational
instances and their elements than training set 1. In        Table 2 â€” The instances and patterns in training set 2.
particular, both Fred and Jane like pizza and Jane buys
both dogs and books. For the first two instances the         Relational instance      Action    Subject    Object
same OBJECT is liked by two SUBJECTS and can be             likes (jane, pizza)       10000      10000     00100
characterized as a many-to-one mapping, and for the         likes (fred, pizza)       10000      01000     00100
last two instances the same SUBJECT buys two                hates(mary,icecrm)        00100      00100     10000
different OBJECTS, a one-to-many mapping. In
contrast, hates(mary,icecream) is the only instance that    buys (gina, book)         01000      00010     00010
does not overlap with the other four, and is a one-to-one   buys (gina, dog)          01000      00010     00001
mapping. The OBJECT and SUBJECT in this instance
are unique and are not contained in any of the other           We also tried training the network with the 3
instances. The five relational instances contained in       patterns: likes(fred+jane,pizza), buys(gina,book+dog),
training set 2 are shown in Table 2. The overlap or         and hates(mary,icecream), that is, the 3 outputs the net
interrelations between the elements of the relational       just discussed (call it net 2A) produced in response to
instances can be seen graphically in Figure 3 (see          the training patterns. The network rapidly learned these
training set 2).                                            patterns, not surprisingly. We tested this network (net
   The system was trained for 20 000 epochs. At around      2B) on the queries shown in Appendix 1, test pattern set
2000 epochs the error stabilized at around 0.6 in terms     2, and found that it had inferior accessibility
of Tlearnâ€™s error measure. The trained net transformed      performance compared with net 2A.
the training patterns as follows:                              The greater number of uncertain or partially correct
input                       output                          scores obtained during testing, for training set 2 (net
likes(jane, pizza)    â†’ likes(fred+jane, pizza)             2A) reflects the fact that these five assertions may be
likes(fred, pizza)    â†’ likes(fred+jane, pizza)             considered harder to learn. These findings suggest that
buys(gina, book)      â†’ buys(gina, book+dog)                as the degree of overlap between the relational
buys(gina, dog)       â†’ buys(gina, book+dog)                instances and their elements increases and as the
hates(mary, icecrm) â†’ hates(mary, icecrm).                  amount of related information that needs to be
   It can be seen that the four (related) assertions have   considered at once increases, so the level of
now been combined into two assertions. What has been        accessibility that the system can cope with, decreases.
learned is intelligible - likes(gina,book+dog) is easy to   This corresponds with our understanding of difficulty
interpret as signifying that gina likes both books and      associated with learning for people. The more
dogs.                                                       interactivity there is between different learning
   A set of test patterns (see Appendix 1, test pattern set elements, the harder information is to learn (Sweller &
2) was then given to the system to assess performance       Chandler, 1994). The more difficult it is to learn
on the accessibility property. All output units were then   information, the harder it is to transform and use that
inspected to see if they matched the target units. Using    information. It thus appears, that as the information
the scoring criterion described above, output patterns      becomes more complex and so more difficult to learn,
were either considered to be 1) correct, 2) incorrect or    the backpropagation system struggles to achieve a
3) uncertain. All of the queries with a single unknown

reasonable level of accessibility. The next training set   The rest of the queries with two unknown elements
supports this hypothesis.                                  were incorrect. These queries all access information
                                                           from more than one relational instance. For example,
Training set 3                                             the query X(jane,Y) should have a response of
The last training set has the highest degree of            likes+buys, pizza+dog. However, the systemâ€™s
interaction between the relational instances and their     response to this query is only buys, pizza+dog. As with
elements. The relational instance likes(fred,pizza)        all the other incorrect queries, some of the relevant
overlaps with two other instances. The SUBJECT fred        information has been lost. It appears that as the
performs the RELATION likes on both the OBJECTS            information becomes more and more overlapping, the
dog and pizza, a one-to-many mapping. Also, both           network finds it harder and harder to handle queries that
SUBJECTS fred and jane perform the RELATION                access related elements of information. This type of
likes on the same OBJECT pizza, a many-to-one              network appears to be more suited to dealing with one-
mapping. The five relational instances contained in        to-one relations, rather than many-to-many mappings.
training set 3 are shown in Table 3. Figure 3 (see
training set 3) contains a graphic representation of the   Training set 4
relational instances and their interrelatedness.           A fourth training set was given to the system to learn. It
                                                           consisted of the relational instances likes(jane,pizza),
Table 3 â€” The instances and patterns in training set 3.    likes(fred,pizza), likes(fred,dog), buys(fred,dog), and
                                                           buys(jane,book). The amount of overlap between these
 Relational instance     Action    Subject    Object       instances, and the test results, fall somewhere between
likes (jane, pizza)     10000       10000     00100        training sets 2 and 3. All the single unknown element
likes (fred, pizza)     10000       01000     00100        queries were answered correctly. Three of the two
likes (fred, dog)       10000       01000     00001        unknown element queries were answered correctly, two
buys (fred, book)       01000       01000     00010        were uncertain and two were incorrect.
buys (jane, dog)        01000       10000     00001
                                                                                  Conclusion
   The system was trained for 20 000 epochs. At around     As the degree of overlap between the arguments and
3000 epochs the error stabilized at around 0.7 in terms    predicates of the relational instances in the training set
of Tlearnâ€™s error measure. It should be noted that         increases, the degree of accessibility provided by the
buys(fred,book) and buys(jane,dog) each have at most       nets simulated decreases. It is well-known that when
one attribute in common with the other instances. The      trained on data that corresponds to a one-to-many
trained net transformed the three more overlapping         mapping, the activations of the output units
instances as follows:                                      corresponding to the "many" will be reduced in
input                      output                          comparison to a one-to-one mapping. To us, the
likes(jane, pizza) â†’ likes(fred+jane, pizza)               interesting thing is the effect of argument and predicate
likes(fred, pizza) â†’ likes(fred+jane, pizza+dog)           overlap on accessibility, and the fact that beyond some
likes(fred, dog)     â†’ likes(fred, pizza+dog).             critical level of overlap, the trained net starts to produce
   Notice that from this output, there is no way to        "generalizations" which, seen from the relational-
interpret these instances without inferring that Jane also instance point of view, mean that the net has learnt false
likes dogs. The whole is not truly equivalent to the sum   propositions e.g. likes(jane,dog).
of the parts, which in this case are the three (and not       By way of contrast, tensor product networks (Halford
four) given relational instances. Thus the pattern         et al., 1998) provide full accessibility for arbitrary sets
likes(fred+jane,pizza+dog) even if it were valid, would    of relational instances, and do not lose critical
be unintelligible (in contrast to likes(gina,book+dog) in  information when tested.
training set 2).                                              Backpropagation nets can handle propositional
   The test patterns shown in Appendix 1 (test pattern     information that is in the form of distinct functions. For
set 3) were used to test the trained net for accessibility example, the model of Rumelhart and Todd (1993)
properties. As before, output units were inspected to see  handles propositions such as "canary can fly" in the
if they matched the target units. Using the scoring        sense that, given an input "canary can" it produces the
criterion described above output patterns were either      output "fly". However, it was not tested for the
considered to be correct, incorrect or uncertain. All of   accessibility property. Our backpropagation net was
the queries with a single unknown element were             tested for accessibility, but succeeded in only a limited
correctly answered by the system. However, only two        sense. It could only handle queries to data sets that are
of the queries with two unknown elements were correct.     relatively simple, in terms of the overlap and
The two correct queries were X(Y,dog) and X(Y,book).       relatedness of information. As the relational instances

in the data set become more and more related, so             Hummel, J. E., & Holyoak, K .J. (1997). Distributed
accessibility deteriorates. Consequently the net could          representations of structure: A theory of analogical
not model propositional knowledge adequately. In                access and mapping. Psychological Review, 104, 427-
contrast, a tensor product net can process more complex         466.
data sets and still have full access to all the elements of  Marcus, N., Cooper, M., & Sweller, J. (1996).
the relational instances.                                       Understanding Instructions. Journal of Educational
  In a sense, it is not surprising that a backprop-trained      Psychology, 88(1), 49-63.
net does not do as well at this task - backprop tends to     Plunkett, K., & Elman, J. L. (1997). Exercises in
do well at perceptual tasks where generalization of an          Rethinking Innateness: A Handbook for Connectionist
interpolative type is useful, whereas the data used in          Simulations. Cambridge, Mass: MIT Press.
this is discrete. Since their introduction, backprop nets    Plate, T. A. (2000). Analogy retrieval and processing
and variants have been used in cognitive modeling tasks         with distributed vector representations. Expert
including those concerned with discrete relational              Systems: The International Journal of Knowledge
knowledge (Hinton, 1986; Rumelhart & Todd, 1993).               Engineering & Neural Networks, 17(1), 29-40.
This paper has attempted to explore the boundaries of        Rumelhart, D. E., & Todd, P. M. (1993). Learning and
applicability of such models.                                   connectionist representations. In D.E. Meyer & S.
  What has come out in the wash is evidence from the            Korhnblum (Eds), Attention and Performance XIV
modelâ€™s performance of a new dimension of task                  (figure 1.9 p15, top paragraph p16). Cambridge,
difficulty. This dimension measures component overlap           Mass: MIT Press.
in a set of facts to be learned. This type of difficulty     Shastri, L. & Ajjanagadde, V. (1993). From simple
seems to correlate with model performance at the                associations to systematic reasoning: A connectionist
boundary between rank 1 and rank 2 tasks (in the sense          representation of rules, variables, and dynamic
of Halford et al., 1998).                                       bindings using temporal synchrony. Behavioural and
  It is clear that humans do have accessibility with            Brain Sciences, 16(3), 417-494.
respect to their relational knowledge. What might be         Smolensky, P. (1990). Tensor product variable binding
interesting to investigate is whether they have greater
                                                                and the representation of symbolic structures in
difficulty learning sets of facts like those in training set
                                                                connectionist systems. Artificial Intelligence, 46,
3 than those in training set 1, and whether accessibility
also takes longer to develop (see Sweller & Chandler,           159-216.
1994 for a discussion of element interactivity and its       Sweller, J. (1994). Cognitive load theory, learning
effects on learning).                                           difficulty and instructional design. Learning and
                                                                Instruction, 4, 295-312.
                                                             Sweller, J., & Chandler, P. (1994). Why some material
                  Acknowledgments
                                                                is difficult to learn. Cognition and Instruction, 12,
This work was supported by a grant from the Australian          185-233.
Research Council. We wish to acknowledge helpful
discussions with Steve Phillips, and helpful comments                                   Appendix 1
made by a reviewer of a previous version of the paper.
                                                             Test pattern set 1
                                                             likes(jane,X), buys(fred,X), hates(mary,X), has(gina,X), sells(bob,X),
                       References                            likes(X,pizza), buys(X,fred), hates(X,dog), has(X,icecrm),
                                                             sells(X,hat), X(jane,pizza), X(fred,book), X(mary, dog), X(gina,
Halford, G. S., Wilson, W. H., Guo, J., Gayler, R. W.,       icecrm), X(bob,hat), likes(X,Y), buys(X,Y), hates(X,Y), has(X,Y),
  Wiles, J., Steward, J. E. M. (1994). Connectionist         sells(X,Y), X(jane,Y), X(fred,Y), X(mary,Y), X(gina,Y), X(bob,Y),
  implications for processing capacity limitations in        X(Y,pizza), X(Y,book), X(Y,dog), X(Y,icecrm), X(Y,hat).
  analogies. In K. J. Holyoak & J. Barnden (Eds.),
                                                             Test pattern set 2
  Advances in connectionist and neural computation           likes(jane,X), likes(fred,X), buys(gina,X), hates(mary,X),
  theory, vol. 2: Analogical connections. Norwood, NJ:       likes(X,pizza), buys(X,book), buys(X,dog), hates(X,icecrm),
  Ablex.                                                     X(jane,pizza), X(fred,pizza), X(gina,book), X(gina,dog),
Halford, G. S., Wilson, W. H., & Phillips, S. (1998).        X(mary,icecrm), likes(X,Y), buys(X,Y), hates(X,Y), X(jane,Y),
                                                             X(fred,Y), X(gina,Y), X(mary,Y), X(Y, pizza), X(Y,book),
  Processing capacity defined by relational complexity:      X(Y,dog), X(Y,icecrm).
  Implications for comparative, developmental, and
  cognitive psychology. Brain and Behavioural                Test pattern set 3
  Sciences, 21, 803-864.                                     likes(jane,X), likes(fred,X), buys(fred,X), buys(jane,X),
Hinton, G. E. (1986). Learning distributed                   likes(X,pizza), likes(X,dog), buys(X,book), buys(X,dog),
  representations of concepts. In Proceedings of the         X(jane,pizza), X(fred,pizza), X(fred,dog), X(fred,book), X(jane, dog),
                                                             likes(X,Y), buys(X,Y), X(jane,Y), X(fred,Y), X(Y,pizza), X(Y,dog),
  Eleventh Annual Conference of the Cognitive Science        X(Y,book).
  Society, 1-12, Hillsdale, NJ: Lawrence Erlbaum
  Associates.

