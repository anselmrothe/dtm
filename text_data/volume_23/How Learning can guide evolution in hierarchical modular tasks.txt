UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
How Learning can guide evolution in hierarchical modular tasks
Permalink
https://escholarship.org/uc/item/8291n256
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 23(23)
Authors
Wiles, Janet
Tonkes, Bradley
Watson, James R.
Publication Date
2001-01-01
Peer reviewed
 eScholarship.org                              Powered by the California Digital Library
                                                                University of California

            How learning can guide evolution in hierarchical modular tasks
                                           Janet Wiles (janetw@csee.uq.edu.au)
                      School of Psychology and School of Computer Science and Electrical Engineering
                                         University of Queensland, Qld 4072 Australia
                                        Bradley Tonkes (btonkes@csee.uq.edu.au)
                                       James R. Watson (jwatson@csee.uq.edu.au)
                                     School of Computer Science and Electrical Engineering
                                         University of Queensland, Qld 4072 Australia
                             Abstract                            populations as a whole, reflecting their original motiva-
                                                                 tions as models (albeit abstract ones) of real evolutionary
   This paper addresses the problem of how and when learn-       processes.
   ing is an aid to evolutionary search in hierarchical modu-
   lar tasks. It brings together two major areas of research in     Some of the oldest and most popular techniques for
   evolutionary computation, the performance of evolution-       evolutionary search are genetic algorithms (GAs), which
   ary algorithms on hierarchical modular tasks, and the role    use crossover as their major search technique. Originally
   of learning in evolutionary search, known as the Bald-        developed by Holland (1992), their efficacy is thought
   win effect. A new task called the jester’s cap is pro-        to be based on groups of genes acting together as mod-
   posed, formed by adding learning to Watson, Hornby
   and Pollack’s Hierarchical-If-and-only-If, function, using    ules (or building blocks, to use Holland’s original ter-
   the simple guessing framework of Hinton and Nowlan’s          minology), and have been studied extensively since (for
   Baldwin effect simulations. Whereas Hinton and Nowlan         general introductions see Goldberg, 1989 and Mitchell,
   used a task with a single fitness peak, ideally suited to     1996).
   learning, the jester’s cap is a hierarchical task that has
   two major fitness peaks and multiple sub-peaks. We con-          A variety of modular tasks have been proposed to
   ducted a series of simulations to explore the effect of dif-  study the conditions under which GAs outperform com-
   ferent amounts of learning on the jester’s cap. The sim-      parable search techniques. The most widely known of
   ulations demonstrate that learning aids evolution only in     these are the Royal Road (RR) problems introduced by
   search spaces in which the simplest level of modules are      Mitchell et al. (1992). However, some forms of hill-
   difficult to find. The learning mechanism explores lo-
   cal regions of the search space, while crossover explores     climbers were found to easily outperform the GA, and a
   neighborhoods in higher-order modular spaces.                 variety of tasks that incorporate deceptive elements have
                                                                 been defined (s.a., RR4 by Mitchell et al., 1994; HDF by
                                                                 Pelikan and Goldberg, 2000; hdf by Holland, 2000).
                         Introduction                               An alternative approach to incorporating deceptive
This paper addresses the problem of how and when learn-          elements is to define a fitness function with two or
ing is an aid to evolutionary search in hierarchical mod-        more conflicting maxima. Watson et al. (1998) defined
ular tasks. It brings together two major areas of research       Hierarchical-If-and-only-If (H-IFF) as such a function.
in evolutionary computation (EC), the performance of             H-IFF is a simple function that is hierarchical, modular,
evolutionary algorithms (EAs) on hierarchical modular            is not searchable by mutation, but is amenable to search
tasks, and computational models of the role of learning          by crossover. Its defining characteristics are two fitness
in evolutionary search, known as the Baldwin effect.             peaks at opposing ends of the search space. Combina-
   We begin with a brief review of modular tasks that            tions of the sub-components that comprise each level of
have been proposed to explore the performance of evo-            the competing hierarchies cause many sub-optimal peaks
lutionary algorithms, and then briefly describe the Bald-        and consequently many local minima (see below for the
win effect. We then describe a specific task, the jester’s       complete definition).
cap, that incorporates learning into a hierarchical mod-            Before proceeding further with the computational as-
ular task. In many simulation tasks, learning is costly          pects, it is worthwhile considering the relevance of mod-
and does not improve the performance of an evolutionary          ule building to many areas of cognitive science. The
algorithm (French and Messinger, 1994; Mayley, 1996;             role of modules in evolution has long been recognized
Pereira et al., 2000). This study is as much an investiga-       (e.g., Dawkins, 1986). In evolutionary psychology there
tion of things that don’t learn, as of ones that do.             is a particularly strong interest in modules, in part due
   There are many types of EAs, and the field of evolu-          to Tooby and Cosmides (1994) claims that humans have
tionary computation is still determining features of prob-       behavioral modules analogous to other mental functions.
lems that are easy or hard for a particular class of EA, and     By studying building block problems, we are consider-
the conditions under which such algorithms will perform          ing the types of processes that allow species to evolve
better than other search techniques. In evolutionary com-        varieties of modules, and their combination into com-
putation, characterization of an EA’s performance con-           plex mental organs. For example, echolocation in bats
cerns not just optimization per se, but the behaviors of         requires both the ability to emit and to receive high fre-

quency sounds. Each of these abilities has utility as a          Hinton and Nowlan’s original simulations have stim-
module in its own right, but the combination provides an      ulated a considerable body of literature, which is only
additional capacity that goes beyond the cumulative ben-      briefly mentioned here: Belew (1990) replicated and ex-
efit of the independent components.                           tended the original study, adding changing environments
                                                              and cultural advantage; Harvey (1993) showed how
The Baldwin effect: How learning can guide                    remnants of residual learning are due to genetic drift;
evolution                                                     French and Messinger (1994) investigated under what
                                                              conditions learning supplements genetic search; May-
Under Darwinian inheritance, the things that an animal        ley (1996) demonstrated how learning is first selected
learns during its life cannot be passed directly to its off-  for, then against as evolution progresses; and Mitchell
spring via the genotype. However, researchers in the late     and Belew (1996) discuss issues arising from the orig-
19th century (Baldwin, 1896; Morgan, 1896), proposed          inal study. A useful reference is the edited volume of
a way in which learned behaviors could be incorporated        papers relating to learning and evolution by Mitchell and
into a genome over many generations (i.e., become in-         Belew (1996), which includes reprints of the original pa-
stinctual). The mechanism is purely Darwinian, and re-        pers by Baldwin (1896), Morgan (1896), and Hinton and
lies on gradual changes in the distribution of genes in a     Nowlan (1987) as well as many other related studies.
population, as the following rationale explains.
   Consider a population of agents comprising a variety
of search strategies with initially random starting points    Learning in hierarchical modular spaces
and a range of search radii. The starting point and search
radius of an agent is its “bias”. An evolutionary algo-       The issues in this study follows from French and
rithm that selected for speed in finding a point in the       Messinger’s (1994): Consideration of the circumstances
search landscape over many generations would evolve a         under which is it reasonable to expect that learned be-
population of individuals that had starting points close      haviors will firstly enhance evolutionary search, and
to the fitness maximum and small search radii. That is,       secondly be gradually replaced by genetically specified
behaviors that were initially interpreted as general learn-   traits. However, we take an alternative approach and
ing abilities would, over time, become innate. No in-         investigate whether learning aids evolution in search
formation about the content of learning is passed from        spaces that contain competing modules. One way of
parent to offspring, but in the general variation across      thinking about this issue is in terms of the difficulty of
the population, some individuals would by chance have         the search task compared to the operators that are avail-
starting points closer to the fitness maximum and smaller     able. In Hinton and Nowlan’s NIAH task, the set of
search radii (i.e., slightly stronger biases). These individ- twenty ones can be considered as one module, with no
uals would have more offspring than those with weaker         intermediate fitness levels for partial results. The search
biases, and in environments with fixed fitness functions,     task for the genome is too large to find by populations of
innate behaviors would gradually replace learned ones.        size substantially smaller than that of the search space. In
   This process is often called the Baldwin effect and has    this case, learning performs the function of a local search
two interesting components. The first is the explanation      through points close together in Hamming distance. The
of how learned behaviors can become innate as described       local search supplements the genetic search, effectively
above. The other is the power of learning to augment          smoothing the search landscape (see top, Figure 1).
genetic search to build complex modules. Agents that             The NIAH task is a particularly pathological as it con-
can search their local environment will be able to explore    tains no partial information to guide a search process.
whole regions of search space in their lifetimes, rather      The majority of tasks of interest in EC and cognitive sci-
than the single point of their own genotype. In this way,     ence have some internal structure, or distinct modules.
learning can enable an evolutionary algorithm to solve        The most tractable problems that have modular structure
problems that are too costly for genetic search alone.        are those in which the genes that comprise modules can
   The first computational simulations of the Baldwin ef-     be selected independently of the settings or global struc-
fect were by Hinton and Nowlan (1987). They used              ture of other genes.
a needle-in-a-haystack (NIAH) problem, in which the
maximum fitness of an agent corresponded to a geno-              As described above, a variety of such tasks have been
type comprising all ones. Each gene could be one, zero        proposed to explore the functionality of GAs, including
or question mark. The ones and zeroes were fixed val-         the Royal Road problems (Forrest and Mitchell, 1993;
ues that did not change during an individual’s lifetime.      Mitchell et al., 1994). The Royal Road problems had
The question marks were learnable genes, which could          only one fitness peak, and hence hill climbing strategies
change during a lifetime. Hinton and Nowlan showed            worked well (see Mitchell, 1996 for a summary of the
that the zero alleles quickly dropped out of the popu-        reasons).
lation and the number of question marks reduced over             The H-IFF problem has the interesting property of
time. Hinton and Nowlan’s study is a landmark in EC be-       symmetry around diametrically opposed fitness peaks
cause it was the first computational demonstration show-      with many sub-optimal peaks and consequently many lo-
ing how learning can guide evolution.                         cal minima (see bottom, Figure 1).

                                                                          0       0       1       0       1       1       1       1   Value
                                                                          1       1       1       1       1       1       1       1     8
                                                                              2               —               2               2         6
             Fitness
                                                                                      —                               4                 4
                                                                                                      —                                 0
                           Combinations of Alleles                Figure 2: Evaluation of H-IFF for the bit-string
                                                                  00101111 showing hierarchical decomposition. For this
                                                                  bit-string, H-IFF evaluates to 8  6  4  0  18. Note
                                                                  that the maximum obtainable value for each level of the
             Fitness                                              hierarchy is 8, so the maximum value for H-IFF on bit-
                                                                  strings of length 8 is 32. In general, there will be n  1
                                                                  levels of building blocks. Within these levels there will
                        Combinations of Alleles                   be 2n k building blocks of size k, each of which has value
                                                                  k. The optimal bit-strings of length 2n therefore have
Figure 1: Fitness landscapes in different search tasks.           value n  12k . The minimum value for H-IFF on bit-
(top) The needle-in-a-haystack task has a single fitness          strings of length l  2n is l. Such a bit-string contains all
peak, and learning smoothes the search landscape around           building blocks of size 1 but no higher-level blocks.
the peak (adapted from Hinton and Nowlan, 1987, Fig-
ure 1). (bottom) A slice through the fitness landscape of
                                                                  cally, we consider a genome of 32 genes, each of which
H-IFF, showing the multiple fitness peaks and the two
                                                                  may be a 0, 1 or ?. During its lifetime, an individual
maxima at all ones and all zeros (adapted from Watson             tries to ‘learn’ the best setting of the ? genes. Each of
and Pollack, 1999, Figure 1).                                     the ? genes can be set to either 0 or 1, and the resulting
                                                                  bit-string, comprising all 1s and 0s is evaluated with the
                                                                  H-IFF fitness function, described above. We take a very
      Tasks: H-IFF and the Jester’s Cap                           simplistic view of learning (as in Hinton and Nowlan’s
H-IFF (Watson and Pollack, 1999) is a function defined            original simulations), and give the agent N attempts at
on bit-strings of length 2n . The fitness value of a par-         guessing the best setting. The agent tests N replacements
ticular string is defined in terms of hierarchical ‘building      of all of the question marks with random choices of 1s
blocks’ which are sub-strings of the main bit-string. The         and 0s. After N guesses, the best guess (i.e., that which
building block at the highest level of the hierarchy is the       maximizes fitness) is taken and the fitness of the agent
entire bit-string (i.e., all 2n bits). Each building block        is the H-IFF fitness of that guess. (Unlike Hinton and
is recursively divided into two equally-sized blocks, ex-         Nowlan’s simulations, there is no scaling of the fitness
cept for blocks of size one, which cannot be divided. For         based on the number of guesses required.) For example,
a building block to be correctly set, it must consist of          an agent with the genome 0??1 may generate the guesses
either all 1s or all 0s. The value of a correctly set build-      0101, 0111 and 0011 which evaluate to 4, 6 and 8 respec-
ing block of size n is 2n plus the sum of the values of           tively. The highest scoring guess (0011) is taken as the
its two sub-blocks (whose values depend on the sub-sub-           ‘learned’ setting. However, this ‘learned’ setting is not
blocks). Thus, the overall value of a bit-string of length        passed on during reproduction, it is the initial genome,
2n is the sum of values for the building blocks of sizes          0??1 that is used in reproduction.
1 2 4     2n . The optimum bit-strings consist of either
all 0s or all 1s so that they are rewarded for building                    Simulation 1: The Jester’s Cap
blocks of every size. In the simulations in this paper,
we use 2n  32. The evaluation of the H-IFF function is
                                                                  We consider the jester’s cap task with three variations
                                                                  of the amount of learning time available to the agents:
more easily understood by way of example, shown for an            no learning (replicating the H-IFF task), a small amount
8-bit string in Figure 2.                                         of learning (N  10) and a moderate amount of learning
   As described above, the major difference between H-            (N  100). A population comprising 500 individuals is
IFF and the more well known Royal Road (RR) func-                 embedded within a genetic algorithm. In this initial pop-
tion is that RR has a single optimal bit-string (all 1s) and      ulation, 50% of the genes are ?s, 25% are 1s and 25% are
significantly, no local optima other than the global opti-        0s, except in the case without learning, where there are
mum (although there are local plateaus). By comparison,           no ?s and equal proportions of 1s and 0s. In each genera-
H-IFF has two optimal bit-strings and, for bit-strings of         tion, the fitness of each of the agents is determined, after
length l  2n , there are 2l 2 local optima.                     learning when appropriate. These fitness values are used
   In this paper, we apply the learning-based approach of         to determine the parents for the next generation, those
Hinton and Nowlan’s simulations to the H-IFF function.            agents with higher fitness being (probabilistically) more
We call this modified version the jester’s cap. Specifi-          likely to be selected as parents than those with lower fit-

nesses. (We used a sigma-scaled roulette algorithm for                                                               Jester’s Cap
choosing the parents, see Wiles et al. for further details.)                                    195
Each pair of parents is used to generate two new off-                                           190
                                                                Final mean population fitness
                                                                                                185
spring using single point crossover (zero mutation). In
                                                                                                180
this recombination technique a ‘cut-point’ is selected at
                                                                                                175
a random position on the genome. One offspring is gen-                                          170
erated by joining the genes to the left of the cut-point                                        165
in parent 1 to the genes to the right of the cut-point in                                       160
parent 2. The second offspring is formed by the reverse                                         155
combination.                                                                                    150
   The idea behind this evolutionary approach is that in                                        145
the initial population some agents will, by chance, hap-                                        140
                                                                                                      No Learning   10 Learning Steps   100 Learning Steps
pen to have lower-level modules (or the ability to learn
them). These agents will have a slightly higher fitness
than the rest of the population, and will be selected as       Figure 3: Performance of populations on the jester’s cap
parents more often. When two such agents are paired to-        task under three conditions. Each point represents the
gether for reproduction, it is quite likely that one of the    final mean fitness of a population. Error bars show first
offspring will have two modules, one from each parent.         and third quartile and the medians are linked. Note that in
These modules may also combine to form a higher-level          the no learning (left) and moderate learning (right) con-
module. In later generations, even larger modules can          ditions, the error bars are obscured because first, second
form, so that after a sufficient number of generations the
                                                               (median) and third quartiles are all equal. Note also that
low-level modules that were initially scattered randomly
throughout the population have combined in single in-          many populations have identical mean fitness and tend
dividuals. The genes of these individuals then begin to        to cluster around a set of discrete values because of the
dominate the population due to an enhanced fitness, so         nature of the H-IFF function.
that every individual has the high-level modules.
   These simulations were repeated 100 times for each
                                                               needed to search module space, not Hamming space. As
condition, varying the initial random seed. During the
                                                               posed, the jester’s cap assumes that low level modules
course of a simulation, the mean fitness of the population
                                                               are trivial to find. We next consider a sparse version of
is monitored. Simulations were run for a maximum of
                                                               the task in which they are not so readily revealed.
2000 generations, or until the population converged.
Results                                                                                         Simulation 2: The Sparse Jester’s Cap
In all three conditions, a sizeable proportion of the 100      In the jester’s cap simulations, rewards were given for
populations converged on genotypes of maximum fit-             modules of all levels (i.e, 1, 2, 4, 8, 16 and 32). In the
ness. On average, trials in which agents were allowed ei-      sparse jester’s cap, we consider rewarding only a subset
ther no learning or a moderate amount of learning outper-      of the levels. For example, in Figure 2 the blocks of size
formed those where less learning was allowed, as shown         2 may not contribute to the overall fitness of the solution.
in Figure 3.                                                   This modification allows us to vary the nature of the task
                                                               from the maximally hierarchical H-IFF function (where
Discussion                                                     all levels rewarded) to the NIAH function (where only
The genetic operators of crossover are maximally suited        the highest level rewarded). Varying the rewarded levels
to the hierarchical structure of the H-IFF problem. Un-        of the H-IFF function changes both the ease with which
surprisingly, crossover works well on this problem.            the initial modules can be found, as well as the ease with
Learning, which one might expect to perform as well or         which the lower-level modules may be combined into the
better, does not match the performance of the genetic op-      next higher-level of module. In the simulations in this
erators alone. This result can be explained by consid-         section, only the building blocks of size 1, 16 and 32
ering the way that learning searches the space. Learn-         are rewarded. With these choices of building-block and
ing in the jester’s cap is a mechanism for searching the       population sizes the smallest non-trivial modules (those
neighborhood as determined by Hamming distance. This           of size 16) are difficult to find (cf. Hinton and Nowlan’s
search is only effective for the lowest level of building      simulations where the module is of size 20). It is thus ex-
blocks. At subsequent levels, local and global minima          pected that learning will substantially assist for the low-
are close in recombination space, but not in Hamming           level modules. We repeated the first series of simulations
space. At these higher levels, learning merely adds dis-       using this alternative fitness function.
tractions to an otherwise successful algorithm, although
adding a sufficient amount of learning can negate any          Results
detrimental effects.                                           Not surprisingly, the populations evolved using the
   In conclusion, learning in this type of hierarchical task   sparse jester’s cap fitness function fared substantially
is no more effective than genetic search because a way is      worse than those evolved with the (standard) jester’s cap,

                                            Sparse Jester’s Cap: Levels 1, 16, 32              formance on H-IFF relate to maintaining a diversity of
                                 100                                                           modules at intermediate levels. The population size of
                                  90                                                           500 in these simulations was clearly adequate for main-
 Final mean population fitness
                                                                                               taining this diversity. Adding learnable alleles increases
                                  80
                                                                                               the search space, without a reciprocal benefit in assisting
                                  70                                                           search.
                                                                                                  Simulation 2 showed that the sparse jester’s cap
                                  60
                                                                                               (1,16,32), a problem intermediate between H-IFF and
                                  50                                                           NIAH, was not amenable to evolutionary search by the
                                  40
                                                                                               GA. The majority of populations only found a single
                                                                                               module (fitness level 48). The smallest module involved
                                  30                                                           16 bits, and the likelihood of finding two such modules
                                       No Learning    10 Learning Steps   100 Learning Steps
                                                                                               in any one population before convergence was minimal.
                                                                                                  In contrast to Simulation 1, in Simulation 2 a small
Figure 4: Performance of populations on the sparse                                             amount of learning (ten steps) marginally improved the
jester’s cap task under three learning conditions. Each                                        success with which populations discovered modules. Ten
point represents the final mean fitness of one population.                                     learning steps is sufficient to effectively search four to
Error bars show first and third quartile which are again                                       five learnable genes, and in this case, learning clearly did
obscured in some cases. Most populations with no learn-                                        provide a reciprocal benefit that more than compensated
ing (left) converged on a final fitness of 48, correspond-                                     for the increase in the search space.
ing to one module. Populations in the moderate learn-                                             Increasing the learning from ten to 100 steps substan-
                                                                                               tially improved the success of populations. All popu-
ing case converged on genomes giving a variety of fit-
                                                                                               lations found at least one module, 75% found two mod-
ness values, indicating some amount of residual learning
                                                                                               ules, and at least 25% reached optimal performance. One
in the genome (right). With a small amount of learn-                                           hundred learning steps is sufficient to search six to seven
ing (center), performance was marginally improved over                                         learnable genes. Although this is only two more than
the no-learning case, although again residual learning re-                                     searched by ten learning steps, it had a demonstrable ef-
mained.                                                                                        fect on performance.
                                                                                                  Simulation 2 demonstrated that in the sparse version of
                                                                                               the jester’s cap, learning is required to discover the mod-
as shown in Figure 4 (note that the y-axes differ be-                                          ules, as in NIAH. As in Hinton and Nowlan’s simula-
tween Figure 3 and Figure 4). In the condition of no                                           tions, populations are unlikely to find high-level modules
learning, most populations (83 of 100) found a single                                          by crossover alone. Learning is able to guide the popu-
module. With a small amount of learning, populations                                           lation towards finding the low level modules, and then
converged on marginally better solutions on average. In                                        crossover combines them. However, the performance is
this condition, many residual question marks remained                                          still not optimal, and room for improvement remains.
in the final populations. As a result, the individuals from                                       In conclusion, there appears to be a role for learning in
these populations could only find modules with some de-                                        situations where crossover is an ineffective search tech-
gree of chance, causing the observed scattering of results                                     nique. Crossover searches module space whereas learn-
in Figure 4. With moderate learning, some populations                                          ing searches Hamming space. In tasks such as the jester’s
converged on the optimal genomes, others converged on                                          cap there is very little need for searching Hamming space
genomes that gave agents the potential of finding the op-                                      and the majority of optimization can be effectively per-
timal solutions (i.e., there was again residual learning),                                     formed in module space. In this task, Hamming search
while others converged on poor genomes. However, the                                           is useful only at the lowest-level of module. For higher-
populations with a moderate degree of learning, on aver-                                       level modules crossover searches through combinations
age, outperformed the populations in the other learning                                        of peaks, rather than traversing the troughs between them
conditions.                                                                                    (Figure 1). Local search provides the wrong operator for
                                                                                               preserving and improving fitness because it spends too
                                       Discussion and Conclusions                              much time in the troughs of fitness space. In the sparse
This paper addressed the problem of how and when                                               jester’s cap, modules must be discovered before search-
learning is an aid to evolutionary search in the jester’s                                      ing module space becomes a viable approach. The diffi-
cap, a hierarchical modular task. Simulation 1 showed                                          culty in finding these modules necessitates local search.
that evolutionary search could efficiently find the opti-                                         NIAH and H-IFF may be viewed as being on alter-
mal solution with no learning. The addition of a small                                         native ends of a spectrum. In the former, the (single)
amount of learning detracted from this performance. A                                          module is difficult to find as there is no partial feedback
moderate amount of learning had no benefit, but did not                                        to guide search. Learning is required to act as a proxy
detract either. It turns out that H-IFF is not a difficult task                                for this partial feedback. In the latter, modules abound
for a GA at the levels of complexity studied in this paper.                                    so the important factor is not finding the modules, but
   The main issues in reaching the highest levels of per-                                      discovering how to put them together. Consequently, the

best search process is one that searches through combi-      Mitchell, M. and Belew, R. K. (1996). Preface to ‘How
nations of modules rather than searching for the modules       learning guides evolution’ by G. E. Hinton & S. J.
themselves. In this case, learning is merely a hindrance.      Nowlan. In Adaptive Individuals in Evolving Popula-
The sparse jester’s cap represents an intermediate point       tions: Models and Algorithms, volume XXVI of Santa
on the spectrum. Modules are sufficiently difficult to find    Fe Institute Studies in the Science of Complexity, pages
so that learning is required to give partial feedback in the   443–446. Addison-Wesley.
search for the lowest-level modules. Once the modules        Mitchell, M., Forrest, S., and Holland, J. H. (1992). The
have been found, recombination can function.                   royal road for genetic algorithms: Fitness landscapes
                                                               and GA performance. Proceedings of the First Euro-
                 Acknowledgements                              pean Conference on Artificial Life, pages 245–254.
We thank Rik Belew and two anonymous reviewers for           Mitchell, M., Holland, J. H., and Forrest, S. (1994).
their constructive feedback on this paper. This research       When will a genetic algorithm outperform hill climb-
has been supported by a CSEE scholarship to JRW.               ing. In Cowan, J. D., Tesauro, G., and Alspector,
                                                               J., editors, Advances in Neural Information Process-
                        References                             ing Systems, volume 6, pages 51–58, San Mateo, CA.
Baldwin, J. M. (1896). A new factor in evolution. Amer-        Morgan Kaufmann Publishers, Inc.
  ican Naturalist, 30:441–451. Reproduced in Belew,          Morgan, L. C. (1896). On modification and variation.
  R. K. & Mitchell, M. (Eds.), Adaptive Individuals            Science, 4:733–740.
  in Evolving Populations. Addison-Wesley, Reading,          Pelikan, M. and Goldberg, D. E. (2000). Hierarchi-
  MA.                                                          cal problem solving by the bayesian optimization al-
Belew, R. K. (1990). Evolution, learning and culture:          gorithm. IlliGAL Report No. 2000002, Illinois Ge-
  computational metaphors for adaptive search. Com-            netic Algorithms Laboratory, University of Illinois at
  plex Systems, 4(1):11–49.                                    Urbana-Champaign, Urbana, IL.
Dawkins, R. (1986). The Blind Watchmaker. Penguin            Pereira, F. B., Machado, P., Costa, E., Cardoso, A.,
  Books.                                                       Ochoa-Rodriguez, A., Santana, R., and Soto, M.
Forrest, S. and Mitchell, M. (1993). Relative building-        (2000). Too busy to learn. In Proc. of the 2000
  block fitness and the building-block hypothesis. In          Congress on Evolutionary Computation, pages 720–
  Whitley, D., editor, Foundations of Genetic Algo-            727, Piscataway, NJ. IEEE Service Center.
  rithms, volume 2, pages 109–126. San Mateo, CA:            Tooby, J. and Cosmides, L. (1994). Origins of domain
  Morgan Kaufmann.                                             specificity: The evolution of functional organisation.
French, R. and Messinger, A. (1994). Genes, phenes and         In Hirschfeld, L. and Gelman, S., editors, Mapping
  the baldwin effect: Learning and evolution in a simu-        the Mind, pages 85–116. Cambridge University Press.
  lated population. In Brooks, R. A. and Maes, P., edi-      Watson, R., Hornby, G., and Pollack, J. (1998). Model-
  tors, Artificial Life IV, pages 277–282.                     ing building-block interdependency. Parallel Problem
Goldberg, D. E. (1989). Genetic Algorithms in Search,          Solving from Nature, proceedings of the Fifth Interna-
  Optimization and Machine Learning.              Addison-     tional Conference, pages 97–106.
  Wesley, Reading, MA.                                       Watson, R. and Pollack, J. (1999). Hierarchically-
Harvey, I. (1993). The puzzle of the persistent question       consistent test problems for genetic algorithms. In
  marks: A case study of genetic drift. In Forrest, S.,        Angeline, P. J., Michalewicz, Z., Schoenauer, M.,
  editor, Proceedings of the Fifth International Confer-       Yao, X., and Zalzala, A., editors, Proceedings of 1999
  ence on Genetic Algorithms, pages 15–22, San Mateo,          Congress on Evolutionary Computation, pages 1406–
  CA. Morgan Kaufmann.                                         1413. IEEE Press.
Hinton, G. and Nowlan, S. (1987). How learning can           Wiles, J., Schulz, R., Bolland, S., Tonkes, B., and Hal-
  guide evolution. Complex Systems, 1:495–502.                 linan, J. Selection procedures for module discovery:
Holland, J. H. (1992). Adaption in Natural and Artificial      Exploring evolutionary algorithms for cognitive sci-
  Systems. MIT Press, 2nd edition.                             ence. This volume.
Holland, J. H. (2000). Building blocks, cohort genetic
  algorithms, and hyperplane-defined functions. Evolu-
  tionary Computation, 8(4):373–391.
Mayley, G. (1996). The evolutionary cost of learn-
  ing. In Maes, P., Mataric, M. J., Meyer, J.-A., Pol-
  lack, J., and Wilson, S. W., editors, Proceedings of
  the Fourth International Conference on Simulation of
  Adaptive behavior: From Animals to Animats 4. MIT
  Press/Bradford Book.
Mitchell, M. (1996). An Introduction to Genetic Algo-
  rithms. MIT Press, Cambridge, MA.

