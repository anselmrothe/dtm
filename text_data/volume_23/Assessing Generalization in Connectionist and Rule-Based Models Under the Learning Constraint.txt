UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Assessing Generalization in Connectionist and Rule-Based Models Under the Learning
Constraint
Permalink
https://escholarship.org/uc/item/4xr5p23p
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 23(23)
Author
Shultz, Thomas R.
Publication Date
2001-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

  Assessing Generalization in Connectionist and Rule-based Models Under the
                                                 Learning Constraint
                                      Thomas R. Shultz (shultz@psych.mcgill.ca)
                                         Department of Psychology; McGill University
                                                Montreal, QC H3C 1B1 Canada
                            Abstract                            predicate-argument form, where arguments can be
                                                                constants, variables, or other propositions.
   Although it is commonly assumed that rule-based                Leaving aside the issue of whether people actually
   models generalize more effectively than do connectionist     generalize as well as such rules do, the claim has
   models, the comparison is often confounded by pitting
                                                                commonly been made that connectionist models rarely
   hand-written rules against learned connections. Three
   case studies from cognitive development show that,
                                                                learn to generalize that well. Indeed, this argument
   under the constraint that both types of models learn their   seems to have been accepted by many connectionists
   representations from equivalent examples, generalization     (e.g., Anderson, 1995), and is at least partly responsible
   is consistently superior in connectionist models.            for the many attempts to improve generalization in
                                                                neural network learning (e.g., Reed & Marks, 1995).
               Generalization Problems                            However, closer inspection reveals a serious
A significant part of the ongoing debate between rule-          confound in this argument. The symbolic rules are often
based and connectionist modeling in psychology has              written by hand, or perhaps merely alluded to, while the
focused on the ability to generalize. A common claim            neural network learns its own connection weights by
from supporters of the classical, symbolic approach is          processing examples. The purpose of this paper is to
that rule-based models are superior because they                remove this confound between representation and
generalize more effectively than do connectionist               learning by requiring both types of model to learn their
models (Ling & Marinov, 1993; Pinker, 1997; Marcus,             representations from equivalent examples. It is already
1998). Generalization is considered important by most           well known that an alternate method of removing this
modelers because it distinguishes understanding of a            confound, by hand-designing neural networks to
problem from mere memorization of solutions.                    explicitly implement rules and variables, also produces
   The generalization ability of rules is often enhanced        excellent generalization (Shastri, 1995).
by the use of variables that can be bound to any number           The learning constraint proposed here is reminiscent
of objects or events. Consider the following rule,              of the developmental tractability constraint proposed by
written in Common Lisp for a production system                  Klahr (1984). In discussing cognitive development,
program. It generates correct responses on some                 Klahr argued that any two plausible, consecutive
Piagetian conservation of number problems:                      developmental states must be integrated in a transition
        ((response more ?x ?y)                                  theory that can transform one state into the other.
         (and                                                   Similarly, and a bit more generally, I propose a
          (initially-same-number ?x ?y)                         constraint that acquired knowledge representations,
          (or (add1 ?x)                                         whether rules or weight vectors, must be learned by a
              (subtract1 ?y))))                                 model in order to be considered plausible. Knowledge
The rule says to conclude that row x has more items             representations that are instead hypothesized to be
than row y if the two rows initially had the same               produced through biological evolution may be dealt
number of items and if one item was subsequently                with by hand designing rule-based and connectionist
either added to row x or subtracted from row y. It has          models as noted earlier, or even more ambitiously, by
plenty of generality because the variables x and y can be       simulated evolution. Covering both inherited and
bound to any rows with any number of items. It could            acquired representations is the more general principle
be made even more general by adding a third variable            that other, non-representational features must be held
n, representing the number of items added or                    constant when assessing generalization ability.
subtracted.                                                     Otherwise claims about superior generalization ability
   More generally, a rule can be defined as a conditional       may be confounded with acquisition issues and possibly
statement in which conjunctively and disjunctively              other differences.
connected conditions, if verified as true, produce a set
of conjunctively connected conclusions. Each condition
and conclusion is a proposition that can be stated in

      Choice of Algorithms and Domains                  comparison, and infant familiarization to sentences in
                                                        an artificial language.
A systematic test of generalization under a learning
constraint should eventually involve many algorithms
and problem domains. To begin this process, this paper
                                                                      Conservation Acquisition
compares one leading connectionist algorithm to one     A recent CC model of conservation acquisition focused
leading rule-learning algorithm in three different      on Piaget’s conservation of number problems (Shultz,
domains of cognitive development.                       1998). In one version of these problems, a child first
  One of the most frequently used connectionist         agrees that two rows have the same number of items,
algorithms in cognitive development, and the principal  and is then asked which row has more after one of the
one used in my laboratory, is cascade-correlation (CC). rows is transformed, for example, by compression.
CC creates feed-forward networks by recruiting new      Children below about six years of age typically judge
hidden units that correlate well with network error and the longer row to have more items, whereas older
installing them in cascaded layers (Fahlman & Lebiere,  children correctly judge the rows to remain equal. The
1990). It has been used to simulate a wide variety of   vast psychological literature on conservation (over 1000
cognitive developmental phenomena, including            studies) has produced a number of well-replicated
conservation (Shultz, 1998), seriation (Mareschal &     regularities. Among them are acquisition (with a sudden
Shultz, 1999), the balance scale (Shultz, Mareschal, &  jump in performance), the problem size effect (with
Schmidt, 1994), shift learning (Sirois & Shultz, 1998), better performance and earlier success on small number
pronoun acquisition (Oshima-Takane, Takane, &           problems than on large number problems), length bias
Shultz, 1999), infant familiarization to rule-governed  in pre-conservation children (choosing the longer row
sentences (Shultz & Bale, 2001), and integration of the as having more), and the screening effect (with young
concepts of velocity, time, and distance for moving     children giving a correct answer to a screened
objects (Buckingham & Shultz, 2000).                    transformation until the screen is removed).
  Choosing an equivalent rule-learning algorithm           CC networks were trained on 420 examples of
encounters the problem that there are not all that many number conservation problems of row lengths and
successful rule-based models of cognitive development,  densities ranging between 2 and 6, with number of
in the sense of implementing developmental transitions. items being the product of length and density. Using
A good case can be made that the largest number of      inputs coding the length and density of each row, both
successful rule-based developmental models have been    before and after the transformation, the identity of the
achieved by the C4.5 algorithm (Quinlan, 1993) and its  transformed row, and the identity of the transformation
immediate predecessor ID3 (Quinlan, 1986). These        (addition, subtraction, compression, and elongation),
include models of English past tense morphology (Ling,  networks learned to judge whether the rows had equal
1994; Ling & Marinov, 1993), the balance scale          numbers or not, after the transformation. Both equal
(Schmidt & Ling, 1996), grammar learning (Ling &        and unequal initial rows were included. Length and
Marinov, 1994), and reading (Ling & Wang, 1996).        density were coded as real numbers, and the other
There is also a simulation of non-conscious acquisition inputs were coded in a localist binary fashion. There
of rules for visual scanning of a matrix (Ling &        were 100 test problems of the same type, not used in
Marinov, 1994), and numerous applications in            training, to assess generalization performance.
engineering and decision support (Quinlan, 1993).          C4.5 was trained with the same examples, learning to
Among alternative symbolic rule-learning algorithms     classify them into three numerical judgments: one row
applied to the same phenomenon, the balance scale,      has more, the other row has more, or both rows have
C4.5 produced an arguably superior model.               the same. C4.5 was equipped with ability to deal with
  C4.5 learns to classify examples described with       continuous, as well as qualitative inputs,1 and to use the
features and values by forming a smallish decision tree option for information gain ratio, which is generally
that can be converted into production rules. It is a    superior to simple information gain (Quinlan, 1993).
greedy (i.e., non-backtracking) algorithm that             Proportions correct on training and test problems,
repeatedly finds the most informative feature with      respectively, were 1.0 and .95 for 20 CC networks, and
which to classify so far unclassified examples.         .40 and .35 for 20 C4.5 trees. For both algorithms,
  There are a number of intriguing similarities between generalization performance (on the test problems) was
C4.5 and CC. Both algorithms use supervised learning    just a bit worse than performance on the training
of examples, focus on largest current source of error,  problems; but training and generalization performance
gradually construct a solution based on what is already was much higher for CC than for C4.5. If the learned
known, and aim for a small solution that generalizes
well. In this paper, I report on generalization         1
                                                          C4.5 finds the gain ratios for each possible cutoff on a
performance of the CC and C4.5 algorithms on the        continuous feature and then chooses the partition of examples
three problems of conservation acquisition, number      with the highest gain ratio in the usual way.

knowledge representation is inadequate, it does not        remaining 50 pairs comprised the test set. The integers
afford good generalization. This makes a pure test of      were coded as real numbers, and there were three
generalization ability difficult. To control for learning  discrete output classes, including ties. Mean proportion
success, proportion correct on the test problems can be    correct on training and test problems, respectively, over
divided by proportion correct on the training problems,    20 runs was 1.0 and 1.0 for CC and .75 and .66 for
creating a generalization ratio. This ratio is .95 for CC  C4.5. The mean generalization ratio of test correct to
and .87 for C4.5.                                          train correct was 1.0 for CC and .89 for C4.5. Not only
   Because a failed model is not by itself very            did CC learn the problem and generalize more
meaningful, I adopted the strategy of changing the input   effectively than did C4.5, but only CC captured the min
coding to C4.5 until learning was successful and then      and distance effects.
evaluating what is required to learn in terms of both         Knowledge representation analysis revealed a
theoretical plausibility and psychological coverage.       sensible solution for CC networks that involved
   Following the lead of other C4.5 modelers (Schmidt      positioning a hyper-plane near the diagonal axis
& Ling, 1996), I coded the length and density input in     designated by x = y, where x and y are the two numbers
relational, rather than absolute terms. For example, was   being compared. The fact that this hyper-plane is
the first row longer or shorter or the same length as the  anchored at the origin and drifts away from the ideal
second row? Although this relational coding produced       diagonal at the higher values generates the min effect.
100% success on training and test problems, it created     The soft boundary created by the sigmoid activation
knowledge representations that are unlike any that have    function in CC networks produces the distance effect.
been reported with children. For example, an English       In contrast, the rules learned by C4.5 made no
gloss of one of the smaller rules is: If the first row is  psychological sense, e.g., If x > 5 and y > 7, then x > y.
longer than the second row before the transformation,         Another coding trick employed by C4.5 modelers
and shorter than the second row after the                  uses the difference between two numbers that are being
transformation, then the first row has more items.         compared (Schmidt & Ling, 1996). Mean proportions
   Because of this exclusive focus on relative length and  correct on training and test problems, respectively, were
density of the rows, there was never any reference to      .902 and .875 for C4.5 difference coding in the
information on the transformation or the identity of       interpolation experiment. This is an improvement, but
transformed row. Nor could the C4.5 models cover any       again there is no coverage of the min and difference
of the various psychological regularities. This is in      effects, and the rules are psychologically inappropriate,
distinction to both the CC model and children,             e.g., If difference > 1, and y > 2, then y > x.
characterized by a shift from concern with how the            In a study of extrapolation, the models were trained
rows look to the nature and identity of the                on pairs of the integers 0-4 and tested on pairs of the
transformation. The CC model also covers all of the        integers 5-9. There is no variation in C4.5 performance
psychological regularities mentioned: sudden jump in       here because training patterns are not randomly selected
acquisition, problem size effect, length bias, and the
                                                           for each run. Training and test results are shown in
screening effect. Thus, although relational input coding
                                                           Table 1. Again, the CC algorithm learns and generalizes
can produce perfect learning and generalization in C4.5,
it creates implausible knowledge representations and       better than the C4.5 algorithm, whether input coding
fails to cover the psychological data. In contrast, the CC uses standard raw integers or differences.
model can learn and generalize effectively from raw
input coding, acquire knowledge representations that        Table 1: Proportion correct and generalization ratio for
are similar to those seen in children, and cover the                              extrapolation.
psychological regularities.
                                                                  Algorithm/coding       Train    Test     Ratio
                Number Comparison                                 CC                      1.00     .99       .99
One of the most basic of numerical skills is that of              C4.5/standard            .56      .40      .71
comparing the size of two numbers. Prominent                      C4.5/difference          .76     .40       .53
psychological regularities in number comparison are the
min and distance effects. The min effect refers to earlier    In conclusion, C4.5 does not learn or generalize well
success and quicker performance the smaller the            with either standard or difference coding of input on
smaller of the two numbers. The distance effect refers     number comparison problems. It also fails to cover the
to earlier success and quicker performance the larger      min and difference effects, and the rules it learns are
the absolute difference between the two numbers.           psychologically implausible. The only apparent way to
   My simulations focus on pairs created from the          get C4.5 to learn appropriate number comparison rules
integers 0-9. In a study of interpolation, a randomly      and generalize effectively is to build those rule
selected 50 pairs comprised the training set and the       conditions into the input coding, in which case there is
                                                           nothing to learn. In contrast, CC learns and generalizes

well, while covering min and difference effects and           consistency effect extends to patterns outside of the
generating reasonable knowledge representations, and it       training range reveals substantial extrapolation ability
does so with raw numerical inputs.                            in these networks. As well, the CC networks exhibited
                                                              the typical exponential decrease in attention to
     Infant Familiarization to Sentences                      familiarization stimuli that are found with infants.
The third case study concerns infant familiarization to
                                                                               60
sentences in an artificial language. A recent paper in
this area has been of particular interest because it                                         Consistent
claimed to have data that could only be accounted for                          50
by rules and variables (Marcus, Vijayan, Rao, &                                              Inconsistent
Vishton, 1999). That study found that 7-month-olds                             40
                                                                  Mean error
attend longer to sentences with unfamiliar structures
than to sentences with familiar structures. Particular                         30
features of the experimental design and some
unsuccessful neural network models allowed the
                                                                               20
authors to conclude that unstructured neural networks
cannot simulate these results. Several unstructured
connectionist models have since disproved that claim                           10
(Shultz & Bale, 2001), but the current focus is on
generalization ability of connectionist and rule-based                         0
models that learn representations of these sentences.                               Inside           Close            Far
   The present simulations focus in particular on                                            Distance from training
Experiment 1 of Marcus et al. (1999). In this
experiment, infants were familiarized to sentences with
an ABA pattern, for example, ga ti ga or li na li. There
were 16 of these ABA sentences, created by combining             Figure 1: Mean error for CC networks simulating
four A words (ga, li, ni, and ta) and four B words (ti,           infant interest in consistent and inconsistent test
na, gi, and la). Subsequently, the infants were tested                                 sentences.
with two novel sentences that were consistent with the
ABA pattern (wo fe wo, and de ko de) and two others             I did C4.5 simulations in several different ways to try
that were inconsistent with ABA in that they followed         to achieve successful learning and generalization. The
an ABB pattern (wo fe fe, and de ko ko). There was also       initial attempt involved a literal symbolic encoding of
a condition in which infants were familiarized instead        each word in the sentences. For example, the word ga
to sentences with an ABB pattern. Here the novel ABB          was coded as the symbol ga. Because there was only
sentences were consistent and the novel ABA sentences         one output class when only one type of sentence was
were inconsistent with the familiarized pattern. Infants      used as in the infant experiment (ABA or ABB), the
attended more to inconsistent than to consistent novel        resulting decision tree had only one leaf labeled with
sentences, suggesting that they were sensitive to             the syntactic class. In other words, if exposed only to
syntactic properties of the sentences.                        ABA sentences, then expect more of the same. This is
   For consistency, I focus on a particular CC model of       not really a rule and it captures none of the gradual
these data (Shultz & Bale, 2001). In this model,              characteristics of familiarization in infants. There is no
sentences were coded by real numbers representing the         variation in any of these C4.5 runs of the familiarization
sonority (vowel likeness) of particular consonants or         problem because each run uses all of the examples,
vowels. An encoder version of CC was used, enabling           rather than a random selection of examples.
the network to learn to reproduce its inputs on its output      The next C4.5 simulation added the 16 ABB
units. Deciding on whether a particular sentence is           sentences to the examples to be classified, in order to
correctly rendered in such networks is somewhat               ensure that rules would be learned. This effectively
arbitrary. A more natural index of performance on             changes the experiment to one of discrimination rather
training and test sentences is mean error, which is           than familiarization. In this case, C4.5 focused only on
plotted in Figure 1. Test patterns inside the range of the    the third word, concluding that the ABA syntax would
training patterns were the same as those used with            be signaled by ga, li, ni, or ta as the third word, whereas
infants. Two additional sets tested extrapolation by          the ABB syntax would be identified by ti, na, gi, or la
using sonority values outside of the training range, by a     as the third word. This is a sensible focus because the
distance that was either close or far. The greater error to   third word does distinguish the two syntactic types,
inconsistent sentences corresponds to the attention           producing a training success rate of 1.0, but it does not
difference found with infants. The fact that this
                                                              reflect Marcus et al.’s (1999) assumptions about infants

comparing the first and third words in each sentence.       (number comparison) and difficult (conservation)
Moreover, because the test words are novel, this            problems, finding structural relations that exist
solution does not enable distinction between consistent     implicitly within training examples, learning rule-like
and inconsistent test sentences. The generalization         functions that are psychologically plausible, covering
success rate is 0, as is the generalization ratio.          psychological effects, and generalizing to novel
   To obtain successful generalization with this kind of    examples, even to the extent of extrapolating outside of
literal symbolic coding in C4.5, it is necessary to code    the training range. A pure comparison of generalization
the input relationally, explicitly representing equality of is difficult because of differences in learning success.
the first and second words, the first and third words,      However, comparison of generalization ratios that scale
and the second and third words. When the first and          test performance by training performance, to control for
third words are the same, then one has an ABA               learning success, consistently showed an advantage for
sentence; when the second and third words are the same      CC over C4.5. This advantage occurred both with
then one has an ABB sentence. This allows perfect           identical input coding for the two algorithms and with a
generalization to novel words, but the problem is that      variety of coding modifications that made it easier for
C4.5 can learn this relation perfectly with only one        C4.5 to learn.
example of each pattern because the entire solution is         Some of the generalization success of CC networks
explicitly presented in the inputs. Infants presumably      can be traced to the use of analog coding of inputs. In
require more examples than that to distinguish these        analog codes, the amount of unit activation varies with
syntactic patterns, reflecting the fact that their inputs   the intensity of the property being represented. Analog
are not coded so explicitly and fortuitously.               codes are well known to facilitate learning and
   C4.5 was also trained with discrimination examples       generalization in artificial neural networks (Jackson,
coded on sonority values as in the CC model. This           1997), and exploratory comparative simulations suggest
model yielded 62.5% of training sentences correct, 0%       that they were important determinants of the present
correct on ABA and ABB test sentences, and a                results. Their use in some of the present simulations can
generalization ratio of 0. Moreover, the rules learned by   be justified by psychological evidence that people also
this model were rather odd, e.g., If C1 < -5, C3 < -5,      employ analog representations, for example, of number.
and C2 > -6, then syntax is ABA, where C1 refers to the        Analog coding is not the entire story, however,
consonant of the first word, C3 is the consonant of the     because of two considerations. One is that not all of the
third word, etc.                                            CC inputs were analog. Some of the inputs to
   In contrast, the knowledge representations learned by    conservation problems that are essential to mature
the CC model were psychologically interesting. The          knowledge representations are coded in a discrete
hidden units were found to use sonority sums of the         binary fashion. A second qualifier is that analog input
consonant and vowel to represent variation in sonority.     codes were insufficient to allow successful learning and
This was achieved first in the duplicated-word category     generalization in C4.5 models, even though C4.5 is
and next in the single-word category. This hidden unit      equipped to deal with continuous inputs.
representation was then decoded with similar weights to        For a learning system to generalize effectively, it
outputs representing the duplicate-word category.           must of course learn the right sort of knowledge
   Summarizing the results of the familiarization           representation. This is why the present results show a
simulations, C4.5 did not show gradual familiarization      close correspondence between success on the training
effects. When the problem was changed to a                  examples and generalization performance. It was
discrimination problem, C4.5 did not learn the proper       typical for performance to be slightly worse on test
rules and did not generalize effectively. With explicit     problems than on training problems, although
relational coding, C4.5 learns and generalizes perfectly,   generalization was considerably worse in some C4.5
but it requires only two examples. When trained with        runs, as indicated by low generalization ratios.
sonority codes, C4.5 does not master the training              Because connectionist models generalized better than
examples, learns inappropriate rules, and does not          rule-based models under the learning constraint in three
generalize. In contrast, CC learns and generalizes well,    different domains, the argument that rule-based models
both inside and outside of the range of the training        show superior generalization is highly suspect.
examples,      and      acquires    sensible     knowledge  However, it is reasonable to ask whether connectionist
representations.                                            models invariably generalize better than rule-learning
                                                            models. Would this finding hold up in different
                        Discussion                          domains and with different learning algorithms?
When learning of knowledge representations is               Obviously, more research is needed, but we are now
required, CC reveals a number of advantages over C4.5:      beyond facile comparisons of hand-written or imagined
familiarizing to a single category, learning both simple    rules to laboriously learned connections.

   Choice of algorithm is a key issue because both           connectionist models. Journal of Artificial
symbolic and neural algorithms may vary considerably         Intelligence Research, 1, 209-229.
in their ability to learn and generalize. Certainly, CC    Ling, C. X., & Marinov, M. (1993). Answering the
benefits from its ability to learn difficult problems that   connectionist challenge: A symbolic model of
are beyond the ability of other neural learning              learning the past tenses of English verbs. Cognition,
procedures and its tendency to build the smallest            49, 235-290.
network necessary to master the problem on which it is     Ling, C. X., & Marinov, M. (1994). A symbolic model
being trained. Likewise, C4.5 benefits from its use of       of the nonconscious acquisition of information.
information gain to select the best feature on which to      Cognitive Science, 18, 595-621.
partition unclassified examples. Both algorithms have      Ling, C. X., & Wang, H. (1996). A decision-tree model
                                                             for reading aloud with automatic alignment and
led the way in their respective class in producing
                                                             grapheme        generation.     Unpublished     paper,
successful simulations of cognitive development.
                                                             Department of Computer Science, University of
Nonetheless, it is important for other algorithms of each    Western Ontario.
type to be tried. It is possible that other rule-learning  Marcus,      G.    (1998).     Rethinking    eliminative
algorithms would have better success in finding more         connectionism. Cognitive Psychology, 37, 243-282.
abstract and thus more general knowledge                   Marcus, G. F., Vijayan, S., Rao, S. B., & Vishton, P. M.
representations than C4.5 does. Although C4.5 is adept       (1999). Rule learning by seven-month-old infants.
at learning from examples, it seems unable to represent      Science, 283, 77-80.
those examples in anything more abstract than the          Mareschal, D., & Shultz, T. R. (1999). Development of
features used in their input descriptions. This limitation   children's seriation: A connectionist approach.
could make learning and generalization difficult.            Connection Science, 11, 149-186.
   Finally, it is important to stress that generalization  Oshima-Takane, Y., Takane, Y., & Shultz, T. R.
ability should not be taken as the ultimate criterion on     (1999). The learning of first and second pronouns in
which to evaluate different cognitive models. Surely, it     English: Network models and analysis. Journal of
is more critical to determine whether a given model          Child Language, 26, 545-575.
generalizes like human subjects do. This is an issue that  Pinker, S. (1997). How the mind works. New York:
has not yet been adequately addressed.                       Norton.
                                                           Quinlan, J. R. (1986). Induction of decision trees.
                  Acknowledgments                            Machine Learning, 1, 81-106.
                                                           Quinlan, J. R. (1993). C4.5: Programs for machine
This research was supported by a grant from the              learning. San Mateo, CA: Morgan Kaufmann.
Natural Sciences and Engineering Research Council of       Reed, R., & Marks II, R. J. (1988). Neurosmithing:
Canada. Yoshio Takane, Alan Bale, and Francois               Improving neural network learning. In M. A. Arbib
Rivest contributed insightful comments on an earlier         (Ed.), The handbook of brain theory and neural
draft.                                                       networks. Cambridge, MA: MIT Press.
                                                           Schmidt, W. C., & Ling, C. X. (1996). A decision-tree
                      References                             model of balance scale development. Machine
                                                             Learning, 24, 203-229.
Anderson, J. A. (1995). An introduction to neural
                                                           Shastri, L. (1995). Structured connectionist models. In
   networks. Cambridge, MA: MIT Press.
                                                             M. A. Arbib (Ed.), The handbook of brain theory and
Buckingham, D., & Shultz, T. R. (2000). The
                                                             neural networks. MIT Press, Cambridge, MA.
   developmental course of distance, time, and velocity
                                                           Shultz, T. R. (1998). A computational analysis of
   concepts: A generative connectionist model. Journal
                                                             conservation. Developmental Science, 1, 103-126.
   of Cognition and Development, 1, 305-345.
                                                           Shultz, T. R., & Bale, A. C. (2001, in press). Neural
Fahlman, S. E., & Lebiere, C. (1990). The cascade-
                                                             network simulation of infant familiarization to
   correlation learning architecture. In D. S. Touretzky
                                                             artificial sentences: Rule-like behavior without
   (Ed.), Advances in neural information processing
                                                             explicit rules and variables. Infancy.
   systems 2 (pp. 524-532). Los Altos, CA: Morgan
                                                           Shultz, T. R., Mareschal, D., & Schmidt, W. C. (1994).
   Kaufmann.
                                                             Modeling cognitive development on balance scale
Jackson, T. O. (1997). Data input and output
                                                             phenomena. Machine Learning, 16, 57-86.
   representations. In E. Fiesler & R. Beale (Eds.),
                                                           Sirois, S., & Shultz, T. R. (1998). Neural network
   Handbook of neural computation. Oxford: Oxford
                                                             modeling of developmental effects in discrimination
   University Press.
                                                             shifts. Journal of Experimental Child Psychology, 71,
Klahr, D. (1984). Transition processes in quantitative
                                                             235-274.
   development. In R. J. Sternberg (Ed.), Mechanisms of
   cognitive development. New York: Freeman.
Ling, C. X. (1994). Learning the past tense of English
   verbs: The symbolic pattern associator vs.

