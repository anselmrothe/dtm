UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
A Connectionist Investigtion of Linguistic Arguments from the Poverty of the Stimulus:
Learning the Unlearnable
Permalink
https://escholarship.org/uc/item/3wv86519
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 23(23)
Authors
Lewis, John D.
Elman, Jeffery L.
Publication Date
2001-01-01
Peer reviewed
 eScholarship.org                                  Powered by the California Digital Library
                                                                    University of California

  A Connectionist Investigation of Linguistic Arguments from the Poverty of
                                   the Stimulus: Learning the Unlearnable
                                             John D. Lewis (jlewis@crl.ucsd.edu)
                            Department of Linguistics, McGill University; 1085 Dr. Penfield Avenue
                                                 Montreal, Quebec H3A1A7 Canada
                                                                    &
                               Center for Research in Language, UC San Diego; 9500 Gilman Dr.
                                                     La Jolla, CA 92093-0526 USA
                                           Jeffrey L. Elman (elman@crl.ucsd.edu)
                               Center for Research in Language, UC San Diego; 9500 Gilman Dr.
                                                     La Jolla, CA 92093-0526 USA
                              Abstract                                that this rejection is a serious error     that UG has been
                                                                      attributed with principles to account for properties of lan-
   Based on the apparent paucity of input, and the non-               guage that are demonstrably learnable from the statistical
   obvious nature of linguistic generalizations, Chomskyan
   linguists assume an innate body of linguistically detailed         properties of the input.
   knowledge, known as Universal Grammar (UG ), and at-                  Chomsky’s celebrated argument for the innateness of
   tribute to it principles required to account for those “prop-      the principle of structure-dependence (Chomsky, 1975)
   erties of language that can reasonably be supposed not to          serves as an example. Chomsky claims that, during
   have been learned” (Chomsky, 1975). A definitive ac-
   count of learnability is lacking, but is implicit in exam-         the course of language acquisition, children entertain
   ples of the application of the logic. Our research demon-          only hypotheses which respect the abstract structural
   strates, however, that important statistical properties of         organization of language, though the data may also
   the input have been overlooked, resulting in UG being              be consistent with structure-independent hypotheses,
   credited for properties which are demonstrably learnable;          i.e. relationships over utterances considered only as
   in contradiction to Chomsky’s celebrated argument for
   the innateness of structure-dependence (e.g. Chomsky,              linearly ordered word sequences. As support for this
   1975), a simple recurrent network (Elman, 1990), given             claim, Chomsky notes that though questions like (1) are
   input modelled on child-directed speech, is shown to learn         apparently absent in the child’s input, questions like (2)
   the structure of relative clauses, and to generalize that          are never erroneously produced         a claim subsequently
   structure to subject position in aux-questions. The result
   demonstrates that before a property of language can rea-              1) Is the man who is smoking crazy?
   sonably be supposed not to have been learned, it is neces-
   sary to give greater consideration to the indirect positive           2) * Is the man who smoking is crazy?
   evidence in the data         and that connectionism can be
   invaluable to linguists in that respect.                           empirically tested and substantiated by Crain and
                                                                      Nakayama (1987, also see Crain 1991). Chomsky sug-
                                                                      gests that it is reasonable to suppose that children de-
                         Introduction                                 rive aux-questions from declaratives, and exposed to
Chomskyan linguists argue that language acquisition                   only simpler structures, might hypothesize either of two
cannot strictly be a matter of learning           the child’s tar-    sorts of rules: a structure-independent rule      i.e. move
get grammar is “hopelessly underdetermined by the frag-               the first ‘is’    or the correct structure-dependent rule.
mentary evidence available” (Chomsky, 1968)                  rather   Chomsky claims that “cases that distinguish the hy-
it must rest on a set of innate linguistic principles; the            potheses rarely arise; you can easily live your whole
goal of the Chomskyan linguist is to determine the con-               life without ever producing a relevant example to show
tents of this set, known as Universal Grammar (UG ).                  that you are using one hypothesis rather than the other
The idea is to attribute to UG all and only the princi-               one” (Piatelli-Palmarini, 1980). The fact that children do
ples required to account for those “properties of lan-                not produce questions like (2), despite that the correct
guage that can reasonably be supposed not to have been                rule is supposedly more complex, and that the learner
learned” (Chomsky, 1975). Learning theory is thus of                  might not encounter the relevant evidence leads Chom-
central importance to the enterprise, but, oddly, a defini-           sky to suggest that “the only reasonable conclusion is
tive account of the notion of learning that Chomskyans                that UG contains the principle that all such rules must
adopt is lacking, and is given only implicitly in the                 be structure-dependent” (Chomsky, 1975).
examples of the principles attributed to UG. Statisti-                   As a number of researchers have noted, however, there
cal approaches, however, and the notions of generaliza-               are several weaknesses in this argument. Slobin (1991),
tion and analogy have been explicitly rejected as irrel-              for instance, points out that the conclusion rests on the
evant (Chomsky, 1975). In this paper we demonstrate                   assumption that aux-questions are derived from declar-

atives by movement         an assumption which lacks jus-                                Next Word
tification     as well as on the equally questionable as-
sumption of the autonomy of syntax. The argument
has also been widely criticized for its reliance on the                         Hidden
extremely limited conception of learning as hypotheses
generation and testing. And the premise that the rel-                                               Context
evant evidence is not available to children has repeat-
edly been argued to most likely be false. As Samp-
son (1989) points out, evidence to distinguish the two                                 Current Word
hypotheses is provided by any utterance in which any
auxiliary precedes the main clause auxiliary; thus evi-        Figure 1: An SRN . Solid lines represent full connectiv-
dence is available not only in questions like “Is the jug      ity; the dashed line indicates unit-to-unit connections.
of milk that’s in the fridge empty?” (from Cowie, 1998),       The unlabeled layers are reduction layers.
but also “Is the ball you were speaking of in the box with
the bowling pin?”, or “Where’s this little boy who’s full     rization frames, and selectional restrictions) from reg-
of smiles?”, or even “While you’re sleeping, shall I make     ularities in the input. The network learned the struc-
the breakfast?” None of these forms seem to be of the         ture of such sentences so as to predict the correct agree-
sort that a person might go for long without encounter-       ment patterns between subject nouns and their corre-
ing; the latter three examples, in fact, are taken from the   sponding verbs, even when the two were separated by
CHILDES database,1 and Pullum and Scholz (2001) es-           a relative clause with multiple levels of embedding, e.g.
timate that such examples make up about one percent of        boys who like the girl who Mary hates hate Mary.2 ,3
a typical corpus.                                                Such networks have also been shown to go beyond
    These are strong criticisms, but a conclusive counter-    the data in interesting ways. Elman (1998) and Morris
argument, or an alternate account of the acquisition of       et al. (2000) showed that SRNs induce abstract gram-
aux-questions remains to be given. This paper builds on       matical categories which allow both distinctions such as
recent work with simple recurrent networks (SRNs; El-         subject and object, and generalizations such that words
man 1990) to close this gap      i.e. to provide a proof that which have never occurred in one of these positions are
the correct form of aux-questions is learnable from data      nonetheless predicted to occur, if they share a sufficient
uncontroversially available to children.                      number of abstract properties with a set of words which
    Figure 1 shows the general structure of an SRN. The       have occurred there.
recurrent connections from the hidden layer to the con-          Together these results suggest that an SRN might
text layer provide a one-step state memory. At each time      be able to learn the structure of relative clauses, and
step the activation values of each of the hidden units is     generalize that structure to subject position in aux-
copied to the corresponding unit in the context layer, and    questions      and thus to learn the aspect of grammar in
the connections from the context layer back to the hidden     question despite not having access to the sort of evidence
layer make these values available as additional inputs at     that has been assumed necessary. This paper reports on
the next time step. The network receives its input se-        simulations which show that this is the case. An initial
quentially, and at each step attempts to predict the next     experiment verifies that the two results combine in the
input. At the outset of training, the connection weights      required way; then an SRN is shown to generalize from
and activation values are random, but to the extent that      training sets based on CHILDES data to predict (1), but
there are sequential dependencies in the data, the net-       not (2). This result clearly runs counter to Chomsky’s ar-
work will reduce its prediction error by building abstract    gument, and thus both draws into question the validity of
representations that capture these dependencies. Struc-       poverty of the stimulus arguments in general, and shows
tured representations thus emerge over time as a means        that neural networks provide a means of assessing just
of minimizing error.                                          how impoverished the stimulus really is.
    Elman (1991, 1993) provided such a network with a
corpus of language-like sentences which could be ei-                   Abstractions and Generalization
ther simple (transitive or intransitive), or contain mul-     Training sets similar to those used by Elman (1991,
tiply embedded relative clauses (in which the head noun       1993) were used to test whether an SRN would gen-
could be either the subject or object of the subordinate      eralize to predict relative clauses in subject position in
clause). The input was presented as word sequences,           aux-questions from data which contained no such ques-
where words were represented as orthogonal vectors a          tions. An artificial grammar was created such that it gen-
localist representation so that no information about ei-      erated a) aux-questions of the form ‘AUX NP ADJ?’,
ther the words or the grammatical structure was supplied;         2 The network succeeded only if either the input was struc-
thus the network had to extract all information (e.g. the
                                                              tured, or the network’s memory was initially limited, and de-
grammatical categories, number agreement, subcatego-          veloped gradually.
                                                                  3 An SRN ’s performance with such recursive structures has
    1 The second through fourth examples are from Brown’s     also been shown to fit well to the human data (Christiansen and
Adam, Korman’s’ St, and Manchester’s Anne, respectively.      Chater, 1999).

                                                                                                                                                               ?
and b) sequences of the form ‘Ai NP Bi’, where Ai and
Bi were of varying content and length. Proper names and
NPs of the form ‘ DET ( ADJ) N ( PP)’ were generated in
                                                                                              NOUN sg
both types, and NPs with relative clauses were generated
for the ‘Ai NP Bi’ type, but were restricted from appear-                        NAME
                                                                       Ai
                                                                                                                                 AUX sg
ing in aux-questions. Some representative examples are                                                        ADJ
                                                                                                                                          PARTICIPLE
                                                                        AUX sg
given in Figure 2.
                                                                                                                    PREP
A i Mummy Bi                       is Mummy beautiful?
A i the dog Bi                     is the dog hungry?                                   DET             ADJ                                             ADJ
                                                                                                                           REL
A i the little girl Bi             is the little girl pretty?
A i the cat on the mat Bi          is the cat on the mat fat?
A i the boy who is smiling Bi       *
                                                                       Is        the          boy              who               is smoking            crazy   ?
 Figure 2: Examples of the various types of utterances
 generated by the artificial grammar.
                                                                     Figure 3: The SRN’s categorized predictions for the
                                                                     test sentence “Is the boy who is smoking crazy?” Target
   A three-stage training set was generated from this                words appear under the network’s predictions; and the
grammar, with the degree of complexity in NPs increas-               strength of the predictions is represented vertically.
ing at each stage, and the percentage of aux-questions de-
creasing crudely approximating the structure of child-              relative clause.5 The network does not, of course, make
directed speech. Names constituted 80% of the NPs in                the predictions corresponding to the ungrammatical form
the first set, and the remaining 20% was shared among               in (2)     i.e. the network does not predict a participle
the other NP forms (such that the more complex the form,            following ‘who’; the training sets do not contain cop-
the fewer the instances of it), with relative clauses mak-          ula constructions, and so there can be no hypothesis of
ing up only 1%; there were 40% aux-questions, and 60%               a movement derivation. Rather, the network has appar-
‘Ai NP Bi’ forms. In the second set, names constituted              ently formed an abstract representation of NPs which in-
70% of the NPs, relative clauses made up 2.5% of the re-            cludes NPs with relative clauses. That this is so is shown
mainder, and the percentage of aux-questions decreased              by the networks prediction of an adjective when pre-
to 30%. And in the third set, 60% of the NPs were                   sented with ‘is the boy who is smoking           ’; the se-
names, relative clauses made up 5% of the remainder,                quence ‘. . . PARTICIPLE ADJ . . . ’ never occurs in the
and the percentage of aux-questions decreased to 20%.               training sets, and thus the prediction indicates that the
Each training set consisted of 50,000 examples. An SRN              network has formed an abstract representation of aux-
was trained on each set successively, for 10 epochs each,           questions, and generalized over the NP forms.
and tested with the structures in (1) and (2) after each               That the data available to children are sufficient to
epoch.4 The network received the input in the same form             provide for this generalization, however, remains to be
as used by Elman (1991, 1993), i.e. a localist representa-          shown.
tion was used, and the data was presented one word at a
time.                                                                                         Child-Directed Speech
   Figure 3 shows the networks predictions (after the
                                                                    There are a number of features of child-directed speech
third stage of training) for successive words of the ques-
                                                                    that run counter to the notion that the child’s input is
tion “Is the boy who is smoking crazy?” As should be
                                                                    “meager and degenerate” (Chomsky, 1968)           i.e., that
expected, the network predicts an AUX as a possible first
                                                                    appear to be important for language acquisition, and par-
word, a name or a DET as a continuation when presented
                                                                    ticularly for the issue at hand. Complexity increases
with ‘is’, and a noun or an adjective as possibilities after
                                                                    over time       which has been shown to be a determi-
‘is the’. These sequences all occur in the training sets.
                                                                    nant of learnability (e.g. Elman, 1991, 1993) and there
But, following presentation of ‘is the boy’, not only is
                                                                    are also arguably meaningful shifts in the distribution of
an adjective or a preposition predicted, but also a rela-
                                                                    types, and the limitations on forms.
tivizer     a sequence which never occurs in the train-
                                                                       The increasing complexity of the child’s input is es-
ing sets. And upon presentation of ‘who’ the network
                                                                    pecially relevant to the problem here, since it is directly
predicts an AUX , and when given ‘is’, predicts a par-
                                                                    linked to the frequency of occurrence of relative clauses.
ticiple; the network has thus generalized to predict the
                                                                        5 The fact that the network predicts ‘who’ given ‘is the boy’
   4 The  networks were simulated with LENS (Rohde, 1999),          is, on its own, not enough — early in training, the network will
and trained with a fixed learning rate of 0.01, using a variation   make this prediction, but when presented with ‘who’ will pre-
of cross entropy which assigned smaller errors for predicting       dict a ‘?’, apparently mistaking the relativizer for an adjective.
incorrectly than for failure to predict. The architecture shown     That the network is predicting a relative clause is shown by the
in Figure 1 is used, with 100 input and output units, 50 units      fact that it predicts ‘is’ when subsequently given ‘who’, and a
in the reduction layers, and 500 units in both the hidden and       participle when then given ‘is’. Since participles are restricted
context layers.                                                     to only occur in relative clauses, the latter is decisive.

Complexity in the child’s input is introduced in a way            almost never used; the aux-questions in child-directed
akin to the staged presentation of data used to train the         speech overwhelmingly use proper names, pronouns, de-
network in the experiment described above; Figure 4               ictics, e.g. ‘Is that . . . ’, and other such forms which
charts the occurrences of tagged relative clauses            i.e. do not provide the correct context for a relative clause.
marked with ‘who’ or ‘that’           found in child-directed     Thus, given the low frequency of relative clauses in gen-
speech in the CHILDES’ Manchester corpus (Theakston               eral, one should expect them to almost never occur in
et al., 2000). Pronominal relatives (e.g., ‘the girl you          subject position.
like’) show a similar increase, and occur approximately              These are ideal conditions for an SRN . The target
as frequently. And prepositional phrases increase in fre-         generalization is supported by the appearance of relative
quency slightly more dramatically; they seem to occur             clauses in all other positions in which noun phrases oc-
approximately twice as often as relatives.6                       cur, and making the generalization incurs little cost since
                                                                  the context in which the generalization applies seldom
                                                                  occurs. If this were not the case, and questions like ‘Is the
                                                                  boy crazy?’ were common, then the generalization would
                                                                  be threatened       each such occurrence would produce a
                                                                  false prediction which backpropogation would attempt to
                                                                  eliminate.
 Figure 4: The percentage of NPs that contain relative
 clauses, for each month, averaged over all twelve chil-
 dren in the Manchester corpus.
   The difference between the distribution of types in
child-directed speech and speech between adults is also            Figure 5: The percentage occurrence of various forms,
potentially significant. Child-directed speech contains            at three stages, averaged over all children.
a much greater proportion of questions              estimated at
about one third of the child’s input (Hart and Risley,
1995; Cameron-Faulkner et al., 2001)             and thus there          Motherese and the Generalization
is more of a balance between types. This may be criti-
cal in establishing the multiple roles that, e.g.auxiliaries,     Training sets generated on the basis of this analysis were
can take on; and also to reserve representational space           used to determine if an SRN would generalize to pre-
for the the large variety of question forms. Figure 5             dict (1), but not (2) from input of this sort. As before, the
shows the percentages of copula constructions, subject-           training sets contained aux-questions of the form ‘AUX
                                                                  NP ADJ?’; but here the ‘ Ai NP Bi’ forms were elimi-
predicate forms (e.g., transitives and intransitives), and
wh-, do-, and aux-questions for representative months             nated, and copula constructions, subject-predicate forms,
near the beginning, middle, and end of the time period            and wh- and do-questions were added. The prohibition
covered by the Manchester corpus.                                 on NPs with relative clauses in aux-questions extended
   And finally, aux-questions in the child’s input not only       also to wh- and do-questions          i.e. NPs with relative
lack relative clauses in subject position, but are limited in     clauses could occur in object position in these forms, but
a way that both predicts this absence, and potentially al-        not in subject position. Thus these training sets also con-
lows for the correct generalization to be formed. In child-       tained no evidence of the sort assumed to distinguish the
directed speech, aux-questions with a determiner in the           structure-dependent hypothesis. Some examples from
subject noun phrase          like ‘Is the boy crazy?’        are  these training sets are given in Figure 6. The propor-
                                                                  tions of these general types, and the frequency of relative
    6 A precise count of the prepositional phrases has not been   clauses and prepositional phrases, were manipulated in
made — in part because of the lesser significance to the cur-     each portion of the training set to match with successive
rent research issue, and in part because it is considerably more  portions of the Manchester data          e.g., the type distri-
problematic to determine whether or not a prepositional phrase    butions can be read directly from figure 5. And, as per
is within a noun phrase. But, (Cameron-Faulkner et al., 2001)
analyzed a sample from this same corpus, and they report that     the observation of the previous section, noun phrases in
prepositional phrases make up about 10% of all fragments,         aux-questions were restricted to be, almost exclusively,
which may be indicative of their general frequency.               names. The three training sets again consisted of 50,000

 Mummy is beautiful.                 is Mummy beautiful?
 the little boy bites.               is the little boy nice?
 the dog likes Mummy.                is the dog hungry?
 does Mary smoke?                     .
 who likes Mary?                      .
 who does Mary like?                  .
 who likes the cat on the mat?
 who does the girl at the shop like?
 does the cat on the mat scratch?
 does the little girl like the boy who is smiling?
 Figure 6: Examples of the various types of utterances
 generated by the artificial grammar.
                                                                     Figure 7: The sum-squared error after each word of the
examples each; and again the network was trained for 10              test sentence “Is the boy who is smoking crazy?” at the
epochs on each set, and was tested with the structures               end of each stage of training.
in (1) and (2) after each epoch.
   Figures 7 and 8 chart the sum-squared error for (1)
and (2) after each stage of training. As the figures show,
the network succeeds in generalizing to predict (1), and
generates significant error and progressively larger er-
ror      at several points, when presented with (2).7 The
reasonably small error generated by the network when
presented with ‘who’ in the context of ‘is the boy ’
shows that the relativizer is predicted. And the contrast in
the errors generated by the subsequent presentation of ei-
ther ‘is’ or ‘smoking’ shows clearly that the network has
learned to predict an AUX after a relativizer, rather than
entertaining the possibility of it’s extraction, as in (2).
Note, as well, that this contrast is monotonically increas-
ing       at no point in training does the network predict
a participle to follow the relativizer. And, for (1), the
network’s error is quite low for each successive word,               Figure 8: The sum-squared error after each word of the
including the presentation of the adjective after the par-           test sentence “Is the boy who smoking is crazy?” at the
ticiple, despite that ‘. . . PARTICIPLE ADJ . . . ’ never            end of each stage of training.
occurs in the training sets. In contrast, for (2), as well as
the error produced by the presentation of ‘smoking’, the
network also generates a substantial error upon the sub-            in relative clauses. These erroneous predictions, how-
sequent presentation of ‘is’; And though when presented             ever, gradually erode. And it is worth noting that they
with ‘is the boy who smoking is ’ the network success-              would be correct for a more realistic grammar.
fully predicts an adjective, the success is illusory: when             The error associated with the adjective following the
subsequently presented with ‘crazy’ the network’s pre-              participle most likely has a similar source. Relative
dictions are somewhat random, but a period is predicted             clauses occur only in either sentence final position, or
more strongly than a question mark.                                 preceding an auxiliary or a verb; thus the network ini-
   The network does, however, have some difficulties                tially expects participles to be followed by either a
with this input. Although the grammar restricts relative            verb, a period, a question mark, or most prominently,
clauses to the form ‘REL AUX VERBing’, the network                  an auxiliary. Again the problem is somewhat persis-
persists in predicting noun phrases and adjectives after            tent, but is gradually resolved; by the end of the third
the auxiliary       presumably because the ‘is’ that occurs         stage such predictions, though remaining, are substan-
in initial position in aux-questions, followed by a noun            tially weaker than the correct predictions      thus, ar-
phrase, and the ‘is’ in declaratives, followed by an adjec-         guably, not truly problematic. And it is plausible that
tive, are relatively more frequent in the data than the ‘is’        such errors would not arise were the grammar to be
                                                                    made yet more realistic. The grammar used here con-
    7 The SRN responsible for these results incorporates a vari-
                                                                    tained little variation in terms of either NP types, syn-
ant of the developmental mechanism from (Elman, 1993). That         tactic structures, or lexical items, and thus generaliza-
version reset the context layer at increasing intervals; the ver-
sion used here is similar, but does not reset the context units un- tions were based on a quite limited set of distributional
less the network’s prediction error is greater than a set threshold cues. Lifting the artificial limitations on the grammar
value.                                                              might also help to eliminate such errors: questions like

‘what’s the lady who was at the house called?’               in Christiansen, M. and Chater, N. (1999). Toward a con-
Manchester’s ruth28a.cha            are not only evidence of      nectionist model of recursion in human linguistic per-
the sort assumed not to be available, but also data which         formance. Cognitive Science, 23(2):157–205.
discourage these sorts of false predictions.                    Cowie, F. (1998). What’s Within? Nativism Reconsid-
   But, such errors are also potentially meaningful. The          ered. Oxford University Press.
most prominent and persistent of the errors is the predic-      Crain, S. (1991). Language acquisition in the absence of
tion of an auxiliary following the participle, i.e., ‘is the      experience. Behavioral and Brain Sciences, 14:597–
boy who is smoking is . . . ’; in fact an auxiliary is pre-       650.
dicted as a possible continuation after any NP, e.g.,‘is
                                                                Crain, S. and Nakayama, M. (1987). Structure depen-
the boy is . . . ’. And this is an error that children make as
                                                                  dence in grammar formation. Language, 63:522–543.
well (Crain and Thornton, 1998).
                                                                Crain, S. and Thornton, R. (1998). Investigations in Uni-
                         Discussion                               versal Grammar: A Guide to Experiment’s on the ac-
                                                                  quisition of Syntax and Semantics. MIT Press.
The objective here was to provide a proof that the struc-
ture of aux-questions is learnable from the input available     Elman, J. (1990). Finding structure in time. Cognitive
to children. To make the results convincing, we have              Science, 14:179–211.
been careful to avoid providing the network with input          Elman, J. (1991). Distributed representations, simple re-
that could be controversial with respect to its availabil-        current networks, and grammatical structure. Machine
ity, and have represented the input in a way that encodes         Learning, 7:195–225.
no grammatical information beyond what can be deter-            Elman, J. (1993). Learning and development in neural
mined from its statistical regularities.                          networks: The importance of starting small. Cogni-
   The fact that a neural network generalizes to make the         tion, 48:71–99.
correct predictions from input represented in this way,         Elman, J. (1998). Generalization, simple recurrent net-
and modeled on child-directed speech             but limited to   works, and the emergence of structure. In Gerns-
contain no data of what has been considered the rele-             bacher, M. and Derry, S., editors, Proceedings of the
vant sort         shows that poverty of the stimulus argu-        20th Annual Conference of the Cognitive Science So-
ments must give greater consideration to the indirect ev-         ciety, Mahway, NJ. Lawrence Erlbaum Associates.
idence available to the child. The statistical structure of     Gomez, R. and Gerken, L. (1999). Artificial grammar
language provides for far more sophisticated inferences           learning by one-year-olds leads to specific and abstract
than those which can be made within a theory that con-            knowledge. Cognition, 70:109–135.
siders only whether or not a particular form appears in
                                                                Hart, B. and Risley, T. (1995). Meaningful Differences
the input. And there is a growing body of evidence that
                                                                  in the Everyday Experiences of Young Children. Paul
children, not only neural networks, make use of the sta-
                                                                  H. Brookes, Baltimore, MD.
tistical properties of the input in acquiring the structure
of language (e.g. Aslin et al., 1998; Gomez and Gerken,         Morris, W., Cottrell, G., and Elman, J. (2000). A connec-
1999). Thus learnability arguments cannot ignore those            tionist simulation of the empirical acquisition of gram-
properties.                                                       matical relations. In Wermter, S. and Sun, R., editors,
   But discovering what those properties are, and deter-          Hybrid Neural Systems. Springer Verlag, Heidelberg.
mining their potential worth in language acquisition is         Piatelli-Palmarini, M. (1980). Language and Learning:
difficult. This work shows that neural networks provide           The debate between Jean Piaget and Noam Chomsky.
a means of dealing with this problem. As demonstrated             Harvard University Press, Cambridge, MA.
here, neural networks can be used to assess just how im-        Pullum, G. and Scholz, B. (2001). Empirical assessment
poverished the stimulus really is, and so can be invalu-          of stimulus poverty arguments. The Linguistic Review.
able to linguists in establishing whether or not a property       to appear.
of language can reasonably be assumed not to have been          Rohde, D. (1999). Lens: The light, efficient network sim-
learned.                                                          ulator. Technical Report CMU-CS-99-164, Carnegie
                                                                  Mellon University, Department of Computer Science,
                         References                               Pittsburgh, PA.
Aslin, R., Saffran, J., and Newport, E. (1998). Compu-          Sampson, G. (1989). Language acquisition: Growth or
   tation of conditional probability statistics by 8-month        learning? Philosophical Papers, 18:203–240.
   old infants. Psychological Science, 9:321–324.               Slobin, D. (1991). Can Crain constrain the constraints?
Cameron-Faulkner, T., Lieven, E., and Tomasello, M.               Behavioral and Brain Sciences, 14:633–634.
   (2001). A construction based analysis of child directed      Theakston, A., Lieven, E., Pine, J., and Rowland,
   speech. forthcoming.                                           C. (2000). The role of performance limitations in
Chomsky, N. (1968). Language and Mind. Brace &                    the acquisition of ’mixed’ verb-argument structure at
   World, New York.                                               stage 1. In Perkins, M. and Howard, S., editors, New
Chomsky, N. (1975). Reflections on Language. Pantheon             Directions in Language Development and Disorders.
   Books, New York.                                               Plenum.

