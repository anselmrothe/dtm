UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Towards a Theory of Semantic Space
Permalink
https://escholarship.org/uc/item/0wk159m0
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 23(23)
Author
Lowe, Will
Publication Date
2001-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

                                   Towards a Theory of Semantic Space
                                               Will Lowe (wlowe02@tufts.edu)
                                                     Center for Cognitive Studies
                                                 Tufts University; MA 21015 USA
                           Abstract                                             Motivating Semantic Space
                                                                    Firth (1968) observed that “you shall know a word by
   This paper adds some theory to the growing literature            the company it keeps”. If we interpret company as lex-
   of semantic space models. We motivate semantic space
   models from the perspective of distributional linguistics        ical company, the words that occur near to it in text or
   and show how an explicit mathematical formulation can            speech, then two related claims are possible. The £rst
   provide a better understanding of existing models and            is unexceptional: we come to know about the syntactic
   suggest changes and improvements. In addition to pro-            character of a word by examining the other words that
   viding a theoretical framework for current models, we
   consider the implications of statistical aspects of language     may and may not occur around it in text. Syntactic theory
   data that have not been addressed in the psychological           then postulates latent variables e.g. parts of speech and
   modeling literature. Statistical approaches to language          branching structure, that control the distributional prop-
   must deal principally with count data, and this data will        erties of words and restrictions on their contexts of occur-
   typically have a highly skewed frequency distribution due        rence. The second claim is that we come to know about
   to Zipf’s law. We consider the consequences of these
   facts for the construction of semantic space models, and         the semantic character of a word by examining the other
   present methods for removing frequency biases from se-           words that may and may not occur around it in text.
   mantic space models.                                                 The intuition for this distributional characterization of
                                                                    semantics is that whatever makes words similar or dis-
                                                                    similar in meaning, it must show up distributionally, in
                       Introduction                                 the lexical company of the word. Otherwise the suppos-
There is a growing literature on the empirical adequacy             edly semantic difference is not available to hearers and it
of semantic space models across a wide range of sub-                is not easy to see how it may be learned.
ject domains (Burgess et al., 1998; Landauer et al., 1998;              If words are similar to the extent that they occur in
Foltz et al., 1998; McDonald and Lowe, 1998; Lowe                   the similar contexts then we may de£ne a statistical re-
and McDonald, 2000). However, semantic space mod-                   placement test (Finch, 1993) which tests the meaning-
els are typically structured and parameterized differently          fulness of the result of switching one word for another
by each researcher. Levy and Bullinaria (2000) have ex-             in a sentence. When a corpus of meaningful sentences is
plored the implications of parameter changes empirically            available the test may be reversed (Lowe, 2000a), and un-
by running multiple simulations, but there has up until             der a suitable representation of lexical context, we may
now been no work that places semantic space models                  hold each word constant and estimate its typical sur-
in an overarching theoretical framework; consequently               rounding context. A semantic space model is a way of
there there are few statements of how semantic spaces               representing similarity of typical context in a Euclidean
ought to be structured in the light of their intended pur-          space with axes determined by local word co-occurrence
pose.                                                               counts. Counting the co-occurrence of a target word with
                                                                    a £xed set of D other words makes it possible to position
   In this paper we attempt to develop a theoretical
                                                                    the target in a space of dimension D. A target’s position
framework for semantic space models by synthesizing
                                                                    with respect to other words then expresses similarity of
theoretical analyses from vector space information re-
                                                                    lexical context. Since the basic notion from distributional
trieval and categorical data analysis with new basic re-
                                                                    linguistics is ‘intersubstitutability in context’, a semantic
search.
                                                                    space model is effective to the extent it realizes this idea
   The structure of the paper is as follows. The next sec-
                                                                    accurately.
tion brie¤y motivates semantic space models using ideas
from distributional linguistics. We then review Zipf’s
law and its consequences the distributional character of                                    Zipf’s Law
linguistic data. The £nal section presents a formal de£-            The frequency of a word is (approximately) proportional
nition of semantic space models and considers what ef-              to the reciprocal of its rank in a frequency list (Zipf,
fects different choices of component have on the result-            1949; Mandelbrot, 1954). This is Zipf’s Law. Zipf’s
ing models.                                                         law ensures dramatically skewed distributions for almost

all statistics applied to language; the power scaling en-      distributionally related to one another only through their
sures that the majority of words occur very infrequently,      syntactic properties e.g. by the fact that they are both
creating a severe sparse data problem, and that the top        nouns. For simplicity we ignore any residual syntactic
few most frequent words constitute the majority of all to-     dependence and model their empirical frequencies f (t1 )
kens. For example, the 10 most frequent word stems, or         and f (b) as independent binomially distributed random
lemmas, in the 100M word British National Corpus are           variables
‘the’, ‘be’, ‘of’, ‘and’, ‘to’, ‘a’, ‘in’, ‘have’, ‘that’ and
‘it’, constituting slightly over one quarter of all tokens                       f (t1 )      B (p(t1 ), N)
in the corpus (25974687 / 99985962 0.26). Also the                                f (b)       B (p(b), N).
most frequent words of English are grammatical functors
or closed class words (Cann, 1996), which although vi-         In this idealization t1 and b are perfectly distributionally
tal to syntax, are typically uninformative with respect to     independent so f W (b,ti ) = W N p(b,t1 ) = W N p(t1 )p(b)
word meaning. Much of the next sections will be devoted        (this is just the expected co-occurrence frequency
to dealing with the distributional effects of Zipf’s law.      summed over each possible position in the window).
    To introduce some notation, semantic space mod-               The fact that the expected co-occurrence count under
els typically represent the distributional context of each     independence is linear in the probability of t1 leads to
word t in terms of a set of representative ‘context’ words     a problem in any model that sets A((b,ti ) = f W (b,ti ),
b1 . . . bD . t’s distributional pro£le is then represented by e.g. the Hyperspace Analogue to Language (HAL; Lund
a vector of co-occurrences v where vi is a function of         et al., 1995). Even if t1 and t2 are unrelated, if p(t1 )
 f W (bi ,t), the number of times bi occurs in a window W      p(t2 ) then their vectors will contain elements with simi-
words either side of t in a corpus of N words. For future      lar magnitudes. This implies that any similarity measure
reference f (t) is the occurrence frequency of t in the cor-   applied to the vectors will judge them to be similar. Con-
pus, p(t) is the probability of t, often estimated by t N,     versely if they are related but p(t1 )       p(t2 ) then their
and pW (bi ,t) is the probability of seeing bi and t together  vectors will contain elements with widely differing mag-
in a window of size W .                                        nitudes, simply due to their differing occurrence proba-
                                                               bility. Zipf’s Law threatens that any difference in distri-
                        Semantic Space                         butional pro£le available in f W (b,ti ) may be swamped
A semantic space model is method of assigning each             by the effect of a difference in occurrence probability.
word in a language to a point in a real £nite dimensional         The upshot for models such as the HAL that use vec-
vector space. Formally it is a quadruple A, B, S, M :          tors of counts that are not corrected for chance is that dis-
    B is a set b1...D of basis elements that determine the     tances will have a frequency bias. That is, proximity on
dimensionality D of the space and the interpretation           semantic space will be partly due to distributional simi-
of each dimension. B is often a set of words (Lund             larity, and partly due to relative frequency; the larger the
et al., 1995, e.g.) although lemmas (Lowe and McDon-           difference in occurrence probability, the larger associa-
ald, 2000), encyclopedia articles (Landauer and Dumais,        tion a context element must have to affect the similarity
1997) and whole documents have been used.                      function.
    A speci£es the functional form of the mapping                 Since it is unlikely that semantic similarity depends
from co-occurrence frequencies between particular ba-          on relative frequency, we have a theoretical reason not
sis elements and each word in the language so                  to use raw co-occurrence counts as a lexical association
that each word is represented by a vector v =                  function.
 A(b1 ,t), A(b2 ,t), . . . , A(bD ,t) . A may be the identity     Researchers in information retrieval have also noted
function.                                                      problems with raw co-occurrence counts and use various
    S is a similarity measure that maps pairs of vectors       weighting schemes to counteract them. Latent Seman-
onto a continuous valued quantity that represents con-         tic Analysis (LSA; Landauer and Dumais, 1997; Re-
textual similarity.                                            hder et al., 1997), a semantic space model derived from
    M, is a transformation that takes one semantic space       information retrieval research uses an entropy-weighted
and maps it onto another, for example by reducing its          function: A(b,t) ∝ log( f W (b,t) 1). The logged co-
dimensionality. Various choices for these elements are         occurrence count is then divided by the entropy of the
possible, and lead to rather different spaces. M may also      distribution of b over each documents. If b is evenly dis-
be an ‘identity’ mapping that does not change the space.       tributed across documents then it is probably not infor-
In the following sections we consider the implications of      mative about any particular document. In contrast if it
different choices of A, B, S and M.                            occurs in some but not others it may be more informative
                                                               about their content.
A : Lexical Association Function                                  LSA’s lexical association function is designed to allow
Zipf’s law suggest that using vectors of co-occurrence         arbitrarily many basis elements into the similarity calcu-
counts directly may not be a good choice when construct-       lation by weighting them appropriately. However neither
ing a semantic space. To see why, consider two words           logging nor dividing by entropy is guaranteed to reverse
t1 and b with probabilities p(t1 ) and p(b). If t1 and b       the effects of chance co-occurrence since this is never
have no semantic relation to each other, then they will be     explicitly estimated.

                              Target           Non-target         is greater than 1. When the presence of b makes no dif-
                                                                  ference to the probability of seeing t then θ = 1 and we
            Context            f W (b,t)        f W (b, t)        can conclude that b and t are distributionally indepen-
            Non-context        f W ( b,t)       f W ( b, t)       dent. Finally, if θ < 1 the presence of t makes seeing b
                                                                  less probable.
                                                                     We can estimate the odds ratio from Table 1:
Table 1: Co-occurrence frequency within a window of                                           f W (b,t) f W ( b, t)
target, context and all other words. t represents a word                       θ̂(b,t) =                            .
                                                                                              f W (b, t) f W ( b,t)
that is not t.
                                                                  Where the W N factors have canceled. This measure is
                                                                  often logged so that then the magnitude of log θ̂(b,t)
    Lowe and McDonald (2000) used a log-odds-ratio                can be interpreted as a direct measure of the level of as-
measure to explicitly factor out chance co-occurrences.           sociative strength between t and b, with the effects of
The empirical counts necessary for computing the log-             chance co-occurrence factored out. Positive values indi-
odds-ratio are shown in Table 1. t represents any word            cate greater than chance positive association.
that is not t, b represents a word that is not the con-
text word b and f W ( b,t) is the number of times a word          Lexical Association in Lexicography
that is not the context word occurs among the W words             The most informative words for t are those that occur
surrounding t.                                                    only in its context, e.g. t=‘sealed’ and b=‘hermetically’.
    Computing the cell counts is straightforward because          Instances of word pairs like this are concordances, or
there exists a very close approximation that is a function        collocations, and are of interest to lexicographers. Con-
only of f W (b,t) itself, f (t), f (b), W , and N:                sequently, the log-odds-ratio also provides a method of
                                                                  £nding collocations between words. Previous work in
         f W (b, t) = W f (b)             f W (b,t)               lexicography has used pointwise mutual information,
                                                                  log-likelihood ratios, and T-tests. Since by symmetry
         f W ( b,t) = W f (t)            f W (b,t)
                                                                  these alternative measures can also be lexical association
       f W ( b, t) = W N             ( f W (b, t)      f W ( b,t) functions, we review them brie¤y below.
                                f W (b,t)).                       Mutual Information The pointwise mutual informa-
                                                                  tion I(b,t) between t and b Church and Hanks (1990) is
To derive these expressions consider the limiting situa-
tion where W = 1 and f (b,t) is the number of times                                                    pW (b,t)
the bigram b,t occurs. Since by de£nition f (b) =                                  I(b,t)  = log
                                                                                                     W p(b)p(t)
 f (b,t) f (b, t), then f (b, t) = f (b) f (b,t), and the
same reasoning applies to f ( b,t). Similarly the number          and can be also be estimated using the frequencies in
of elements in the table, f (b) f ( b), must be the num-          Table 1. I(b,t) measures how much information an
ber of bigrams in the corpus. For a large corpus this is          occurrence of b contains about t. If b occurs with t
essentially N, the number of words in the corpus. There-          no more often than would be expected by chance then
fore since f ( b, t) is the only cell undetermined it is          pW (b,t) = W p(b)p(t) and I(b,t) = 0, so the mutual in-
obtained by subtracting the sum of the other cells from           formation measure effectively factors out random co-
N. The W factors appear on quantities other than the co-          occurrences. However, if t and b always occur together
occurrence count when the window size is more than one            then pW (b,t) = p(b) and I(b,t) = log 1 p(t), so the less
because only f W ( b, t) already takes the window size            frequent b and t are the larger their association is. In
into account1 .                                                   contrast, changing the marginal probabilities of t or b is
    We obtain probabilities from Table 1 by dividing              equivalent to adding a constant value to rows or columns
each cell count by W N. Then the odds of seeing t                 of the contingency tables above (Bishop et al., 1975). It
rather than some other word when b is present are                 is easy to con£rm that this change makes no difference
pW (b,t) pW (b, t), and the odds of seeing t in the ab-           to θ.
sence of b is pW ( b,t) pW ( b, t). Therefore if the
presence of b increases the probability of seeing t then          The G-score Dunning (1993) uses a log-likelihood ra-
the odds ratio (Agresti, 1990)                                    tio statistic (Agresti, 1990), which he calls the G-score,
                                                                  to discover collocations in text. This method compares
                                                                  two models of the relationship between t and b. In the
                                 pW (b,t) pW (b, t)
              θ(b,t) =                                            £rst model (association) assumes that p(b t) = p(b
                              pW (    b,t) pW ( b, t)               t), whereas the second model (no association) assumes
                              pW (b,t) pW ( b, t)                 that p(b t) = p(b          t). The statistic is the ratio of
                         =                                        the maximized log-likelihoods for each model’s parame-
                              pW (b, t) pW ( b,t)
                                                                  ters. This measure takes chance co-occurrence into ac-
     1 The  derivation is reported elsewhere (Lowe, 2000a).       count because it implicitly compares the observed co-

occurrence frequencies with the co-occurrence frequen-       parameter because f W (b,t) is large enough to provide
cies that would be expected by chance. For example,          a reliable estimate of pW (b,t). However, every vector
the expected value of the top left cell in Table 1 is        will be similar because all words in the language tend
W f (t) f (b) N under (no association) but f W (b,t) under   to occur with the high frequency words in the basis, ir-
(association). Empirically using log-likelihood ratios as    respective of their distributional pro£le. Consequently,
vector elements in a semantic space generates similar        distances between words will be extremely similar and
results to using log-odds-ratios. This is to be expected     vectors in the biased model will fail to re¤ect important
since both measures take chance co-occurrences into ac-      distributional differences.
count. Alternative measures include the χ2 statistic and        Alternatively, if only low frequency content words
Fisher’s exact test. However, Dunning shows that the dis-    are chosen as basis elements then vectors will be more
tributional properties of the G-score are superior under     highly informative and distances in the space will be able
normal lexicographic conditions, and the hypergeomet-        to re¤ect subtle distributional similarities. This model
ric probabilities required in Fisher’s test are intractable  will have high variance because the co-occurrence counts
to compute for contingency tables containing very large      needed to determine A(b,t) are unreliable. Variance can
counts (Agresti, 1990). For example, f W ( b         t) will always be decreased by providing more data, but Zipf’s
typically exceed the number of words in the corpus.          law suggests a power relation between the amount of new
   Considering the lexicographic task emphasizes the         text that would need to be found and the reduction in co-
‘second order’ nature of semantic space measures of sim-     occurrence count variability.
ilarity: they re¤ect regularities across multiple ‘£rst or-     In theory the fullest possible distributional pro£le for a
der’ association measures, one for each vector element.      word would include all words in the language, generating
This interpretation is taken up again in discussing appro-   an infeasibly large vector. In practice this is not possible
priate similarity functions below.                           and some subset of words must be chosen.
                                                                The solution for LSA is to use as many words as possi-
B : Choosing a Basis                                         ble with appropriate weighting for each vector element,
When choosing basis elements for a semantic space there      and then use M to compress the original vectors into a
is a trade-off between choosing words that are represen-     smaller space with dimensions that are linear combina-
tative of sentence content, but may not give reliable count  tions of the original ones.
statistics due to their low frequency, and choosing high     The Column Variance Method For HAL, elements of
frequency words that provide reliable statistics but ap-     B are chosen by compiling a 70,000 70,000 matrix of
pear in almost every sentence of the language. The trade-    word co-occurrences and discarding the columns of low-
off is an instance of the bias-variance dilemma in statis-   est variance2 . Consistent with Zipf’s law, column vari-
tical learning theory (Geman et al., 1992).                  ance decreases sharply with the frequency of the word
The Bias-Variance Dilemma Every statistical model            corresponding to the column (Lund et al., 1995). Then
is able to represent a subset of the class of possible hy-   for each set of experimental stimuli, Burgess et al. com-
potheses about data. The range of hypotheses is typically    pute variances over each vector element and retain only
controlled by the model’s structure and by a set of ad-      the most variant. We can refer to this as the column vari-
justable parameters. More ¤exible models can represent       ance method of basis element choice.
more hypotheses and are said to have less bias. In con-         The method is dif£cult to analyze because the basis is
trast, a very ¤exible model will require a large amount      recomputed for each experiment, but we can show that
of data to determine accurate values for its parameters.     it has a frequency bias. If b and t are unrelated then we
When there is not enough data compared to the number         can, again, model them as Binomially distributed. In the
of parameters, parameter estimates may be optimal for        simple case where W = 1, the variance of the frequency
the particular data set the model was trained on, but will   count under independence is
fail to generalize to new data. A model that ‘over£ts’ in
this way is said to have high variance. Model variance              Var f W (b,t) = N p(t)p(b)(1         p(t)p(b))
can be decreased at the cost of adding bias e.g. by con-                             = N p(t)p(b)     N p(t)2 p(b)2 .
straining or removing parameters. Bias can be decreased
by making the model more ¤exible, at the cost of needing     so the expected variance of f W (b,t) is quadratic in p(b).
more data to cope with increased variance.                   The expected variance of the elements of a column of
   In a semantic space the vector elements, A(b,t) are pa-   such counts is the same as the variance of the column
rameters that estimate the amount of association between     sum i.e. the sum of the individual variances. Figure 1
b and t on the basis of observed data f W (b,t). When        shows the expected variances for a 14 14 table of co-
choosing the basis elements b1 . . . bD , we can de£ne a     occurrence counts for perfectly unrelated words with oc-
highly biased model by choosing only very high fre-          currence probabilities ranging from 0.5 to 0.0667. Even
quency words. Co-occurrence counts for high frequency        completely unrelated words will show distinct structure
words are very reliable because high frequency words
appear in nearly all sentences. This biased model will           2 Co-occurrences  are also weighted by distance, but this
have very low variance; each A(b,t) is a well-determined     does not affect the following argument.

                                                             S : Similarity Measure
     2.5
       2
                                                             Two popular similarity measures are Euclidean distance
                                                             and the cosine. For two vectors v and w in a D-
     1.5
                                                             dimensional basis, the squared Euclidean distance v
                                                             w 2 is simply related to the cosine ρvw of the angle be-
                                                             tween them:
       1
                                                                                     D
     0.5                                                             v   w  2
                                                                                =    ∑ (vi   wi )2
                                                                                    i=1
                                                                                                             vw
       0
         0             5              10              15                        =     v  2
                                                                                                w  2
                                                                                                      2
                                                                                                           v    w
                                                                                =     v  2
                                                                                                w  2
                                                                                                      2 ρvw
Figure 1: An example of column variance method. Ex-
pected column means based on expected co-occurrence          where w 2 = ∑D         2
                                                                                i wi is a squared vector length. From
counts between each of 14 hypothetical unrelated words.      this equation it can be seen that v w 2 ∝ ρvw only
To estimate means and variances for a corpus of N words,     when v and w are standardized in length. When A(b,t) =
multiply all quantities by N. Error bars represent ex-        f W (b,t) then vector element may have widely differing
pected column variances.                                     lengths depending on p(b) and p(t).
                                                                 One advantage of the cosine is that it ranges between
                                                             -1 and 1, and so removes any arbitrary scaling induced
in their column variances, but this is entirely due to their by the range of A and the number of elements in B. When
baseline frequencies.                                        A is simple co-occurrence the cosine is also less sensi-
    There are two possible causes for a high column vari-    tive than Euclidean distance to extreme values induced
ance. The £rst cause is simple frequency as shown in         by widely differing basis element frequencies, although
Figure 1. The second reason is that the words are in fact    a good choice of A should avoid this problem.
distributionally related. Then unexpectedly large vari-          The interpretation of similarity as a ‘second order’
ance can be a sign that the Binomial assumption has          regularity can motivate yet another plausible similarity
failed, and that two words are in fact related. However      measure. We may take the correlation coef£cient (Pear-
the size of the variance increase necessary is variable. In  son’s r) as a measure of how well the elements of each
the column variance method, for a word that is distribu-     word’s vector match. The only difference between this
tionally related to some of the experimental materials to    and the cosine measure is that the mean of each vec-
make it into the £nal basis set it must be strongly associ-  tor is included in the similarity measure. This will not
ated enough that its observed column variance moves it       only offset the effect of different vector element magni-
into the window of very high variance words at the up-       tudes, but also place all calculations in a regular statis-
per end of the frequency table. In other words, it is not    tical framework. The statistical implications of taking
enough to be twice as variant as would be expected by        correlation coef£cients over log-odds-ratios remain to be
chance, a word must be as many times more variant as         worked out. In addition, all the measures described here
it takes to have a variance that is absolutely high; lower   will bene£t from a characterization of their properties in
frequency words have to work harder and unrelated but        small samples. This is future work.
high frequency words will get chosen anyway.
    This analysis of the column variance method predicts     M : Model
that, in the absence of strong association, the variance of  A semantic space is fully functional when a B, A and S
a column corresponding to some candidate element will        have been speci£ed. However, it is possible to build
correlate strongly with that element’s frequency.            a more structured mathematical or statistical model. In
    This was tested by taking candidate lemmas of fre-       LSA the model consists of a projecting vectors into a lin-
quency rank 100 to 600 in the BNC, and experimen-            ear subspace of B using singular value decomposition.
tal stimuli from McKoon and Ratcliff’s graded priming        This is equivalent to selecting the k orthogonal axes that
study (see Lowe and McDonald, 2000). The analysis            account for most variance of words in semantic space.
predicts that the levels of genuine association (corrected   Each word is then projected into the the subspace, and
for frequency) between these candidates and the experi-      point is then ‘re-in¤ated’ back into the full dimensional-
mental stimuli will be be low because the words are so       ity and cosine measures applied. Cosines can be taken
frequent that they provide little information about con-     in the linear subspace without subsequent re-in¤ation as
text. In fact for this data log-odds-ratios are mildly neg-  suggested by Berry et al. (1995).
atively correlated with column variance r=-.317 p<.001.          The theoretically important point about LSA’s dimen-
In contrast candidate frequencies strongly positively cor-   sionality reduction is that it is a simple instance of in-
related with column variance for co-occurrence counts,       ferring latent structure in distributional data. Parts of
r=.8553 p<.001.                                              speech, and grammatical structures are also examples of

latent structure in the sense that they are in-principle un-   Foltz, P. W., Kintsch, W., and Landauer, T. K. (1998).
observable aspects of words that re¤ect their distribu-          The measurement of textual coherence with Latent Se-
tional properties. One important direction for seman-            mantic Analysis. Discourse Processes, (25).
tic space research is to £nd an appropriate type of la-        Geman, S., Bienenstock, E., and Doursat, R. (1992).
tent structure to explain the distributional regularities that   Neural networks and the bias/variance dilemma. Neu-
are assumed to underly semantic similarity. Biologi-             ral Computation, 4(1).
cally motivated models using topographic mapping, and          Landauer, T. K. and Dumais, S. T. (1997). A solution to
strictly random mappings have also been investigated             Plato’s problem: the latent semantic analysis theory of
(Lowe, 2000a,b).                                                 induction and representation of knowledge. Psycho-
                                                                 logical Review, (104).
                       Conclusion                              Landauer, T. K., Foltz, P. W., and Laham, D. (1998).
In this paper we have put forward some theory for se-            Introduction to Latent Semantic Analysis. Discourse
mantic space models. In addition to presenting a frame-          Processes, (25).
work for thinking about current semantic space mod-            Levy, J. and Bullinaria, J. (2000). Learning lexical prop-
els we have examined the implications of various de-             erties from word usage patterns. In Proceedings of the
sign choices, emphasized the importance of avoiding fre-         7th Neural Computation and Psychology Workshop.
quency biases, and presented methods for doing so. We            Springer Verlag.
have also connected semantic space theory to lexico-           Lowe, W. (2000a). Topographic Maps of Semantic
graphic methods and to standard problems of bias and             Space. PhD thesis, Institute of Adaptive and Neural
variance discussed in the statistical literature.                Computation, University of Edinburgh.
                                                               Lowe, W. (2000b). What is the dimensionality of hu-
                   Acknowledgments                               man semantic space? In Proceedings of the 7th Neu-
Thanks to Daniel Dennett at the Center for Cognitive             ral Computation and Psychology Workshop. Springer
Studies at Tufts, where much of the work reported here           Verlag.
was done, to the Center for Basic Research in the Social       Lowe, W. and McDonald, S. (2000). The direct route:
Sciences at Harvard for support, and to three anonymous          Mediated priming in semantic space. In Gernsbacher,
referees for helpful comments.                                   M. A. and Derry, S. D., editors, Proceedings of the
                                                                 22nd Annual Meeting of the Cognitive Science Society,
                       References                                New Jersey. Lawrence Erlbaum Associates.
Agresti, A. (1990). Categorical Data Analysis. John            Lund, K., Burgess, C., and Atchley, R. A. (1995). Se-
   Wiley and Sons.                                               mantic and associative priming in high-dimensional
                                                                 semantic space. In Proceedings of the 17th Annual
Berry, M. W., Dumais, S. T., and O’Brien, G. W. (1995).          Conference of the Cognitive Science Society. Mahwah,
   Using linear algebra for intelligent information re-          NJ: Lawrence Erlbaum Associates.
   trieval. SIAM Review, 37(4).
                                                               Mandelbrot, B. (1954). Structure formelle des textes et
Bishop, Y. M. M., Feinberg, S. E., and Holland, P. W.            communication. Word, (10).
   (1975). Discrete Multivariate Analysis: Theory and
                                                               McDonald, S. and Lowe, W. (1998). Modelling func-
   Practice. MIT Press.
                                                                 tional priming and the associative boost. In Gerns-
Burgess, C., Livesay, K., and Lund, K. (1998). Explo-            bacher, M. A. and Derry, S. D., editors, Proceedings
   rations in context space: Words, sentences, discourse.        of the 20th Annual Meeting of the Cognitive Science
   Discourse Processes, (25).                                    Society, New Jersey. Lawrence Erlbaum Associates.
Cann, R. (1996). Categories, labels and types: Func-           McKoon, G. and Ratcliff, R. (1992). Spreading activa-
   tional versus lexical. Edinburgh Occasional Papers in         tion versus compound cue accounts of priming: Medi-
   Linguistics EOPL-96–3, University of Edinburgh.               ated priming revisited. Journal of Experimental Psy-
Church, K. W. and Hanks, P. (1990). Word association             chology: Learning, Memory and Cognition, (18).
   norms, mutual information, and lexicography. Com-           Rehder, B., Schreiner, M. E., Wolfe, M. B., Laham, D.,
   putational Linguistics, (16).                                 Landauer, T. K., and Kintsch, W. (1997). Using Latent
Dunning, T. (1993). Accurate methods for the statistics          Semantic Analysis to assess knowledge: Some techni-
   for surprise and coincidence. Computational Linguis-          cal considerations. Discourse Processes, (25).
   tics, (19).                                                 Zipf, G. K. (1949). Human Behavior and the Principal
                                                                 of Least Effort. Addison Wesley.
Finch, S. (1993). Finding Structure in Language. PhD
   thesis, Centre for Cognitive Science, University of Ed-
   inburgh.
Firth, J. R. (1968). A synopsis of linguistic theory. In
   Palmer, F. R., editor, Selected Papers of J. R. Firth:
   1952-1959. Longman.

