UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Very Rapid Induction of General Patterns
Permalink
https://escholarship.org/uc/item/9d13q0jk
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 23(23)
Author
Hadley, Robert F.
Publication Date
2001-01-01
Peer reviewed
 eScholarship.org                              Powered by the California Digital Library
                                                                University of California

                               Very Rapid Induction of General Patterns
                                                         Robert F. Hadley
                                                  School of Computing Science
                                                 and Cognitive Science Program
                                                     Simon Fraser University
                                                     Burnaby, B.C., V5A 1S6
                                                       hadley@cs.sfu.ca
                            Abstract                                skills may involve “training up” various sub-networks in
                                                                    our brains, but this prior training may well not involve
   Marcus (1998) and Phillips (2000) each have produced             the “hard” kinds of generalization at issue.
   examples of human generalizations which, they argue,                Having said that, I would emphasize that my even-
   cannot be matched by the best known connectionist ar-            tual, general conclusion supports both the positions of
   chitectures and training algorithms. However, I argue that
   humans perform the crucial generalizations without being         Marcus and Phillips. For, in the final section I consider
   trained on the exemplars that Marcus and Phillips cite.          whether connectionist architectures are capable (without
   So, in a sense, the issue whether networks can be trained        implementing classically symbolic methods) of orches-
   to perform the crucial generalizations is a red herring. I       trating the application of our “prior skills” in a fashion
   argue further that humans achieve the dramatic general-
   izations in question as a side-effect of a variety of pre-       that permits very rapid pattern induction and reasoning.
   existing skills, working in concert. Finally, it is shown        My conclusion favors the classicist position on this is-
   that the “hard cases” displayed by Marcus and Phillips do        sue. Moreover, I propose a new challenge for eliminative
   in fact provide the basis for a serious challenge to pure        connectionists which, in my view, formulates the deeper
   (non-modular) connectionist architectures.                       difficulty posed by the aforementioned “hard cases” of
                                                                    Marcus and Phillips.
                        Introduction                                         The Hard Generalization Tasks
In Marcus (1998, in press) and Phillips (2000), intriguing
refinements on the (1988) Fodor-Pylyshyn “generaliza-               Generalizing Outside the Training Space
tion challenge” are presented. Both Marcus and Phillips             Marcus (1998) defines a network’s training space as
argue that linguistically competent humans exhibit im-              the N-dimensional vector space created by the non-zero
portant forms of generalization that backpropagation-               training values of the N units comprising the network’s
trained networks (both recurrent and feedforward) can-              input array. A datum presented during the network’s
not attain. Though neither author categorically asserts             (post-training) test phase lies “outside the training space”
that their negative conclusions apply to every form of              if and only if that datum does not fall within the vector
connectionist training, both authors argue that commonly            space just mentioned. In effect, this entails that the datum
recognized varieties of eliminativist architectures (i.e.,          is novel relative to the training corpus. For example, any
those eschewing classical representations) are at stake.1           datum would be novel in the relevant sense if it presented
   In this paper, I examine two instances which typ-                non-zero values to the input array in units that contained
ify the “hardest challenges” produced by these authors.             only zero values during training.
While I agree that they have each exposed some impor-                  This “generalization hurdle” differs somewhat from
tant training limitations of backpropagation networks, I            the hierarchy of systematicity given in Hadley (1994a),
shall argue that humans perform the crucial generaliza-             but it appears equivalent to one of several levels of gener-
tions without being trained on the exemplars that Mar-              alization formulated in Niklasson and van Gelder (1994).
cus and Phillips cite. So, in one sense, the issue whether          Interestingly, in the latter paper, the authors claim to sat-
networks can be trained to perform the crucial general-             isfy this particular generalization challenge. Their claim
izations is a red herring. As I argue, humans possess the           is questioned in Hadley (1994b) and wholly disputed by
relevant generalization capacity because they have previ-           Marcus (1998). Moreover, Marcus discusses a number
ously acquired separate skills which, working in concert,           of specific ways in which a network can fail to gener-
allow for nearly instantaneous pattern induction and rea-           alize outside its training space, and we now consider a
soning. To be sure, the prior acquisition of these separate         particular “hard case” which Niklasson and van Gelder
                                                                    had not addressed.
    1 Here I am following Fodor and Pylyshyn’s (1988) usage,
                                                                       Suppose a linguistically competent human is presented
according to which a composite (or complex) representation
is classical in structure if one cannot activate (or token) that    with the following series: “A rose is a rose”, “A frog is a
representation without, at the same time, tokening its syntactic    frog”, “A pencil is a pencil”. Humans will typically have
constituents.                                                       no difficulty inducing the general pattern and complet-

ing the following sentence: “A blicket is a ...”. Humans     Moreover, those features are shared by both nouns and
will succeed here even though ‘blicket’ is a novel word      verbs, and, being a nonsense word, ‘blicket’ has no se-
which is outside their training space. In contrast, Mar-     mantic features. So, if Marcus objects to the deployment
cus offers persuasive arguments, based upon the training-    of distributed representations in these network experi-
independence of output nodes, to show that backpropa-        ments, it seems incumbent upon him to demonstrate that
gation networks necessarily fail to match this success.      humans are using only local representations when they
These arguments are buttressed by several connectionist      successfully generalize from “A rose is a rose”, etc. to
experiments conducted by Marcus.                             “A blicket is a blicket”. In the absence of such a demon-
   On the basis of the above and related tasks, where a      stration, there seems no reason to grant that humans are
strong discrepancy exists between human performance          in fact generalizing outside their training space in cases
and that of eliminative networks, Marcus concludes C:        such as this.
that human success in such cases is not purely due to           For all the above reasons, I have serious reservations
any training of putative eliminative networks within our     about Marcus’ argument for conclusion C. Nevertheless,
brains. This conclusion forms a keystone of Marcus’          as mentioned, I believe there is a compelling reason to
larger thesis – that the human ability to discover general   accept (C). And, if I am right about this latter reason,
patterns in cases such as these involves symbolic rule in-   then the disputed capacity of eliminative networks to
duction, and the application of such rules entails variable  generalize outside their training spaces may be irrelevant
binding.                                                     as the task is presently formulated.
   Now, while I agree with conclusion (C), I accept this        Here is the situation: humans clearly are able to per-
conclusion for reasons other than any offered by Marcus.     form very rapid pattern induction, not only in the various
For one thing, I suspect that some Hebbian-competitive       cases that Marcus cites, but in many other instances. In
networks can generalize outside their training spaces on     the above case, humans are able to induce a general pat-
at least some tasks. This suspicion derives from recent      tern, and supply ‘blicket’ in response to the test phrase
experimentation with an architecture I have reported in      “A blicket is a ... ”, within mere seconds after hearing “A
(Hadley, et al, to appear). Another difficulty is that Mar-  rose is a rose”, and the few remaining sample sentences.
cus himself notes that when distributed, rather than local,  Given the very short time span involved, we may be quite
representations are assigned to input tokens, backpropa-     certain that human success in this and similar cases does
gation networks will, at first blush, provide the appear-    not stem from some extremely rapid training of “neural
ance of generalizing outside their training spaces. For      networks” (whether eliminative or not). As emphasized
example, in the “A blicket is a .......’ test, a backpropa-  in Hadley (1993), in cases where humans make virtually
gation network can successfully produce the distributed      instantaneous inferences, and when they acquire general
representation for ‘blicket’, provided all the separate fea- rules in a matter of mere seconds, rapid synaptic weight
tures encoding blicket had, at some point, been employed     change can be ruled out. Synapses simply do not grow
in various nouns during the training phase. Admittedly,      fast enough to permit the acquisition of coherent func-
one could argue that this last proviso undermines any        tionality within the span of a few seconds. Functionally
well founded claim to generalization outside the train-      coherent synaptic changes occurs within spans of hours
ing space, but in doing so, one would undercut the entire    or days, not in a few seconds.
force of the ‘blicket’ test case. For the word ‘blicket’        Now, it might be objected that in the case of the
itself possesses only phonetic and graphemetic features      ‘blicket’ generalization, humans have in fact had entire
that humans have often encountered prior to being pre-       days or even years to “train up” their networks, since, ar-
sented with the ‘a blicket is a ...’ test phrase. That is, a guably, they have frequently heard phrases of the precise
plausible distributed representation for ‘blicket’ does not  form, “an X is an X”, in the past. However, this objec-
contain any features novel to English-speaking humans.       tion falters when we reflect that English-speaking adults
   Marcus himself does not stress the objection I have       have no difficulty inducing a novel pattern, and complet-
assigned to some anonymous “one”. Rather, he pri-            ing the final “sentence” in the following series: “Rose
marily objects (Marcus, in press, appendix 1) to the         biffle biffle zarple zarple rose”; “Frog biffle biffle zarple
use of distributed representations on the grounds that       zarple frog”; “Blicket biffle biffle — — —”. In this case,
they fail “... to unambiguously represent all and only       the pattern being induced is clearly novel, since the pat-
the possible continuations to a given string ...”. That      tern (template) itself not only includes the words ‘biffle’
is, when both nouns and verbs share several features         and ‘zarple’, but involves a “syntax” that employs a dou-
in common (as indeed they would if we employ pho-            ble repetitive pattern not found in English. Yet, humans
netic or graphemetic features), we run into the super-       perform this induction in mere seconds. We must con-
position catastrophe (crosstalk). (I would argue, how-       clude, therefore, that the ability to perform rapid pattern
ever, that there is, at most, very little overlap between    inductions of this kind does not derive from some in-
semantic features belonging to nouns and those belong-       stantaneous training of a neural network, but must rely
ing to verbs. For this reason, among others, the system      on at least some pre-existing skills. Certain of these
described in Hadley et al, 2000, employs semantic fea-       prior skills involve the capacity to recognize phonemes
tures.) Be that as it may, it remains true that the pho-     or graphemes, which doubtless entails modification of
netic and graphemetic features of ‘blicket’ are not novel.   synaptic “weights”, which in turn (presumably) amounts

to the training of sub-networks within the brain.            task. Clearly, in the elapsed time between your having
   Note, however, that this prior network training is not    read the problem statement and your having derived the
specifically directed to the generalization task just con-   remaining propositions no neural network was trained
sidered. The novelty of the pattern being induced en-        within your brain to perform the relevant inferences.
sures that very rapid, successful induction of this pattern  Rather, your success stems from a prior ability to engage
must arise as a side-effect of prior skill acquisition. An   in iterative processing and modus ponens inferences. Ar-
appropriate challenge, therefore, for eliminative connec-    guably, in the case of humans who lack formal logic
tionism, is not whether a single network can be trained      training, the latter capacity derives from prior training in
to generalize successfully from the few samples of data      language use (with sentences of the form: if P then Q).
cited above, but whether an essentially non-classical net-      Of course, from a connectionist perspective, the ca-
work can exercise its hitherto acquired skills in a manner   pacity to apply inference patterns to novel data (Fiffle,
that yields, as a side-effect, the kind of rapid pattern in- Giffle, and Kiffle) is a significant achievement, and it is
duction considered above. Clearly, these are deep waters;    questionable whether any cognitively plausible ANN ex-
I shall return to this issue in section 3.                   periment has succeeded in this task.2 However, just as in
                                                             the case of ‘blicket’, ‘Fiffle’, ‘Giffle’, and ‘Kiffle’ pos-
Generalization in Rapid Inference                            sess only non-novel phonetic and graphemetic features.
We turn now to consider an apparently “hard case”, pre-      Given that Marcus was able to train a simple recurrent
sented by Phillips (2000). This case is one of a series      net to predict ‘blicket’ in the ‘a Y is a Y’ formula, there
of generalization tasks considered by Phillips. Each task    would seem no obstacle, in principle, to the modus po-
in the series possesses features which, at first blush, ren- nens inference pattern being applied to nonsense words,
der it unlearnable by backpropagation methods in feed-       provided the latter are represented by distributed repre-
forward and recurrent networks. However, Phillips en-        sentations of the right kind.
gages in a dialectical process in each case, and seems          With this in mind, we now consider the problematic
to conclude that, provided overlapping distributed rep-      case that Phillips describes, viz., transverse patterning.
resentations are assigned to functionally similar atomic     Phillips defines transverse patterning as follows:
constituents within the input data, then, with one excep-
tion, each task becomes learnable. The apparent excep-          Transverse patterning is an example of a stimulus-
tion is discussed below.                                        response task that depends on between constituent
   It is noteworthy, though, that even in the case of this      relations (my emphasis) . A task instance or
seeming exception, Phillips describes a network capa-           problem set consists of three unique patterns (e.g.,
ble of performing the task. He produces a carefully de-         strings, shapes, etc.) A, B and C, such that: A pre-
signed, fragile (and hand-crafted) network whose pre-           dicts B; B predicts C; and C predicts A. Once the
scribed weight configuration suffices to display appro-         transverse patterning task structure has been learnt
priate generalization behaviour. However, Phillips nei-         from the first few problem sets, subjects require
ther argues that the requisite weights could be acquired        only one of the three stimulus-response pairs to pre-
by learning, nor that the network possesses any cogni-          dict the remaining two, for any new transverse pat-
tive plausibility. Given the precise and fragile nature of      terning problem set.
the requisite weight vectors, it seems unlikely that the
particular network Phillips discusses could in fact be en-      At first glance, there may appear to be an ambiguity
gendered through training.                                   in the last of the sentences just quoted. However, care-
   Presently, I consider details of Phillips “recalcitrant   fully read, the sentence tells us that human subjects can
case”, but before doing so it will be helpful to consider    predict, when given a single novel stimulus-response pair
a partially analogous example. Let us assume that Fif-       (of the form “shape X predicts the appearance of shape
fle, Giffle, and Kiffle are names of propositions that have  Y”) what the two remaining novel S-R pairs, having this
truth values. (I assume these three names, qua names, are    general form, will be. In personal communication with
novel for most readers.) Also suppose that the following     Phillips I have verified that the sentence is not describ-
three statements are true.                                   ing human predictions of the two remaining geometric
                                                             shapes, given the first geometric shape.
   If Fiffle is true, then Giffle is true.                      Phillips goes on to relate that trained feed-forward and
   If Giffle is true, then Kiffle is true.                   recurrent networks are not able to match the impressive
                                                             kind of generalization just described. This is not surpris-
   If Kiffle is true, then Fiffle is true.                   ing. What is surprising, initially, is that humans can pre-
   Finally suppose that Kiffle is true. What else can then   dict what the two specific novel S-R pairs will be, given
be known to be true? Before reading further, I invite the    exposure only to one of the three novel S-R pairs. This
reader to discover what can be inferred.                     surprise evaporates, though, when we learn (as I did in
   Doubtless, without effort, you have rapidly inferred      further personal communication with Phillips) that hu-
the truth of the two remaining propositions, Fiffle and      man subjects are told in advance what the three geomet-
Giffle. Any number of literate humans, who have no               2 From a cognitive standpoint, I have serious qualms about
training in formal logic, could similarly succeed at this    Boden’s and Niklasson’s (2000) recent results on this issue.

ric shapes will be in the novel test situation. Given this,    Phillips believes that his transverse patterning case
and given that the subjects will have learned the over-     demonstrates that similarity in distributed representa-
all structure of the training experiment (following their   tions (of atomic constituents) does not suffice to enable
first few sessions), they are able to reason analogically,  certain kinds of networks to generalize a particular kind
and to derive by a process of elimination, what the re-     of inference patterns to novel data. To the contrary, I
maining two S-R pairs must be. For example, in the new      have argued that Phillips has conflated the challenge of
test situation, subject Kim learns that the novel shapes    having a network generalize the application of a single
will be a star, an ellipse, and a hexagon. After being      inference pattern with several larger issues. While I cer-
presented with the first S-R pair, Kim is able to infer     tainly agree that the use of distributed representations
immediately, that (say) A corresponds to the star, and      cannot compensate for the absence of separate, previ-
that B corresponds to the ellipse. Knowing this, Kim        ously acquired reasoning skills (together with the con-
can reason analogically that the ellipse (corresponding to  siderable prior training that would engender those skills),
B) must predict the third geometric shape, the hexagon.     this tells us nothing about the efficacy of deploying dis-
Reasoning further, again by analogy, Kim discovers that     tributed representations when attempting to apply a sin-
the hexagon (corresponding to C) must be the predictor      gle known inference pattern to novel data. It is crucial
of (A), the star.                                           to realize that, in the “transverse patterning” experiment
   Now, the crucial point to realize here is that human     discussed above, humans are doing far more than gen-
success in this task involves powerful reasoning skills     eralizing the application of a single inference pattern to
(both analogical and reasoning by elimination) which the    novel data. They are engaged in an elaborate process
human possessed prior to any of the S-R conditioning        involving meta-observations and the composition of sep-
induced in Phillips experiment. In all likelihood, these    arate, sophisticated inference skills.
prior reasoning skills reside in separate modules which        Moreover, it is questionable whether the S-R training
were unaffected by the S-R reasoning presently being        sessions have trained human subjects in any new infer-
considered (see Hadley, 1999, for arguments on the mod-     ence pattern at all. It seems more likely that the sessions
ularity issue). In contrast, the non-modular feedforward    merely provided opportunities for subjects to acquire the
and recurrent networks which Phillips contrasts with the    base atomic facts (of the form X predicts Y, analogous
human success, possess no prior skills in reasoning of      to the simple “if-then” premises in my modus ponens ex-
any kind, much less the powerful reasoning capacities       ample) which provide fodder for the capacity of humans
that humans bring to the experiment.                        to apply pre-existing inference skills to novel data.
   The situation is complicated, and confused, by the fact     In any case, I believe it is clear that Phillips’ “hard
that various of Phillips’ remarks create the impression     case”, like that of Marcus, involves the composition and
that he is contrasting a human ability to generalize an     application of pre-existing skills.
inference pattern, which has been acquired in the S-R
conditioning sessions, with an inability, on the part of                              Discussion
widely used connectionist architectures, to exhibit com-    In the foregoing, I have argued that, for the general-
parable generalization. At various points, Phillips explic- ization tasks in question, the challenges posed to elim-
itly states that the transverse patterning task amounts to  inative connectionism have not been felicitously formu-
the task of generalizing logical inference patterns. For    lated. For, in the tasks considered, we have seen that
example, he says,                                           human success gives every appearance of either arising
                                                            through the composition of multiple prior skills (viz.,
   Under controlled conditions, subjects consistently       language comprehension, analogical reasoning, and de-
   make inferences implied by the underlying logical        duction by process of elimination, in the case of trans-
   rules (Halford et al. 1998a). Indeed such tasks          verse patterning) or arising as a side-effect of the capac-
   are ideal tests for systematicity in connectionist net-  ity for language processing (as in the case of ‘a blicket is
   works (Phillips and Halford 1997, Phillips 1999).        a ...’). Human success in these cases is clearly not due to
                                                            some virtually instantaneous “training” of our synaptic
   Given the type of S-R conditioning employed in           weights. I submit, therefore, that the fundamental chal-
Phillips experiment, one naturally supposes that the ‘un-   lenge posed by these “hard cases” should be formulated
derlying logical rule’ that Phillips currently has in mind  essentially along the following lines:
is tantamount to the rule of modus ponens employed in
the example I offered above. However, as we have now                 Demonstrate that a single holistic ANN, de-
seen, the crucial human success that Phillips highlights       ploying eliminativist, non-classical representations
in not dependent on a simple application of a given infer-     could, as a manifest side-effect of its prior training,
ence pattern (or even repeated applications of that pat-       perform successfully on either of the “hard” tasks
tern) as occurs in the Fiffle example I provided. Rather       we have considered here.3
it depends upon the composition of prior reasoning skills       3 I regard a network’s success on a task, T, as a manifest
(a composition involving both analogical reasoning and      side-effect of prior training just in case the following two con-
deduction by a process of elimination) combined with an     ditions hold: (1) it is clear that prior training had in some way
ability to extend inference patterns to novel data.         contributed to the success; (2) the network’s prior training in-

It might now be objected that the challenge just formu-          interactions with other modules, but for computational
lated is unfair, because my wording clearly precludes            reasons they should still be regarded as distinct modules.
any solution founded upon the interactions of multiple           However, in that same paper, I argued that humans are
connectionist modules. However, solutions predicated             demonstrably able to employ their skill modules in novel
upon the interactions of separate connectionist modules          combinations. To place the issue in a very small nutshell,
represent a radical departure from the pure connection-          the mere fact that humans can follow specific kinds of
ist paradigm. Such modular architectures share much in           novel rules, within mere seconds after being told such a
common with traditional, symbolic AI approaches to in-           rule, suffices to show that the brain can transfer infor-
duction and problem solving, in that much of their pro-          mation (or data) along sets of combinatorially adequate
cessing power derives not from the vector and settling           pathways between the separate skill modules. I argued
operations that characterize the “new” paradigm (involv-         further, by an examination of logically possible cases,
ing weight and activation vectors), but from cooperative         that the existence of such combinatorial pathways entails
data exchanges between separate modules.                         that at least one of several types of classically recognized
   I return to these issues presently, but let us first con-     architectures is present in the human cognitive system.
sider a different sort of objection that may arise. It might     Space limits do not permit a detailed recapitulation of
be argued that the “challenge” I pose above is not es-           these arguments. However, I submit that a modular ap-
pecially worrisome for the connectionist. After all, it          proach to achieving the impressive “side effects” noted
is well known that connectionist networks often display          in the “hard cases” which have concerned us, does not
emergent side effects. Furthermore, we know that some            represent the type of solution that would appeal to re-
networks trained via backpropagation have already dis-           searchers who view connectionism as a radically new
played some degree of compositionality, as evidenced             paradigm. In any case, neither the hybrid-modular ap-
in the capacity of the St. John & McClelland network             proach, nor the single-holistic-network approach has yet
(1990) to assign correct semantic interpretations to novel       been shown to yield side-effects even approaching those
sentences. In reply, it should be noted that the degree          involved in the transverse patterning example.
of skill compositionality, required to solve the transverse
patterning task, is of a radically different kind than any                               References
compositionality displayed by networks that assign se-            Boden, M. & Niklasson, L. (2000). Semantic system-
mantic representations to novel sentences. The degree of            aticity and context in connectionist networks. Connec-
semantic compositionality displayed by Hadley & Hay-                tion Science, 12, 111-142.
ward’s (1997) Hebbian network is markedly greater than
that evidenced by St. John and McClelland, but even the           Fodor, J.A. & Pylyshyn, Z.W. (1988). Connectionism
Hadley-Hayward network displays no compositionality                 and cognitive architecture: a critical analysis. Cogni-
of entirely separate skills.                                        tion, 28, 3-71.
   Indeed, I know of no non-modular connectionist
network, whether eliminativist or not, which exhibits             Hadley, R.F. (1993). Connectionism, explicit rules, and
skill compositionality remotely approaching the level re-           symbolic manipulation. Minds and machines, 3, 183–
quired in the transverse patterning problem. Admittedly,            200.
in the case of the Marcus generalization task (a blicket is
a ...), it may not be obvious at first blush that humans em-      Hadley, R.F. (1994a). Systematicity in connectionist
ploy multiple, separate prior skills to solve the task, but         language learning. Mind and Language, 9, 247-272.
we know that, at the very least, a capacity to understand a
range of natural language is presupposed. Moreover, this          Hadley, R.F. (1994b). Systematicity revisited: reply
capacity is so complex and multi-faceted, that any num-             to Christiansen and Chater and Niklasson and van
ber of competent linguists would affirm that a variety of           Gelder. Mind and Language, 9, 431-444.
separate skills are involved.4
                                                                  Hadley, R.F. & Hayward, M.B. (1997). Strong seman-
   Returning now to earlier comments on a modular ap-               tic systematicity from Hebbian connectionist learning,
proach to skill compositionality, I would stress that, in           Minds and Machines, 7, 1-37.
my view, such an approach is promising. Indeed, I
have argued in (Hadley, 1999) that whenever a variety             Hadley, R.F. (1999). Connectionism and novel combi-
of markedly distinct skills are involved in a task (such as         nations of skills: implications for cognitive architec-
the skills I have noted above), it is likely that separate          ture. Minds and Machines, 9, 197-221.
modules are involved. Such modules may very well be
spatially distributed, and subject to some degree of noisy        Hadley, R.F., Rotaru-Varga, A., Arnold, D.V., & Cardei,
                                                                    V.C. (to appear). Syntactic systematicity arising from
volved no task possessing an underlying structure identical to      semantic predictions in a Hebbian-competitive net-
that of task T.
    4 Examples of such separate skills include: (1) the ability     work, Connection Science.
to recognize distinct words, (2) the ability to recognize highly
ungrammatical sentences, (3) the ability to form the past tense   Marcus, G. (1998). Rethinking eliminative connection-
of verbs.                                                           ism. Cognitive Psychology, Vol. 37.

Marcus, G. (in press). The Algebraic Mind. (Cam-
 bridge, MA: MIT Press).
Phillips, S. (2000). Constituent similarity and system-
 aticity: the limits of first-order connectionism. Con-
 nection Science, 12, 1-19.
Niklasson, L.F. and van Gelder, T. (1994). On being
 systematically connectionist. Mind and Language, 9,
 288-302.
St. John, M.F. and McClelland, J.L. (1990). Learning
 and applying contextual constraints in sentence com-
 prehension. Artificial Intelligence, 46, 217-257.

