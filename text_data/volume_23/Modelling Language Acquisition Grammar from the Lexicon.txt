UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Modelling Language Acquisition: Grammar from the Lexicon?
Permalink
https://escholarship.org/uc/item/9tg9p5fz
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 23(23)
Authors
Howell, Steve R.
Becker, Suzanna
Publication Date
2001-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

              M odelling Language A cquisition:G ram m ar from the Lexicon?
                            Steve R .H ow ell(show ell@ hypatia.psychology.m cm aster.ca)
            D epartm entofPsychology,M cM asterU niversity,1280 M ain StreetW est,H am ilton,O N Canada
                                         Suzanna Becker (becker@ m cm aster.ca)
            D epartm entofPsychology,M cM asterU niversity,1280 M ain StreetW est,H am ilton,O N Canada
                                                               techniques still m aturing. W e do not w ant to include
                            A bstract                          any m ore than absolutely necessary in a m odel of
                                                               language if w e are to be successful. Thus, it is
      A neural netw ork m odel of language acquisition is      im portantto be explicitaboutourassum ptions,in term s
   introduced,based on and m otivated by current research      of pre-linguistic m ental representations,or of w hat w e
   in psychology and linguistics. Itincludes both sem antic-   can exclude from ourm odelorinclude only as inputs.
   feature representations of w ords and localist linguistic      W e assum e here thatm odelling any of the low -level
   representations of w ords. The netw ork learns to           acoustic properties of language is unnecessary for our
   associate the sem antic features of w ords to their
                                                               purposes. W hile issues such as phonem ic segm entation
   linguistic labels,as w ellas to predictthe nextw ord in the
   corpus. This is interpreted to m odelboth the acquisition   are im portant for language, those auditory tasks are
   of a lexicon, and the beginnings of syntax or gram m ar     arguably w ell-learned by the tim e of vocabulary
   (w ord order). The relationship of lexical learning to      acquisition. Further,m odelling to the levelofacoustics
   gram m ar learning is exam ined, and sim ilarities to the   is too com putationally dem anding to include in a m odel
   hum an data found. The results m ay provide supportfor      oflanguage acquisition atpresent.
   the ‘G ram m arfrom the Lexicon’,or‘em ergentgram m ar’        If w e consider the start of vocabulary acquisition to
   position.                                                   be at the age of the child’s first w ord, typically 8-12
                                                               m onths,then w e can ask the follow ing question. W hat
                        Introduction                           cognitive capacities does the child have prior to that
   H ow do children acquire language? M ore generally,         point? W hatdoes language have to build upon? Som e
how does any abstract language learner acquire                 suggestthatthere is a considerable am ount.
language? W hen w e attem pt to m odel language                   Lakoff and colleagues (Lakoff, 1986; Lakoff &
processing via com puter sim ulation,w hatshould w e be        Johnson, 1999) suggest that the child has reached an
attem pting to m odel,m ature adult perform ance,or the        adequate level of concept form ation prior to the
developm ental schedule of a child? W hat can such a           developm ent of language. Few w ould argue, w e
m odel hope to tell us about the process of language           believe, that pre-linguistic children m ust have som e
acquisition in hum an infants?                                 kind of internal representation of the w orld, som e
   These are som e ofthe questions m otivating oureffort       understanding that a cat is fuzzy and can be patted,
to m odellanguage processing. M uch evidence exists as         even if they don’tknow the w ords cat,or pat,or fuzzy.
to the usefulness of the connectionist m odelling              Lakoff argues that children’s sensorim otor experience
enterprise for the understanding of hum an language in         is continually building up these pre-linguistic concepts,
general.H ow ever,as w e seek to m odel m ore fully the        concepts that are very specific and concrete, and that
actualprocessing,and even production,of language,in            these concepts enable the child to function in their
a behaviouralfashion,w e consider itvery im portantto          lim ited w orld.
take a developm ental approach to hum an language                 W ith all of this cognitive m achinery already w ell
processing. That is, a com plete m odel of language            established,the language learning problem has happily
processing should first becom e a m odel of language           becom e m uch sim pler. If a child already has a concept
acquisition. Evidence suggests that a m odel of                for things like ‘cat’, then w hen it begins to learn the
language acquisition in children should provide the            w ord forcat,itis really only attaching a linguistic label
foundation necessary to scale up to a m odel of m ore          to a category of sensorim otor experience that it has
m ature language processing,as w e shallsee.                   previously built up. The learning of w ords is thus
                                                               reduced to the learning of labels for things. The
D evelopm entalLanguage A cquisition                           attributes of those things and the relationships betw een
   In considering a developm ental m odel of language,         them are allpredeterm ined (atleastatthis stage)by the
one im portant aspect is the lim its of the enterprise.        child’s environm entalexperience. O f course,nouns fit
That is, w here does language acquisition start, and           into this view pointw ith greaterease than do verbs;itis
w here does it end? Language is a very com plex                harderto pointto a verb than a noun.
cognitive activity, and our connectionist m odelling

   This is the traditional view in developm ental           factm ore observable than the verbs. Furtherm ore,the
psycholinguistics according to G illette et al. (G illette, im ageability of a w ord is m ore im portant than the
G leitm an,G lietm an,& Lederer,1999). A s they point       lexical class. The m ost observable verbs are learned
out how ever, this view has lim its. Specifically, they     before the less observable initialnouns,accounting for
show evidence that only som e w ords can be derived         the few rare early verbs in children’s vocabularies.
solely via extralinguistic context.                            So, im ageability or concreteness is the m ost
   It is w ell know n that there is an overw helm ing       im portant aspect of the early w ords, nouns and verbs
preponderance of nouns in children’s early speech,not       alike,and itdeterm ines the order in w hich they tend to
only in English butin m ostlanguages,w hile adults,of       be learned by children. This result argues against the
course, have a m uch m ore equal balance. Several           discontinuity hypothesis, and supports Lakoff’s early
explanations have been offered for this distinction.The     concepts and the borders that w e have draw n for our
discontinuity hypothesis holds that the cognitive           language m odelling enterprise. H ow ever, w hat of the
capacities of children are fundam entally differentfrom     less im ageable w ords? H ow are they learned?
adults. Thus, at som e point after the start of                G illette et al. also find evidence for the successive
developm ent of language children’s cognitive capacity      im portance ofnoun co-occurrence inform ation and then
for language changes. G entner describes the noun           argum ent structure. That is, for later learning of the
learning advantage as due to the conceptualcom plexity      less im ageable w ords (m ostly verbs), observing w hich
of the w ays in w hich the tw o classes, noun and verb,     previously know n nouns co-occurin a sentence w ith the
describe the w orld (Cited in G illette et al,1999).That    yet unknow n w ord label helps greatly to uniquely
is,nouns describe objectconcepts,w hile verbs describe      identify the concept. Thus rather than im ageability
relations betw een objects. The latter w ould obviously     determ ining exactly w hich object w e are talking about
be the m ore com plicated task,since it depends on the      over m ultiple experiences, for m any verbs the nouns
success of the form er. A s G illette et al point out, by   involved act to identify it. Thus if the noun ‘ball’ is
this interpretation learning w ords is notjusta m atter of  paired w ith a yet unknow n w ord, the concept
associating labels to concepts. Significant conceptual      ‘throw ing’ m ay be activated for m any learners,
learning m ustoccur as w ell. If true,this interpretation   allow ing them to infer that the unknow n w ord m eans
w ould argue againstthe conceptualization of language-      ‘to throw ’ (G illette etal,1999). A rgum entstructure is
age children as relatively conceptually stable, and         yeta furtherstep to verb inference. G illette etal.show
w ould also invalidate one of the assum ptions of our       that the num ber and position of nouns in the speech
m odelling approach.                                        stream reliably cues w hich verb concept the unknow n
   Fortunately, G illette et al. offer a different          w ord could be.
interpretation,the continuity hypothesis,w hich assum es       A tthis pointin the child’s language learning w e have
that children are conceptually equipped to understand       m oved beyond initial lexical learning and are in the
at least those concepts that underlie the w ords that       realm of syntax. The first w ords (m ainly nouns) have
adults typically use w ith them , both nouns and verbs.     been learned w ithout reference to other w ords, their
H ow ever,they argue that it is still possible to account   sheer im ageability enabling them to be inferred from
for children’s initial restriction to noun learning,using   the adultto child speech stream and the extralinguistic
instead the different inform ational requirem ents of       evidence. The next step involves the use of these
w ords thatare necessary to uniquely identify them from     concrete nouns to help infer the less im ageable verb
extralinguistic context. They refer to their hypothesis     m eanings in the speech stream ,and from there the child
as an inform ation-based account, and describe several      is no longer learning w ords solely from the
experim ents thatsupportthis account.                       extralinguistic context. The lexical structure of
   M ost im portantly G illette et al. provide strong       utterances now assists the child as w ell. For exam ple,
evidence that learnability is not prim arily based on       the first few verbs learned, w hen experienced in adult
lexicalclass. Thatis,itis notw hethera w ord is a noun      speech and involving a novel object, w ill cue the
ora verb thatdeterm ines ifitcan be learned solely from     inference of the new noun labeland,depending on the
observation. Rather, they dem onstrate that the real        particular verb,even the type of noun involved. The
distinction is based upon the w ord’s im ageability or      circular, bootstrapping process of language learning is
concreteness.                                               on its w ay (for further evidence concerning verbs and
   Itis obvious thatthe very firstw ords m ustbe learned    nouns respectively,see G oldberg,1999; Sm ith,1999).
solely by the child attem pting to discovercontingencies    Before long new w ords w ill no longer require explicit
betw een sound categories and aspects of the w orld,        extralinguistic contextatall. The school-age child w ill
over m any different exem plars. G illette et al.           begin reading and acquiring new w ords solely by
dem onstrate that the very first w ords used by m others    lexical constraints, allow ing them to exhibit the
to their children are the m ost straightforw ardly          incredible w ord acquisition rates that have been
observable ones,and thatas a group,the nouns are in         reported (e.g.Bates & G oodm an,1999).

   O f course, once the child’s lexicon has reached a     ‘terrain’ is such that the representation w ill tend to be
certain level of com plexity, perhaps 300 w ords (Bates   draw n dow n into one basin or another, and the larger
and G oodm an,1999) the m ulti-w ord stage begins,and     basins w ill be m ore likely to capture the activation.
gram m ar acquisition begins to be a consideration as     They “attract” the activation.
w ellas justlexicalacquisition.                              Furtherm ore, this attractor representation is
                                                          hierarchical. G eneral or superordinate concepts m ight
G ram m ar From the Lexicon                               have very large basins,containing w ithin them sm aller
   Bates and G oodm an (1999) exam ine the highly         basins corresponding to m ore specific butsem antically
linked developm ent of gram m ar and the lexicon.They     related term s.
provide evidence for the em ergence of gram m ar             O bviously this landscape representation only applies
directly from the lexicon itself. Specifically,they show  to the lexicon. H ow does gram m ar enter into the
the lack of evidence for any dissociation of lexicaland   picture? W ell,ifthe lexicon is view ed as basins in this
gram m aticalprocesses (draw n from studies ofearly and   representation landscape,or state-space,then gram m ar
late talkers, focal brain lesions, and developm ent       is contained in the transitions thatoccur betw een these
deficits), along w ith the very tight developm ental ties states. That is, a true dynam ical system consists not
betw een the tw o.For exam ple,lexicalstatus attw enty    only of these representations in state space, but also
m onths (during children’s vocabulary burst) is the       relationships that influence m ovem ent from one
single bestpredictorofgram m aticalstatus at28 m onths    representation to another. Further details can be found
(during children’s gram m ar burst), w ith a correlation  in Elm an (1995), but for our present purposes it is
coefficient of betw een .70 and .84. This is in fact as   sufficient to realize that this dynam ical system s
good a statistical relationship as that betw een separate approach provides a possible m echanism for the
m easures of gram m ar! This is good evidence that        im plem entation of the w ord-inference processes
gram m ar does em erge,at least partially,from the very   described earlier (G illette et al. (1999). Certainly a
grow th ofthe lexicon itself.                             recurrentnetlike the one w e w illdescribe in ourm odel
   This finding, as w ell as those of G illette et al, is is capable of exhibiting the behaviour of a dynam ical
im portantto the developm entofourm odeloflanguage        system , w ith the hidden unit representations
acquisition, as if gram m ar developm ent is em ergent    corresponding to the state-space vectors and the
from lexicaldevelopm ent,then w e w antto be sure that    operation of the netw ork providing the transitions
w e do not m odel them as tw o separate m odules or       betw een them based on the values stored in its w eights.
com ponents. Rather,a centraltenetof our m odelis to      It can also be argued that the cortex operates in this
use a single process orarchitecture to learn both lexicon fashion (Elm an, 1995; Sulis, 2001, personal
and gram m ar. Furtherm ore, lexical developm ent         com m unication),and thus thatthe sam e explanation can
should precede gram m atical, and gram m atical           be offered forhum an language processing.
developm entshould nottake off untilsufficientlexical
developm ent has occurred. O ur m odel should exhibit     The ‘C om plete’Early Language A cquirer
the sam e sort of acquisition (and production,               Let us assum e,then,that the child (or m odel) starts
eventually)behaviouras a child.                           w ith pre-existing pre-linguistic concepts of the w orld,
                                                          upon w hich linguistic labels w ill be learned by direct
A D ynam icalSystem s A pproach                           instruction as w ell as sim ple exposure. This pre-
   Elm an (1995) suggests view ing the process of initial existing conceptual structure im plies either a pre-
lexical and gram m atical developm ent as a dynam ical    existing m ental representation (sem antic landscape) or
system , or attractor m odel, w hich can be learned       one that is quickly built up as w ords are m atched to
through a process of predicting the input. Roughly        concepts.
speaking, this view point is as follow s. A language          In ourm odel,w e assum e thatthe child begins syntax
learner’s sem antic representations are very lim ited at  or gram m ar learning at the sam e tim e as it begins
first, m uch like a flat three-dim ensional landscape.    learning vocabulary. H ow ever, since there is little
Then as the learner develops stable categories and        evidence that gram m ar is directly instructed (Bates &
concepts,the landscape gradually develops depressions     G oodm an, 1999), unlike noun acquisition (Sm ith,
or basins, each basin corresponding to a w ord or         1999),and since gram m ar is inherently m ore com plex,
concept,and each experience ofthatconceptdeepening        gram m arlearning does notreally succeed untilafterthe
the basin,untileventually the landscape is fullof deep    m ost prim al of the lexical attractors have been firm ly
and w ide basins of attraction. These are “attractors”    set and the lexical and syntactical bootstrapping has
since, w hile any partial or confused activation of a     begun. In essence, gram m ar exposure begins at the
sem antic representation w illtend to indicate a place on sam e tim e as lexical learning, but gram m ar learning
the landscape notin one ofthese basins,the slope ofthe    doesn’t effectively take place until the lexical
                                                          representations are solidified.

   Thus w e w ould expect to see exactly that behaviour    layer and a sem antic output layer m eans that sem antic
that is seen in real children; lexical developm ent        features can be read off for any given linguistic input,
proceeds at an ever accelerating pace, then w hen the      indicating w hether the netw ork has learned the
lexical foundation is firm enough (the ‘noise’ or          “m eaning” ofthe w ord.
uncertainty in the language environm ent is reduced
enough) the m ental m achinery can focus on syntactic
relationships, and gram m atical learning should
                                                                Sem antic          Linguistic        Linguistic
accelerate. O ur m odel should exhibit exactly this           A utoencoder        A utoencoder       Predictor
behaviour if it is capturing the essence of hum an
language acquisition.
                        M ethod
   O ur experim ent consists of training our m odel of             H idden Layer
language acquisition m any tim es from different initial                                    ContextLayer
conditions, and analyzing the perform ance results for
their fit to the hum an data and im provem ents over the
controlm odels.                                                 Sem antic          Linguistic
                                                                  Input               Input
The M odel
   The m odel of language acquisition discussed herein
                                                           Figure 1: M odified SRN architecture, including
(see Figure 1) takes as inputuniquely identified w ords
                                                           standard SRN hidden layer and contextlayer,standard
(localist input representations), and learns how those
                                                           linguistic prediction layer, and novel sem antic
w ords can be used in sentences. This is not a novel
                                                           autoencoderand linguistic autoencoder.
undertaking (see Elm an,1990,1993;H ow ell& Becker,
2000). H ow ever, w hat is new to this m odel is the
addition of a second set of inputs, sem antic-feature
                                                              Finally,the inclusion of both linguistic autoencoding
inputs. By ‘sem antic’,how ever w e actually m ean pre-
                                                           (w ord learning) and linguistic prediction (gram m ar
linguistic sem antics or m eaning (e.g. sensorim otor
                                                           learning) allow s us to explore the dynam ics of the
features). Thus, instead of abstractly m anipulating
                                                           m odel, and determ ine if the learning behaviour of the
locally-distributed w ord representations,a process that
                                                           m odelm aps to the hum an developm entaldata. Thatis,
has been characterized by M cClelland as “learning a
                                                           does the w ord learning have to reach a critical m ass
language by listening to the radio” (Elm an,1990),our
                                                           before the gram m arlearning proceeds? D oes a jum p in
m odel attem pts to ground the w ord representations in
                                                           lexical com petence lead to a linked jum p in
reality by associating them w ith a setof these sem antic
                                                           gram m atical com petence? If so, then perhaps the
features foreach w ord.
                                                           m odelcan provide evidence for the view thatgram m ar
   Furtherm ore,the netw ork is notperform ing only the
                                                           em erges from the lexicon (Bates and G oodm an,1999).
prediction task that is argued (Elm an,1990) to lead to
an internalization of basic aspects of gram m ar,
specifically w ord-order relationships. Instead,itis also  M odelD etails
learning, sim ultaneously, to m em orize its linguistic
inputs,m em orize its sem antic inputs,and associate the      There are tw o input layers and three output layers.
tw o together, such that either one alone w ill elicit the The sem antic output layer is paired w ith the sem antic
other.                                                     input layer. Both are 68 nodes in size, since the
   W hy construct a neural netw ork m odel in this w ay?   sem antic feature dim ensions taken from H inton &
First, using a sim ple recurrent architecture and          Shallice (1991)have 68 dim ensions.
prediction task retains the successfulgram m ar learning      The linguistic input and the linguistic outputs are of
capabilities thathave been show n so w ellby Elm an and    size 29, since the vocabulary has 29 w ords. Both
colleagues. Second, adding a sem antic layer w ill         linguistic outputs are tied to the sam e set of linguistic
eventually allow for the use of phonem ic input            inputs, but w here the linguistic autoencoder’s training
representations and the binding of those phonem es into    signal is the present input, the linguistic predictor’s
w ords (through sem antic constancy across each            training signalis the inputatthe nexttim e step.
individualw ord)although the netw ork discussed in this       Both the hidden and the contextlayer are of size 75,
paper does notdealw ith phonem ic inputs,only w hole-      and the hidden-to-contexttransfer function is a one-to-
w ord inputs. Third,the inclusion of the sem antic input   one copy w ith no hysteresis (see H ow ell & Becker,
                                                           2000). The hidden-to-context connection is not

trainable, but the context-to-hidden feedback                  For the present purposes, our analysis is lim ited to
connection is trained exactly as is eitherofthe input-to-    the lexical-gram m atical relationship (and further
hidden connections.                                          sem antic results are notreported). Specifically,over24
                                                             sim ulation runs the m ean peak lexical accuracy w as
Training Environm ent                                        96.6 percentcorrect,w hile the m ean peak gram m atical
   The netw ork is trained on a corpus of text derived       accuracy w as 37.33 percentcorrect (See Figure 2).
from a sm all (390 w ord) subset of Elm an’s original
corpus oftw o and three w ord sentences w ith a 29 w ord
vocabulary (Elm an,1990).                                                              1
   Input to the sem antic input layer w as derived from
                                                                 A verage A ccuracy
                                                                                      0.8
the above corpus by converting each w ord in the corpus                                                          LexicalA ccuracy
to the w ord’s sem antic featural representation,using a                              0.6
setoffeatures derived from H inton and Shallice (1991).                               0.4
This feature set includes only the sensory features and
excludes the sem antic-association ones found in the                                  0.2
                                                                                                                       G ram m aticalA ccuracy
original. This resulted in a binary distributed                                        0
representation for the sem antic layer. Itis im portantto                                   1   52
note that on language tasks a binary distributed                                                     103   154   205     256   307   358   409   460
representation w ould often be expected to learn faster                                                     Training Epoch
than a localist representation, as it provides m ore
inform ation to the netw ork.
   The netw ork’s w eights w ere random ly initialized,        Figure 2:A verage A ccuracy Curves O ver24 Runs
and training proceeded as usual for Sim ple Recurrent
N etw orks, using the backpropagation algorithm                 Com parisons w ith ‘control’ or partial netw orks
(Rum elhart,H inton,and W illiam s,1986).                    lacking the sem antic or lexical autoencoder task also
   Training proceeded until reasonable levels of             indicate that each task is learned faster and m ore
accuracy w ere achieved. Trial runs of up to 1500            accurately in the experim ental netw ork than in the
epochs indicated that the net asym ptoted near 500           control netw orks. O nly the gram m atical results are
epochs,so training did notin any case proceed beyond         reported here,how ever.
500 epochs.                                                     For control netw ork 1, w hich included only the
   Error m easures and accuracy m easures w ere logged       linguistic prediction task (i.e.an originalElm an net)the
at each input presentation, but averaged over the 390        peak prediction accuracy w as low est, w ith a m ean of
patterns to one value perepoch oftraining.                   18.5 percent correct, and significantly different from
                                                             the experim entalnetw ork via t-test(n = 10,p<0.0001).
              R esults & D iscussion                            For control netw ork 2, w hich excluded only the
The firstfinding from the various runs ofthe netw ork is     sem antic layers,the peak prediction accuracy,achieved
that the net does in fact learn. There had been som e        at epoch 500, w as significantly low er than the
concern that the dem ands of three different tasks           experim entalnetw ork (n=10,m = 28.4,p <0.0001).
sharing a single hidden layer m ightcause significantor         For control netw ork 3, w hich excluded only the
even catastrophic interference in the learning tasks. O n    linguistic autoencoder, the peak prediction accuracy
the contrary, w ith a hidden layer size only slightly        w as stilllow erthan the experim entalnetw ork (m =37.1)
larger than the largest input layer (75 com pared to 68      butthe difference did notreach significance (n=10,p =
for the sem antic input layer) the net learned all three     0.137).
tasks.                                                          Thus,training allthree tasks through a single hidden
   Furtherm ore, the tasks w ere learned in the expected     layer apparently creates synergies that allow each to
order. Thatis,judging from the errorcurves the binary        proceed fasterthan itw ould alone.
distributed sem antic representations w ere learned m ost       M ost interesting, how ever, w as the relationship
quickly (since they provide m ore inform ation for the       betw een the lexical and gram m atical accuracy curves
netw ork to learn on,i.e.m ore bits turned on) follow ed     for the experim ental netw ork. W e expect that if our
by the localist linguistic autoencoding and then the         m odel is catching im portant elem ents of the hum an
localistlinguistic prediction. Prediction,of course,is a     language learning experience, then it should exhibit
m ore difficult task than autoencoding or                    lexicon-then-gram m ar behavior.Certainly,as discussed
‘m em orization’,justas verb learning is a m ore difficult   above, the speed of learning (rate of error decline)
task than noun learning.                                     exhibits this relationship,butthatis only to be expected
                                                             by the difficulty of the tasks. A better question is

w hetherthe netw ork exhibits the lexical-to-gram m atical                 helpfulthroughout.This w ork has been supported by a
perform ance correlations that Bates and G oodm an                         Post-graduate Fellow ship from the N ational Sciences
(1999)discuss. Thatis,does the lexicalperform ance at                      and Engineering Research CouncilofCanada (N SERC)
tim e T correlate w ellw ith the gram m aticalperform ance                 to SRH ,and a research grantfrom N SERC to SB.
atsom e laterpoint?
   By analogy to the m ethods cited in Bates and
G oodm an (1999),a pointon the lexicalaccuracy curves                                            R eferences
that could be considered the ‘lexical burst’ w as                          Bates, E. and G oodm an, J. C. (1999). O n the
identified (approx.Epoch 108). Then,since there w as                          em ergence of gram m ar from the lexicon. In
no explicit ‘gram m ar burst’ w ithin our tim e w indow a                     M acW hinney, B. (Ed.) (1999). The Em ergence of
set of correlations w as calculated to the gram m atical                      Language.         N ew Jersey: Law rence Erlbaum
perform ance atvarious tim e lags from the lexicalburst                       A ssociates.
(see Figure 3). The results indicate that the highest                      Elm an, J. L. (1990). Finding structure in tim e.
correlation,approxim ately .80,is from the lexicalburst                       Cognitive Science,14,179-211.
to gram m aticalperform ance 75 epochs later.                              Elm an, J. L. (1993). Learning and developm ent in
                                                                              neural netw orks: The im portance of starting sm all.
                                                                              Cognition,48.71-99.
                             1.0                                           Elm an,J.L.(1995).Language as a dynam ical system .
    Lexical-G ram m atical
                                                                              In R.F.Port& T.van G elder(Eds.),M ind as M otion:
                             0.8
                                                                              Explorations in the D ynam ics of Cognition.
                             0.6                                              Cam bridge,M A :M IT Press.
                                                                           G illette, J., G leitm an, H ., G leitm an, L., Lederer, A .
                             0.4
                                                                              (1999). H um an sim ulations of vocabulary learning.
         C orrelation
                             0.2                                              Cognition,73,135-176.
                                                                           G oldberg, A . E. (1999). The em ergence of the
                             0.0                                              sem antics of argum ent structure constructions. In
                                   -10   1   10   25   50   75   100 125      M acW hinney, B. (Ed.) (1999). The Em ergence of
                                         G ram m aticalEpoch-Lag              Language.         N ew Jersey: Law rence Erlbaum
                                                                              A ssociates.
                                                                           H inton, G . E. & Shallice, T. (1991). Lesioning a
                                                                              connectionist netw ork: Investigations of acquired
  Figure 3:Lexical-G ram m aticalCorrelations (n = 24)
                                                                              dyslexia,PsychologicalReview,98,74-75.
                                                                           H ow ell, S. R. & Becker, S. (2000). M odelling
   This is sim ilar to Bates & G oodm an’s cited
                                                                              language acquisition at m ultiple tem poral scales.
correlation betw een lexical status at 20 m onths and
                                                                              Proceedings of the Cognitive Science Society, 2000,
gram m atical status at 28 m onths in children. A t first,
                                                                              1031.
the sim ilarity m ay seem lim ited,since our m odel uses
                                                                           Lakoff,G .and Johnson,M . (1999).Philosophy in the
only 29 w ords,notthe 300-plus thatis suggested to be
                                                                              flesh: The em bodied m ind and its challenge to
the criticalm ass required for gram m ar learning. A lso,
                                                                              western thought.N ew Y ork: Basic Books.
our sentences use only the 29 w ords from the m odel’s
                                                                           Lakoff,G .(1987). W om en,fire and dangerous things:
vocabulary, and no unfam iliar w ords, and w ord
                                                                              W hatcategories revealaboutthe m ind. Chicago and
learning is being represented by average accuracy
                                                                              London:U niversity ofChicago Press.
curves. Further,gram m atical status is being m easured
                                                                           Rum elhart, D .E., H inton, G . E. & W illiam s, R. J.
by accuracy of prediction rather than M ean Length of
                                                                              (1986) Learning internal representations by error
U tterance (M LU ).
                                                                              propagation. In J. L. M cClelland, D . E. Rum elhart
   H ow ever,w e believe these results are prom ising,and
                                                                              and the PD P Research G roup, Parallel D istributed
thatfurtherstudy is w arranted. W e have already begun
                                                                              Processing: Explorations in the M icrostructure of
to run sim ulations thatuse larger vocabularies,and that
                                                                              Cognition. V ol. 1: Foundations (pp. 318-362).
provide analogues of M LU m easurem ents for
                                                                              Cam bridge,M A :The M IT press.
gram m atical status, in order to elucidate further the
                                                                           Sm ith, L. B. (1999). Children’s noun learning: H ow
m odel’s relationship to hum an perform ance.
                                                                              generallearning processes m ake specialized learning
                                                                              m echanism s. In M acW hinney,B.(Ed.) (1999). The
                                   A cknow ledgm ents                         Em ergence of Language. N ew Jersey: Law rence
Thanks to G eorge Lakoff,w hose w ritings and personal                        Erlbaum A ssociates.
conversations inspired som e of this w ork, and to
D am ian Jankow icz, w hose com m ents w ere m ost

