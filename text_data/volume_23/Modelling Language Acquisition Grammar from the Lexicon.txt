UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Modelling Language Acquisition: Grammar from the Lexicon?

Permalink
https://escholarship.org/uc/item/9tg9p5fz

Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 23(23)

Authors
Howell, Steve R.
Becker, Suzanna

Publication Date
2001-01-01
Peer reviewed

eScholarship.org

Powered by the California Digital Library
University of California

M odelling Language A cquisition:G ram m ar from the Lexicon?
Steve R .H ow ell(show ell@ hypatia.psychology.m cm aster.ca)
D epartm entofPsychology,M cM asterU niversity,1280 M ain StreetW est,H am ilton,O N Canada

Suzanna Becker (becker@ m cm aster.ca)
D epartm entofPsychology,M cM asterU niversity,1280 M ain StreetW est,H am ilton,O N Canada

A bstract
A neural netw ork m odel of language acquisition is
introduced,based on and m otivated by current research
in psychology and linguistics. Itincludes both sem anticfeature representations of w ords and localist linguistic
representations of w ords. The netw ork learns to
associate the sem antic features of w ords to their
linguistic labels,as w ellas to predictthe nextw ord in the
corpus. This is interpreted to m odelboth the acquisition
of a lexicon, and the beginnings of syntax or gram m ar
(w ord order). The relationship of lexical learning to
gram m ar learning is exam ined, and sim ilarities to the
hum an data found. The results m ay provide supportfor
the ‘G ram m arfrom the Lexicon’,or‘em ergentgram m ar’
position.

Introduction
H ow do children acquire language? M ore generally,
how does any abstract language learner acquire
language? W hen w e attem pt to m odel language
processing via com puter sim ulation,w hatshould w e be
attem pting to m odel,m ature adult perform ance,or the
developm ental schedule of a child? W hat can such a
m odel hope to tell us about the process of language
acquisition in hum an infants?
These are som e ofthe questions m otivating oureffort
to m odellanguage processing. M uch evidence exists as
to the usefulness of the connectionist m odelling
enterprise for the understanding of hum an language in
general.H ow ever,as w e seek to m odel m ore fully the
actualprocessing,and even production,of language,in
a behaviouralfashion,w e consider itvery im portantto
take a developm ental approach to hum an language
processing. That is, a com plete m odel of language
processing should first becom e a m odel of language
acquisition. Evidence suggests that a m odel of
language acquisition in children should provide the
foundation necessary to scale up to a m odel of m ore
m ature language processing,as w e shallsee.

D evelopm entalLanguage A cquisition
In considering a developm ental m odel of language,
one im portant aspect is the lim its of the enterprise.
That is, w here does language acquisition start, and
w here does it end? Language is a very com plex
cognitive activity, and our connectionist m odelling

techniques still m aturing. W e do not w ant to include
any m ore than absolutely necessary in a m odel of
language if w e are to be successful. Thus, it is
im portantto be explicitaboutourassum ptions,in term s
of pre-linguistic m ental representations,or of w hat w e
can exclude from ourm odelorinclude only as inputs.
W e assum e here thatm odelling any of the low -level
acoustic properties of language is unnecessary for our
purposes. W hile issues such as phonem ic segm entation
are im portant for language, those auditory tasks are
arguably w ell-learned by the tim e of vocabulary
acquisition. Further,m odelling to the levelofacoustics
is too com putationally dem anding to include in a m odel
oflanguage acquisition atpresent.
If w e consider the start of vocabulary acquisition to
be at the age of the child’s first w ord, typically 8-12
m onths,then w e can ask the follow ing question. W hat
cognitive capacities does the child have prior to that
point? W hatdoes language have to build upon? Som e
suggestthatthere is a considerable am ount.
Lakoff and colleagues (Lakoff, 1986; Lakoff &
Johnson, 1999) suggest that the child has reached an
adequate level of concept form ation prior to the
developm ent of language. Few w ould argue, w e
believe, that pre-linguistic children m ust have som e
kind of internal representation of the w orld, som e
understanding that a cat is fuzzy and can be patted,
even if they don’tknow the w ords cat,or pat,or fuzzy.
Lakoff argues that children’s sensorim otor experience
is continually building up these pre-linguistic concepts,
concepts that are very specific and concrete, and that
these concepts enable the child to function in their
lim ited w orld.
W ith all of this cognitive m achinery already w ell
established,the language learning problem has happily
becom e m uch sim pler. If a child already has a concept
for things like ‘cat’, then w hen it begins to learn the
w ord forcat,itis really only attaching a linguistic label
to a category of sensorim otor experience that it has
previously built up. The learning of w ords is thus
reduced to the learning of labels for things. The
attributes of those things and the relationships betw een
them are allpredeterm ined (atleastatthis stage)by the
child’s environm entalexperience. O f course,nouns fit
into this view pointw ith greaterease than do verbs;itis
harderto pointto a verb than a noun.

This is the traditional view in developm ental
psycholinguistics according to G illette et al. (G illette,
G leitm an,G lietm an,& Lederer,1999). A s they point
out how ever, this view has lim its. Specifically, they
show evidence that only som e w ords can be derived
solely via extralinguistic context.
It is w ell know n that there is an overw helm ing
preponderance of nouns in children’s early speech,not
only in English butin m ostlanguages,w hile adults,of
course, have a m uch m ore equal balance. Several
explanations have been offered for this distinction.The
discontinuity hypothesis holds that the cognitive
capacities of children are fundam entally differentfrom
adults. Thus, at som e point after the start of
developm ent of language children’s cognitive capacity
for language changes. G entner describes the noun
learning advantage as due to the conceptualcom plexity
of the w ays in w hich the tw o classes, noun and verb,
describe the w orld (Cited in G illette et al,1999).That
is,nouns describe objectconcepts,w hile verbs describe
relations betw een objects. The latter w ould obviously
be the m ore com plicated task,since it depends on the
success of the form er. A s G illette et al point out, by
this interpretation learning w ords is notjusta m atter of
associating labels to concepts. Significant conceptual
learning m ustoccur as w ell. If true,this interpretation
w ould argue againstthe conceptualization of languageage children as relatively conceptually stable, and
w ould also invalidate one of the assum ptions of our
m odelling approach.
Fortunately, G illette et al. offer a different
interpretation,the continuity hypothesis,w hich assum es
that children are conceptually equipped to understand
at least those concepts that underlie the w ords that
adults typically use w ith them , both nouns and verbs.
H ow ever,they argue that it is still possible to account
for children’s initial restriction to noun learning,using
instead the different inform ational requirem ents of
w ords thatare necessary to uniquely identify them from
extralinguistic context. They refer to their hypothesis
as an inform ation-based account, and describe several
experim ents thatsupportthis account.
M ost im portantly G illette et al. provide strong
evidence that learnability is not prim arily based on
lexicalclass. Thatis,itis notw hethera w ord is a noun
ora verb thatdeterm ines ifitcan be learned solely from
observation. Rather, they dem onstrate that the real
distinction is based upon the w ord’s im ageability or
concreteness.
Itis obvious thatthe very firstw ords m ustbe learned
solely by the child attem pting to discovercontingencies
betw een sound categories and aspects of the w orld,
over m any different exem plars. G illette et al.
dem onstrate that the very first w ords used by m others
to their children are the m ost straightforw ardly
observable ones,and thatas a group,the nouns are in

factm ore observable than the verbs. Furtherm ore,the
im ageability of a w ord is m ore im portant than the
lexical class. The m ost observable verbs are learned
before the less observable initialnouns,accounting for
the few rare early verbs in children’s vocabularies.
So, im ageability or concreteness is the m ost
im portant aspect of the early w ords, nouns and verbs
alike,and itdeterm ines the order in w hich they tend to
be learned by children. This result argues against the
discontinuity hypothesis, and supports Lakoff’s early
concepts and the borders that w e have draw n for our
language m odelling enterprise. H ow ever, w hat of the
less im ageable w ords? H ow are they learned?
G illette et al. also find evidence for the successive
im portance ofnoun co-occurrence inform ation and then
argum ent structure. That is, for later learning of the
less im ageable w ords (m ostly verbs), observing w hich
previously know n nouns co-occurin a sentence w ith the
yet unknow n w ord label helps greatly to uniquely
identify the concept. Thus rather than im ageability
determ ining exactly w hich object w e are talking about
over m ultiple experiences, for m any verbs the nouns
involved act to identify it. Thus if the noun ‘ball’ is
paired w ith a yet unknow n w ord, the concept
‘throw ing’ m ay be activated for m any learners,
allow ing them to infer that the unknow n w ord m eans
‘to throw ’ (G illette etal,1999). A rgum entstructure is
yeta furtherstep to verb inference. G illette etal.show
that the num ber and position of nouns in the speech
stream reliably cues w hich verb concept the unknow n
w ord could be.
A tthis pointin the child’s language learning w e have
m oved beyond initial lexical learning and are in the
realm of syntax. The first w ords (m ainly nouns) have
been learned w ithout reference to other w ords, their
sheer im ageability enabling them to be inferred from
the adultto child speech stream and the extralinguistic
evidence. The next step involves the use of these
concrete nouns to help infer the less im ageable verb
m eanings in the speech stream ,and from there the child
is no longer learning w ords solely from the
extralinguistic context. The lexical structure of
utterances now assists the child as w ell. For exam ple,
the first few verbs learned, w hen experienced in adult
speech and involving a novel object, w ill cue the
inference of the new noun labeland,depending on the
particular verb,even the type of noun involved. The
circular, bootstrapping process of language learning is
on its w ay (for further evidence concerning verbs and
nouns respectively,see G oldberg,1999; Sm ith,1999).
Before long new w ords w ill no longer require explicit
extralinguistic contextatall. The school-age child w ill
begin reading and acquiring new w ords solely by
lexical constraints, allow ing them to exhibit the
incredible w ord acquisition rates that have been
reported (e.g.Bates & G oodm an,1999).

O f course, once the child’s lexicon has reached a
certain level of com plexity, perhaps 300 w ords (Bates
and G oodm an,1999) the m ulti-w ord stage begins,and
gram m ar acquisition begins to be a consideration as
w ellas justlexicalacquisition.

G ram m ar From the Lexicon
Bates and G oodm an (1999) exam ine the highly
linked developm ent of gram m ar and the lexicon.They
provide evidence for the em ergence of gram m ar
directly from the lexicon itself. Specifically,they show
the lack of evidence for any dissociation of lexicaland
gram m aticalprocesses (draw n from studies ofearly and
late talkers, focal brain lesions, and developm ent
deficits), along w ith the very tight developm ental ties
betw een the tw o.For exam ple,lexicalstatus attw enty
m onths (during children’s vocabulary burst) is the
single bestpredictorofgram m aticalstatus at28 m onths
(during children’s gram m ar burst), w ith a correlation
coefficient of betw een .70 and .84. This is in fact as
good a statistical relationship as that betw een separate
m easures of gram m ar! This is good evidence that
gram m ar does em erge,at least partially,from the very
grow th ofthe lexicon itself.
This finding, as w ell as those of G illette et al, is
im portantto the developm entofourm odeloflanguage
acquisition, as if gram m ar developm ent is em ergent
from lexicaldevelopm ent,then w e w antto be sure that
w e do not m odel them as tw o separate m odules or
com ponents. Rather,a centraltenetof our m odelis to
use a single process orarchitecture to learn both lexicon
and gram m ar. Furtherm ore, lexical developm ent
should precede gram m atical, and gram m atical
developm entshould nottake off untilsufficientlexical
developm ent has occurred. O ur m odel should exhibit
the sam e sort of acquisition (and production,
eventually)behaviouras a child.

A D ynam icalSystem s A pproach
Elm an (1995) suggests view ing the process of initial
lexical and gram m atical developm ent as a dynam ical
system , or attractor m odel, w hich can be learned
through a process of predicting the input. Roughly
speaking, this view point is as follow s. A language
learner’s sem antic representations are very lim ited at
first, m uch like a flat three-dim ensional landscape.
Then as the learner develops stable categories and
concepts,the landscape gradually develops depressions
or basins, each basin corresponding to a w ord or
concept,and each experience ofthatconceptdeepening
the basin,untileventually the landscape is fullof deep
and w ide basins of attraction. These are “attractors”
since, w hile any partial or confused activation of a
sem antic representation w illtend to indicate a place on
the landscape notin one ofthese basins,the slope ofthe

‘terrain’ is such that the representation w ill tend to be
draw n dow n into one basin or another, and the larger
basins w ill be m ore likely to capture the activation.
They “attract” the activation.
Furtherm ore, this attractor representation is
hierarchical. G eneral or superordinate concepts m ight
have very large basins,containing w ithin them sm aller
basins corresponding to m ore specific butsem antically
related term s.
O bviously this landscape representation only applies
to the lexicon. H ow does gram m ar enter into the
picture? W ell,ifthe lexicon is view ed as basins in this
representation landscape,or state-space,then gram m ar
is contained in the transitions thatoccur betw een these
states. That is, a true dynam ical system consists not
only of these representations in state space, but also
relationships that influence m ovem ent from one
representation to another. Further details can be found
in Elm an (1995), but for our present purposes it is
sufficient to realize that this dynam ical system s
approach provides a possible m echanism for the
im plem entation of the w ord-inference processes
described earlier (G illette et al. (1999). Certainly a
recurrentnetlike the one w e w illdescribe in ourm odel
is capable of exhibiting the behaviour of a dynam ical
system , w ith the hidden unit representations
corresponding to the state-space vectors and the
operation of the netw ork providing the transitions
betw een them based on the values stored in its w eights.
It can also be argued that the cortex operates in this
fashion (Elm an, 1995; Sulis, 2001, personal
com m unication),and thus thatthe sam e explanation can
be offered forhum an language processing.

The ‘C om plete’Early Language A cquirer
Let us assum e,then,that the child (or m odel) starts
w ith pre-existing pre-linguistic concepts of the w orld,
upon w hich linguistic labels w ill be learned by direct
instruction as w ell as sim ple exposure. This preexisting conceptual structure im plies either a preexisting m ental representation (sem antic landscape) or
one that is quickly built up as w ords are m atched to
concepts.
In ourm odel,w e assum e thatthe child begins syntax
or gram m ar learning at the sam e tim e as it begins
learning vocabulary. H ow ever, since there is little
evidence that gram m ar is directly instructed (Bates &
G oodm an, 1999), unlike noun acquisition (Sm ith,
1999),and since gram m ar is inherently m ore com plex,
gram m arlearning does notreally succeed untilafterthe
m ost prim al of the lexical attractors have been firm ly
set and the lexical and syntactical bootstrapping has
begun. In essence, gram m ar exposure begins at the
sam e tim e as lexical learning, but gram m ar learning
doesn’t effectively take place until the lexical
representations are solidified.

Thus w e w ould expect to see exactly that behaviour
that is seen in real children; lexical developm ent
proceeds at an ever accelerating pace, then w hen the
lexical foundation is firm enough (the ‘noise’ or
uncertainty in the language environm ent is reduced
enough) the m ental m achinery can focus on syntactic
relationships, and gram m atical learning should
accelerate. O ur m odel should exhibit exactly this
behaviour if it is capturing the essence of hum an
language acquisition.

layer and a sem antic output layer m eans that sem antic
features can be read off for any given linguistic input,
indicating w hether the netw ork has learned the
“m eaning” ofthe w ord.

Sem antic
A utoencoder

Linguistic
A utoencoder

Linguistic
Predictor

M ethod
O ur experim ent consists of training our m odel of
language acquisition m any tim es from different initial
conditions, and analyzing the perform ance results for
their fit to the hum an data and im provem ents over the
controlm odels.

H idden Layer
ContextLayer

Sem antic
Input

Linguistic
Input

The M odel
The m odel of language acquisition discussed herein
(see Figure 1) takes as inputuniquely identified w ords
(localist input representations), and learns how those
w ords can be used in sentences. This is not a novel
undertaking (see Elm an,1990,1993;H ow ell& Becker,
2000). H ow ever, w hat is new to this m odel is the
addition of a second set of inputs, sem antic-feature
inputs. By ‘sem antic’,how ever w e actually m ean prelinguistic sem antics or m eaning (e.g. sensorim otor
features). Thus, instead of abstractly m anipulating
locally-distributed w ord representations,a process that
has been characterized by M cClelland as “learning a
language by listening to the radio” (Elm an,1990),our
m odel attem pts to ground the w ord representations in
reality by associating them w ith a setof these sem antic
features foreach w ord.
Furtherm ore,the netw ork is notperform ing only the
prediction task that is argued (Elm an,1990) to lead to
an internalization of basic aspects of gram m ar,
specifically w ord-order relationships. Instead,itis also
learning, sim ultaneously, to m em orize its linguistic
inputs,m em orize its sem antic inputs,and associate the
tw o together, such that either one alone w ill elicit the
other.
W hy construct a neural netw ork m odel in this w ay?
First, using a sim ple recurrent architecture and
prediction task retains the successfulgram m ar learning
capabilities thathave been show n so w ellby Elm an and
colleagues. Second, adding a sem antic layer w ill
eventually allow for the use of phonem ic input
representations and the binding of those phonem es into
w ords (through sem antic constancy across each
individualw ord)although the netw ork discussed in this
paper does notdealw ith phonem ic inputs,only w holew ord inputs. Third,the inclusion of the sem antic input

Figure 1: M odified SRN architecture, including
standard SRN hidden layer and contextlayer,standard
linguistic prediction layer, and novel sem antic
autoencoderand linguistic autoencoder.

Finally,the inclusion of both linguistic autoencoding
(w ord learning) and linguistic prediction (gram m ar
learning) allow s us to explore the dynam ics of the
m odel, and determ ine if the learning behaviour of the
m odelm aps to the hum an developm entaldata. Thatis,
does the w ord learning have to reach a critical m ass
before the gram m arlearning proceeds? D oes a jum p in
lexical com petence lead to a linked jum p in
gram m atical com petence? If so, then perhaps the
m odelcan provide evidence for the view thatgram m ar
em erges from the lexicon (Bates and G oodm an,1999).

M odelD etails
There are tw o input layers and three output layers.
The sem antic output layer is paired w ith the sem antic
input layer. Both are 68 nodes in size, since the
sem antic feature dim ensions taken from H inton &
Shallice (1991)have 68 dim ensions.
The linguistic input and the linguistic outputs are of
size 29, since the vocabulary has 29 w ords. Both
linguistic outputs are tied to the sam e set of linguistic
inputs, but w here the linguistic autoencoder’s training
signal is the present input, the linguistic predictor’s
training signalis the inputatthe nexttim e step.
Both the hidden and the contextlayer are of size 75,
and the hidden-to-contexttransfer function is a one-toone copy w ith no hysteresis (see H ow ell & Becker,
2000). The hidden-to-context connection is not

R esults & D iscussion
The firstfinding from the various runs ofthe netw ork is
that the net does in fact learn. There had been som e
concern that the dem ands of three different tasks
sharing a single hidden layer m ightcause significantor
even catastrophic interference in the learning tasks. O n
the contrary, w ith a hidden layer size only slightly
larger than the largest input layer (75 com pared to 68
for the sem antic input layer) the net learned all three
tasks.
Furtherm ore, the tasks w ere learned in the expected
order. Thatis,judging from the errorcurves the binary
distributed sem antic representations w ere learned m ost
quickly (since they provide m ore inform ation for the
netw ork to learn on,i.e.m ore bits turned on) follow ed
by the localist linguistic autoencoding and then the
localistlinguistic prediction. Prediction,of course,is a
m ore difficult task than autoencoding or
‘m em orization’,justas verb learning is a m ore difficult
task than noun learning.

1
0.8
LexicalA ccuracy

0.6
0.4
0.2

G ram m aticalA ccuracy

460

409

358

307

256

205

154

103

0
52

The netw ork is trained on a corpus of text derived
from a sm all (390 w ord) subset of Elm an’s original
corpus oftw o and three w ord sentences w ith a 29 w ord
vocabulary (Elm an,1990).
Input to the sem antic input layer w as derived from
the above corpus by converting each w ord in the corpus
to the w ord’s sem antic featural representation,using a
setoffeatures derived from H inton and Shallice (1991).
This feature set includes only the sensory features and
excludes the sem antic-association ones found in the
original. This resulted in a binary distributed
representation for the sem antic layer. Itis im portantto
note that on language tasks a binary distributed
representation w ould often be expected to learn faster
than a localist representation, as it provides m ore
inform ation to the netw ork.
The netw ork’s w eights w ere random ly initialized,
and training proceeded as usual for Sim ple Recurrent
N etw orks, using the backpropagation algorithm
(Rum elhart,H inton,and W illiam s,1986).
Training proceeded until reasonable levels of
accuracy w ere achieved. Trial runs of up to 1500
epochs indicated that the net asym ptoted near 500
epochs,so training did notin any case proceed beyond
500 epochs.
Error m easures and accuracy m easures w ere logged
at each input presentation, but averaged over the 390
patterns to one value perepoch oftraining.

1

Training Environm ent

For the present purposes, our analysis is lim ited to
the lexical-gram m atical relationship (and further
sem antic results are notreported). Specifically,over24
sim ulation runs the m ean peak lexical accuracy w as
96.6 percentcorrect,w hile the m ean peak gram m atical
accuracy w as 37.33 percentcorrect (See Figure 2).

A verage A ccuracy

trainable, but the context-to-hidden feedback
connection is trained exactly as is eitherofthe input-tohidden connections.

Training Epoch

Figure 2:A verage A ccuracy Curves O ver24 Runs
Com parisons w ith ‘control’ or partial netw orks
lacking the sem antic or lexical autoencoder task also
indicate that each task is learned faster and m ore
accurately in the experim ental netw ork than in the
control netw orks. O nly the gram m atical results are
reported here,how ever.
For control netw ork 1, w hich included only the
linguistic prediction task (i.e.an originalElm an net)the
peak prediction accuracy w as low est, w ith a m ean of
18.5 percent correct, and significantly different from
the experim entalnetw ork via t-test(n = 10,p<0.0001).
For control netw ork 2, w hich excluded only the
sem antic layers,the peak prediction accuracy,achieved
at epoch 500, w as significantly low er than the
experim entalnetw ork (n=10,m = 28.4,p <0.0001).
For control netw ork 3, w hich excluded only the
linguistic autoencoder, the peak prediction accuracy
w as stilllow erthan the experim entalnetw ork (m =37.1)
butthe difference did notreach significance (n=10,p =
0.137).
Thus,training allthree tasks through a single hidden
layer apparently creates synergies that allow each to
proceed fasterthan itw ould alone.
M ost interesting, how ever, w as the relationship
betw een the lexical and gram m atical accuracy curves
for the experim ental netw ork. W e expect that if our
m odel is catching im portant elem ents of the hum an
language learning experience, then it should exhibit
lexicon-then-gram m ar behavior.Certainly,as discussed
above, the speed of learning (rate of error decline)
exhibits this relationship,butthatis only to be expected
by the difficulty of the tasks. A better question is

w hetherthe netw ork exhibits the lexical-to-gram m atical
perform ance correlations that Bates and G oodm an
(1999)discuss. Thatis,does the lexicalperform ance at
tim e T correlate w ellw ith the gram m aticalperform ance
atsom e laterpoint?
By analogy to the m ethods cited in Bates and
G oodm an (1999),a pointon the lexicalaccuracy curves
that could be considered the ‘lexical burst’ w as
identified (approx.Epoch 108). Then,since there w as
no explicit ‘gram m ar burst’ w ithin our tim e w indow a
set of correlations w as calculated to the gram m atical
perform ance atvarious tim e lags from the lexicalburst
(see Figure 3). The results indicate that the highest
correlation,approxim ately .80,is from the lexicalburst
to gram m aticalperform ance 75 epochs later.

Lexical-G ram m atical
C orrelation

1.0
0.8
0.6
0.4
0.2
0.0
-10

1

10

25

50

75

100 125

G ram m aticalEpoch-Lag

Figure 3:Lexical-G ram m aticalCorrelations (n = 24)
This is sim ilar to Bates & G oodm an’s cited
correlation betw een lexical status at 20 m onths and
gram m atical status at 28 m onths in children. A t first,
the sim ilarity m ay seem lim ited,since our m odel uses
only 29 w ords,notthe 300-plus thatis suggested to be
the criticalm ass required for gram m ar learning. A lso,
our sentences use only the 29 w ords from the m odel’s
vocabulary, and no unfam iliar w ords, and w ord
learning is being represented by average accuracy
curves. Further,gram m atical status is being m easured
by accuracy of prediction rather than M ean Length of
U tterance (M LU ).
H ow ever,w e believe these results are prom ising,and
thatfurtherstudy is w arranted. W e have already begun
to run sim ulations thatuse larger vocabularies,and that
provide analogues of M LU m easurem ents for
gram m atical status, in order to elucidate further the
m odel’s relationship to hum an perform ance.

A cknow ledgm ents
Thanks to G eorge Lakoff,w hose w ritings and personal
conversations inspired som e of this w ork, and to
D am ian Jankow icz, w hose com m ents w ere m ost

helpfulthroughout.This w ork has been supported by a
Post-graduate Fellow ship from the N ational Sciences
and Engineering Research CouncilofCanada (N SERC)
to SRH ,and a research grantfrom N SERC to SB.

R eferences
Bates, E. and G oodm an, J. C. (1999). O n the
em ergence of gram m ar from the lexicon. In
M acW hinney, B. (Ed.) (1999). The Em ergence of
Language.
N ew Jersey: Law rence Erlbaum
A ssociates.
Elm an, J. L. (1990). Finding structure in tim e.
Cognitive Science,14,179-211.
Elm an, J. L. (1993). Learning and developm ent in
neural netw orks: The im portance of starting sm all.
Cognition,48.71-99.
Elm an,J.L.(1995).Language as a dynam ical system .
In R.F.Port& T.van G elder(Eds.),M ind as M otion:
Explorations in the D ynam ics of Cognition.
Cam bridge,M A :M IT Press.
G illette, J., G leitm an, H ., G leitm an, L., Lederer, A .
(1999). H um an sim ulations of vocabulary learning.
Cognition,73,135-176.
G oldberg, A . E. (1999). The em ergence of the
sem antics of argum ent structure constructions. In
M acW hinney, B. (Ed.) (1999). The Em ergence of
Language.
N ew Jersey: Law rence Erlbaum
A ssociates.
H inton, G . E. & Shallice, T. (1991). Lesioning a
connectionist netw ork: Investigations of acquired
dyslexia,PsychologicalReview,98,74-75.
H ow ell, S. R. & Becker, S. (2000). M odelling
language acquisition at m ultiple tem poral scales.
Proceedings of the Cognitive Science Society, 2000,
1031.
Lakoff,G .and Johnson,M . (1999).Philosophy in the
flesh: The em bodied m ind and its challenge to
western thought.N ew Y ork: Basic Books.
Lakoff,G .(1987). W om en,fire and dangerous things:
W hatcategories revealaboutthe m ind. Chicago and
London:U niversity ofChicago Press.
Rum elhart, D .E., H inton, G . E. & W illiam s, R. J.
(1986) Learning internal representations by error
propagation. In J. L. M cClelland, D . E. Rum elhart
and the PD P Research G roup, Parallel D istributed
Processing: Explorations in the M icrostructure of
Cognition. V ol. 1: Foundations (pp. 318-362).
Cam bridge,M A :The M IT press.
Sm ith, L. B. (1999). Children’s noun learning: H ow
generallearning processes m ake specialized learning
m echanism s. In M acW hinney,B.(Ed.) (1999). The
Em ergence of Language. N ew Jersey: Law rence
Erlbaum A ssociates.

