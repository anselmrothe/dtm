UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Evaluating the Effects of Natural Language Generation Techniques on Reader Satisfaction
Permalink
https://escholarship.org/uc/item/8b15b07x
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 23(23)
Authors
Callaway, Charles B.
Lester, James C.
Publication Date
2001-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

                              Evaluating the Effects of Natural Language
                           Generation Techniques on Reader Satisfaction
                                     Charles B. Callaway (cbcallaw@eos.ncsu.edu)
                                           James C. Lester (lester@csc.ncsu.edu)
                                                     The IntelliMedia Initiative
                                                Department of Computer Science
                                                 North Carolina State University
                                                      Raleigh, NC 27695 USA
                           Abstract                                 Zwaan 1997; Hoover 1997) have focused on mechanical
                                                                    aspects such as reading rate, and did not have access to
   We are witnessing the emergence of a new technology              computational mechanisms for producing the texts they
   for dynamically creating stories tailored to the interests
   of particular readers. Narrative prose generators offer          studied.
   much promise for literacy education, but designing them             To study the changes in perceived text quality stem-
   for maximal effectiveness requires us to understand their        ming from alterations to the underlying text generation
   effect on readers. This article describes the evaluation         architecture, we conducted a formal study gauging the
   of S TORY B OOK, an implemented narrative prose genera-
   tion system that produces original fairy tales in the Little     satisfaction of subjects reading narratives. The study in-
   Red Riding Hood domain. S TORY B OOK creates two to              volved the following:
   three pages of text consistently represented at the deep
   linguistic structure level. Because of this, we can for-         • A consistent representation mechanism which allows
   mally evaluate multiple versions of a single story and be           for the representation of characters, props, locations,
   assured that the content is identical across all versions.          actions and descriptions found in a narrative environ-
   We produced five such versions of two separate stories
   which were compared by a pool of twenty upper division              ment. Holding these entities constant for the duration
   students in English and analyzed with an ANOVA test.                of an experiment ensures that the stories seen by the
   While the results are most informative for designers of             study participants will have identical plots and details
   narrative prose generators, it provides important baselines         except for the variations cued from the experiment’s
   for research into natural language systems in general.
                                                                       parameters.
                       Introduction                                 • A story generation mechanism that, when given the
                                                                       story representation and the experimental parameters,
The emerging technology of narrative prose generation,
                                                                       can produce a specified set of narratives. Our story
which dynamically creates stories tailored to the inter-
                                                                       generator, named S TORY B OOK, creates narratives in
ests of particular readers, offers great promise for liter-
                                                                       the Little Red Riding Hood fairy tale domain. These
acy education. However, to design effective narrative
                                                                       narratives can be tailored to produce a variety of gram-
prose generation software for literacy education, it is im-
                                                                       matical, lexical, and propositional effects.
portant to understand how students perceive texts created
by these algorithms. Do the results of studies based on             • A pool of readers familiar with narratives and the writ-
human-produced texts apply? How does computer con-                     ing process itself. Thus we conducted a study involv-
trol of minute aspects of text production affect readers?              ing 20 upper division undergraduate students majoring
Do readers have quantitative reactions to fundamental al-              in English or Communication. Each student read two
terations in texts as we expect they would?                            distinct Little Red Riding Hood stories averaging two
   As a result of recent work in formally evaluated                    hours per student.
language generation technology (Smith & Hipp 1994;
Robin & McKeown 1995; Allen et al. 1996; Callaway                      There are two primary types of comparisons upon
& Lester 1997; Lester & Porter 1997; Young 1999), we                which an evaluation of a text-producing system can fo-
are seeing an increased awareness of the issues involved            cus: human text vs. computer text and computer text
in successfully generating texts dynamically for specific           vs. computer text. Although there are a number of
target audiences. However, these systems are focused                pre-existing Little Red Riding Hood texts available for
more towards task effectiveness evaluations or explana-             comparison via the World Wide Web, formally compar-
tion generation and are not suitable for the significant dif-       ing such narratives with those produced by computer
ficulties in creating literary narratives. And while there          presents a difficult problem: there is no known objec-
exist story generation systems capable of producing nar-            tive metric for quantitatively evaluating narrative prose
ratives (Meehan 1976; Lebowitz 1985; Lang 1997), none               in terms of how it performs as a story. Simple metrics
of these systems has been formally evaluated by read-               exist for evaluation at the sentence level (e.g., number of
ers. Furthermore, various formal studies on reading com-            words, depth of embedding, etc.), but a narrative per se
prehension (Kintsch & Keenan 1973; Graesser, Millis &               cannot be considered to be just a collection of sentences

that are not related to each other. In addition, because       revision component is usually introduced to aggre-
narrative is not a “deductive” domain, it cannot be evalu-     gate those small sentences (protosentences) into larger
ated in terms of correctness by a panel of human judges.       multi-proposition sentences. It is usually assumed that
To overcome these problems, we instead opted for a com-        these larger sentences will be more readable and less
puter vs. computer style of evaluation that investigates       choppy or visually jarring. For example, “The wolf
whether certain architectural elements are necessary or        saw her” and “She was walking down the path” might
useful when generating narrative prose.                        be aggregated to produce “The wolf saw her walking
   To study the effects of different textual effects upon      down the path.”
the readers, we implemented five versions of the S TO -
RY B OOK story generator (Callaway & Lester 2001). Be-      • Lexical Choice: Narrative planners also tend to cre-
cause a fully interleaved experiment would have required       ate sentences that frequently repeat the same lexical
an excessive amount of time, we required each student          items due to efficiency concerns. To combat this, a
to compare two versions of each story rather than all          lexical choice component performs local search to de-
five versions. Each story was identical in plot, content,      termine when one lexical item can be replaced by an-
and form, but differed in terms of propositions per sen-       other. Thus instead of character dialogue where char-
tence, grammatical fluency, or choice of lexical forms.        acters always introduce utterances with “said”, that
The results of the study show that the participants were       lexical item can be replaced by “mentioned”, “whis-
highly discriminative of the texts which they read, pre-       pered”, “replied”, etc.
ferring some versions over others. The readers most
strongly dispreferred narratives lacking important gram-    • Surface Realizer: Once the lexical and structural con-
matical structures and greatly dispreferred those with a       tent of a set of sentences has been determined, they
small number of propositions per sentence. These re-           must be converted to text. This is accomplished by
sults have important implications for the design of liter-     checking to make sure that each sentence is grammat-
acy software.                                                  ical, imposes linear constraints, and adds morphologi-
                                                               cal changes as necessary. The result is text which can
        The S TORY B OOK Narrative Prose                       be sent to a word processor, a web browser, or saved
                                                               as a text file.
                       Generator
S TORY B OOK is a narrative prose generator that produces      The existence of these architectural modules allowed
narratives in the Little Red Riding Hood domain. To         us to conduct an architectural ablation experiment. By
write stories, S TORY B OOK takes a narrative plan con-     selectively removing a component, the resulting text of a
sisting of the actors, scenes, props and temporally or-     story will be changed in some way. The sentence plan-
dered events and descriptions as input from a narrative     ner and surface realizer are vital components; without
planner. It then evolves that narrative plan into the final them text cannot be produced at all. However, removing
text seen by the reader using a sequence of architectural   the other elements will result in text that we expect to
components:                                                 be degraded in some fashion. Thus without the discourse
• Discourse History: When populating a story with in-       history, the system will be unable to produce pronouns in
   formation from a conceptual network, noun phrases        a reliable way or appropriately mark nouns for definite-
   must be marked for indefiniteness if they have not       ness. Without the revision component, the system will
   yet been mentioned in the story or if they are not       produce a minimal number of propositions per sentence
   visually available references to the character or nar-   due to the lack of clause aggregation. Finally, removing
   rator in focus. Furthermore, frequently repeating        the lexical choice module will result in a decrease in the
   noun phrases can be pronominalized to avoid sen-         variability of the lexical forms of verbs or nouns.
   tences like “Grandmother knew that Grandmother had          Given these three architectural modules, there are 23
   asked Grandmother’s daughter to send some cakes          or 8 possible pairwise comparisons between the presence
   to Grandmother” rather than “Grandmother knew she        or absence of each component when used to produce a
   had asked her daughter to send her some cakes.” A dis-   narrative:
   course history tracks noun phrase concepts and allows    1. All three components are used.
   them to be marked for definiteness or pronominaliza-     2. Only the revision module is unused.
   tion.                                                    3. Only the lexical choice module is unused.
                                                            4. Only the discourse history module is unused.
• Sentence Planner: A sentence planner maps charac-         5. Only the revision module is used.
   ters, props, locations, actions and descriptions to con-
                                                            6. Only the lexical choice module is used.
   crete grammatical structures in a sentential specifica-
                                                            7. Only the discourse history module is used.
   tion. Thus in the example just mentioned, “grand-
   mother” is mapped to the main subject while “know”       8. None of the three components are used.
   is mapped to the main verb, etc.
                                                               Due to the constraints on the logistics of the evaluation
• Revision: Because narrative planners create their con-    process, we decided to utilize only five of those pairwise
   tent as a series of single proposition sentences, a      comparisons: the two all-or-none approaches and the

three approaches where one specific architectural mod-     1. On an absolute scale of how good fairy tales should be
ule is ablated. The remaining three unused approaches         in general, evaluate the story on an A–F scale (A, B,
would evaluate the enhancement that each module adds          C, D, F).
to the whole rather than what is missing when each is
removed. We contend this approach leads to a slightly      2. Style: Did the author use a writing style appropriate
more effective comparison, because as more modules are        for fairy tales?
removed from the generation process, the resulting prose
becomes progressively less desirable and thus unwanted     3. Grammaticality: How would you grade the syntactic
effects from the absence of multiple architectural mod-       quality of the story?
ules might overlap and affect a test subject’s experience
                                                           4. Flow: How well did the sentences flow from one to the
in ways that could not be teased apart when analyzing
the data.                                                     next?
   The ablation of these architectural modules can have    5. Diction: How interesting or appropriate were the au-
a significant impact in text quality, even over very small
                                                              thor’s word choices?
text segments, as is shown in the following excerpts:
• Complete (Version A), with revision, lexical choice,     6. Readability: How hard was it to read the prose?
   and discourse history all turned on:
                                                           7. Logicality: Did the story omit crucial information or
     She had not gone far when she met a wolf.                seem out of order?
     “Hello,” greeted the wolf, who was a cunning look-    8. Detail: Did the story have the right amount of detail,
   ing creature. He asked, “Where are you going?”             or too much or too little?
     “I am going to my grandmother’s house,” she
   replied.                                                9. Believability: Did the story’s characters behave as you
                                                              would expect?
• No Revision (Version B), with lexical choice and dis-
   course history turned on:
                                                                 Figure 1: Grading factors presented to readers
     She had not gone far. She met a wolf.
     “Hello,” greeted the wolf. The wolf was a cunning
   looking creature. He asked, “Where are you going?”           “Hello,” said the wolf. The wolf was the cunning
     “I am going to my grandmother’s house,” she              looking creature. The wolf said, “Where is Little Red
   replied.                                                   Riding Hood going?”
                                                                “Little Red Riding Hood is going to Little Red Rid-
• No Lexical Choice (Version C), with revision and dis-       ing Hood’s grandmother’s house,” said Little Red Rid-
   course history turned on:                                  ing Hood.
     She had not gone far when she met a wolf.
     “Hello,” said the wolf, who was a cunning looking                   Evaluation Methodology
   creature. He said, “Where are you going?”               To test the S TORY B OOK system, we created a modestly
     “I am going to my grandmother’s house,” she said.     sized narrative planner (implemented as a finite state au-
                                                           tomaton containing approximately 200 states), enough to
• No Discourse History (Version D), with revision and      produce two stories comprising two and three pages re-
   lexical choice turned on:                               spectively. Furthermore, we fixed the content of those
                                                           stories and ran five different versions of S TORY B OOK
     Little Red Riding Hood had not gone far when Little   on each one: (A) all three components working, (B) revi-
   Red Riding Hood met the wolf.                           sion turned off, (C) lexical choice turned off, (D) the dis-
     “Hello,” greeted the wolf, who was the cunning        course history turned off, and finally (E) a version with
   looking creature. The wolf asked, “Where is Little      all three components turned off. This resulted in ten total
   Red Riding Hood going?”                                 narratives which we presented to our test subjects using
     “Little Red Riding Hood is going to Little Red Rid-   the grading factors found in Figure 1. While versions
   ing Hood’s grandmother’s house,” replied Little Red     were different in the sense that certain modules were ei-
   Riding Hood.                                            ther ablated or not, the two stories differ because they
                                                           were created from two different finite state automata.
• Empty (Version E), with revision, lexical choice, and    Thus story #1 potentially has different characters, differ-
   discourse history all turned off:                       ent events and properties, and different props than story
                                                           #2 has.
     Little Red Riding Hood had not gone far. Little Red      A total of twenty students were selected from North
   Riding Hood met the wolf.                               Carolina State University’s Departments of English and

                Figure 2: Means for Story #2: 4.0 scale, 8 evaluations per Version × Grading Factor × Story
 Communication via first-come first-serve email notices.            their evaluations within 3 hours of each other at a sin-
 All of the students were registered in upper division or           gle location.
 graduate courses in those departments. Each subject was
 asked to read the directions and ask for clarifications            Subjects graded each narrative following the instruc-
 before the evaluation proceeded and was randomly as-            tions according to an A–F scale, which we then converted
 signed their evaluation task. Subjects were not informed        to a quantified scale where A = 4.0, B = 3.0, C = 2.0, D
 prior to their completion of the questionnaire that the nar-    = 1.0, and F = 0.0. The resulting scores were then tallied
 ratives were produced by computer program. Subjects             and averaged. The means for both stories are shown in
 were paid $25.00 for their participation.                       Figure 2.
    Because each subject compared two versions of story             To determine the quantitative significance of the re-
 #1 to each other and two versions of story #2 to each           sults, we performed an ANOVA test over both stories.
 other, every subject saw a total of four narratives. To pre-    The analysis was conducted for three independent vari-
 vent subjects from evaluating the same types of stories in      ables (test subject, story, and version) and nine grading
 succession, we devised the following policy:                    factors (labelled GF1 – GF9, as described in Figure 1).
                                                                 Because not all possible grading combinations were per-
1. Each subject read four distinct story versions out of the     formed (only 80 observations, or 20 x 2 x 2, out of a
    total of five, two from each story (e.g., subject #1 read    possible 200, or 20 x 2 x 5, due to crossover and time
    versions A and B from story #1, and versions D and           constraints), we performed the mixed procedure analy-
    E from story #2). No subject read the same version           sis. Interactions between variables were only significant
    twice.                                                       for grading factor #9 at 0.0300 for story∗version.
                                                                    The results of the ANOVA analysis point to three sig-
2. Each version was read by the same total number of             nificant classes of narratives due to the architectural de-
    subjects (i.e., each version of every story was read by      sign of the narrative prose generator. Table 1 indicates
    8 separate subjects).                                        that the most preferred narrative class, consisting of ver-
                                                                 sions A & C, were not significantly different from each
3. Each pairwise comparison of different versions was            other overall while they did differ significantly from all
    read by two separate subjects (e.g., subjects #1 and         other versions (although there were similarities in par-
    #11 both read versions A and B of story #1 and ver-          ticular grading factors such as GF2, style, between ver-
    sions D and E of story #2).                                  sions A & B). Interestingly, the affinity for versions A &
                                                                 C is strongly correlated for story #2 (Figure 2) but only
4. For each pair of students reading the same two ver-           weakly for story #1. A two-tailed paired t-test evaluat-
    sions, the narratives were presented in opposite order       ing this difference illustrated that versions A & B were
    (e.g., subject #1 read version A first and then version      not significantly different when only story #1 was con-
    B, while subject #11 read version B first followed by        sidered, but were significantly different in story #2. The
    version A).                                                  opposite was true for versions A & C when the scores
                                                                 for each story were compared individually, even though
5. Students were randomly assigned narrative versions            the combined scores indicated versions A & C were not
    on a first-come first-serve basis; all students performed    significantly different overall.

                  Grading Factors      GF1     GF2     GF3   GF4     GF5      GF6      GF7    GF8     GF9      A LL
       C OMPLETE VS . N O R EV.        n.s.    n.s.      ∗∗  n.s.     n.s.     n.s.    n.s.    n.s.    n.s.    n.s.
       C OMPLETE VS . N O L. C.        n.s.    n.s.     n.s. n.s.     n.s.     n.s.    n.s.    n.s.    n.s.    n.s.
       C OMPLETE VS . N O D. H.         ∗∗       ∗       ∗∗   ∗∗       ∗∗      ∗∗      n.s.     ∗      n.s.     ∗∗
       C OMPLETE VS . N OTHING          ∗∗       ∗       ∗∗   ∗∗       ∗∗      ∗∗      n.s.    n.s.     ∗       ∗∗
       N O R EV. VS . N O L. C.          ∗     n.s.      ∗∗    ∗        ∗       ∗      n.s.    n.s.    n.s.     ∗∗
       N O R EV. VS . N O D. H.         ∗∗       ∗       ∗∗   ∗∗        ∗      ∗∗      n.s.    n.s.    n.s.     ∗∗
       N O R EV. VS . N OTHING          ∗∗     n.s.       ∗   ∗∗      n.s.     ∗∗      n.s.    n.s.     ∗       ∗∗
       N O L. C. VS . N O D. H.         ∗∗      ∗∗       ∗∗   ∗∗       ∗∗      ∗∗        ∗     ∗∗       ∗       ∗∗
       N O L. C. VS . N OTHING          ∗∗      ∗∗       ∗∗   ∗∗       ∗∗      ∗∗        ∗     ∗∗      ∗∗       ∗∗
       N O D. H. VS . N OTHING         n.s.    n.s.     n.s. n.s.     n.s.     n.s.    n.s.    n.s.    n.s.    n.s.
          Table 1: Combined significance values (with Bonferroni adjustment): ∗ = p < 0.01, ∗∗ = p < 0.001
                       Discussion                            clear whether a lexical choice component would play a
Indisputably, versions D & E form the least preferred        much more significant role in subject matter where the
narrative class, differing quite significantly from all      audience was more mature.
other versions while not differing significantly from each       The fact that Version B scored less favorably com-
other. Because the architectural commonality between         pared to Versions A and C indicates that revision is an
these two versions was the lack of a discourse his-          important aspect of narrative prose generation. Test sub-
tory (corresponding to a lack of grammatical confor-         jects frequently commented that Version B was “very
mity to the expected norm, especially lack of appropri-      choppy” or “didn’t seem to have good grammar”. These
ate pronominalization) while versions A, B, and C all        comments can be accounted for by the two main func-
utilized a discourse history, we conclude that this ar-      tions of the revision component: joining small sentences
chitectural component is extremely important in the de-      together and combining sentences with repetitive phrases
sign of a narrative prose generator and that any symbolic    together while deleting the repetitions. This is related
pipelined narrative prose generation system will suffer      to previous work in reading comprehension on proposi-
tremendous degradation in prose quality if a discourse       tional content. Such research (Kintsch & Keenan, 1973)
history component is not present. In addition, we con-       has shown that reading rate increases as the number of
clude that in future ablation experiments, if there is no    propositions per sentence increases. Here, however, we
other methodology for introducing pronominalizations,        have shown that a larger number of propositions per sen-
it is not even desirable to include the discourse history    tence is preferred more than a small number of proposi-
module as one of the components available for ablation.      tions per sentence, although there would certainly be an
Effects of pronominalization and topicalization were pre-    upper limit.
viously studied by Hoover (1997) although that work fo-          Another important note is that there is a difference
cused on recall rates while we concentrate on expressed      among the grading factors themselves. Grading factors
preferences.                                                 2-7 (style, flow, grammar, diction, readability and logi-
   As predicted in advance, the full version (Version A)     cality) directly relate to elements governed by the param-
scored quite well while versions lacking a discourse his-    eters and rules of the various architectural components of
tory (Versions D & E) scored quite poorly. A surprise in     the narrative prose generator. However, grading factors
the results of the analysis was the mild preference sub-     #8 and #9 (detail and believability) are more closely re-
jects had for the version missing the lexical choice com-    lated to the content of the plot line, and as such could be
ponent (Version C) over the full-fledged version. While      expected to remain relatively constant since the content
related work on word choice in spontaneous dialogues         of the narratives was held constant across all versions of
has concluded that dialogue participants tend to converge    each story. Given that the perceptions of the test sub-
onto a limited set of words (Brennan 1996), fictional nar-   jects might have “carried over” from their responses to
rative by and large does not reflect the spontaneity and     previous questions, a future evaluation might randomize
task-orientation reflected in such dialogues.                the order in which these questions are asked to see if this
   Upon analysis of the comments in the evaluations          effect persists.
specifically comparing versions A & C, it became clear           Finally, there appears to be a link between the appeal
that one principal reason was the test subjects’ belief that of the story content itself and the increase in the absolute
the increased lexical variation might prove too difficult    (GF #1) and total means for versions A, B, and C. Story
for children to read (even though we provided no indica-     #1 is a “classic” Brothers’ Grimm fairy tale in the sense
tion that the target audience was children) and thus Ver-    that it typically has a gruesome ending that serves as a
sion A compared less favorably to Version C due to the       behavioral warning to young children. Thus our story
more complex and varied words it contained. It is not        #1 ends with the wolf devouring Little Red Riding Hood

and her grandmother. More modern stories have happier        Lang, R. R. (1997). A formal model for simple narra-
endings, however, and this is reflected in our story #2        tives. Doctoral Dissertation, Department of Computer
which ends with a woodcutter killing the wolf and ex-          Science, Tulane University. New Orleans, LA.
tracting the unharmed Little Red Riding Hood and her         Lebowitz, M. (1985). Story-telling as planning and
grandmother from the wolf’s stomach. A large number            learning. Poetics, 14(3): 483–502.
of our test subjects, worried about the potential impact
on children, complained about the “horrible” ending of       Lester, J. & Porter, B. (1997). Developing and empir-
story #1 in their written comments and this reader bias        ically evaluating robust explanation generators: The
appears to have affected the overall grading scores.           KNIGHT experiments. Computational Linguistics,
                                                               23(1): 65–101.
                     Future Work                             Meehan, J. (1977). Tale-Spin, an interactive program
The existence of a computational system for generating         that writes stories. In Proceedings of the Fifth In-
complete narratives while providing access to the funda-       ternational Joint Conference on Artificial Intelligence.
mental linguistic structure offers superb opportunities for    Cambridge, MA.
future experimentation. Very fine-grained manipulation       Robin, J. & McKeown, K. (1995). Empirically designing
of texts becomes possible on a large scale; for example,       and evaluating a new revision-based model for sum-
within the discourse history, it is possible to run ablation   mary generation. Artificial Intelligence, 85(1–2).
experiments involving subject pronouns vs. object pro-
nouns, correct vs. incorrect reflexive pronouns, random      Smith, R. & Hipp, D. R. (1994). Spoken natural lan-
vs. ambient definite noun phrase marking, among many           guage dialog systems. Cambridge, Massachusetts:
others.                                                        Oxford University Press.
                                                             Young, R. M. (1999). Using Grice’s Maxim of Quantity
                  Acknowledgements                             to select the content of plan descriptions. Artificial In-
The authors wish to thank Joy Smith of NC State Uni-           telligence, 115: 215–256.
versity for her help with the statistical analysis and the
anonymous reviewers for their extremely helpful com-
ments. Support for this work was provided by the Intel-
liMedia Initiative of North Carolina State University.
                       References
Allen, J., Miller, B., Ringger, E., & Sikorski, T. (1996).
   Robust understanding in a dialogue system. Proceed-
   ings of the 34th Annual Meeting of the Association for
   Computational Linguistics, (pp. 62–70). Santa Cruz,
   CA.
Brennan, S. (1996). Lexical entrainment in spontaneous
   dialog. In Proceedings of the International Sympo-
   sium on Spoken Dialogue. Philadelphia, PA.
Callaway, C., & Lester, J. (1997). Dynamically improv-
   ing explanations: A revision-based approach to expla-
   nation generation. In Proceedings of the Fifteenth In-
   ternational Joint Conference on Artificial Intelligence,
   (pp. 952–958). Nagoya, Japan.
Callaway, C., & Lester, J. (2001). Narrative prose gener-
   ation. In Proceedings of the Seventeenth International
   Joint Conference on Artificial Intelligence, in press.
   Seattle, WA.
Graesser, A. C., Millis, K. K. & Zwaan, R. A. (1997).
   Discourse comprehension. Annual Review of Psychol-
   ogy, 48: 163–189.
Hoover, M. L. (1997). Effects of textual and cohesive
   structure on discourse processing. Discourse Pro-
   cesses, 23: 193–220.
Kintsch, W. & Keenan, J. M. (1973). Reading rate and
   retention as a function of the number of propositions in
   the base structure of sentences. Cognitive Psychology,
   5:257–274.

