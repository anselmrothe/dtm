UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Spoken Language Comprehension Improves the Efficiency of Visual Search
Permalink
https://escholarship.org/uc/item/0w83m788
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 23(23)
Authors
Tyler, Melinda J.
Spivey, Michael J.
Publication Date
2001-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

   Spoken Language Comprehension Improves the Efficiency of Visual Search
                                         Melinda J. Tyler (mjt15@cornell.edu)
                                        Department of Psychology, Cornell University
                                                      Ithaca, NY 14853 USA
                                        Michael J. Spivey (spivey@cornell.edu)
                                        Department of Psychology, Cornell University
                                                      Ithaca, NY 14853 USA
                            Abstract                                  The problem of “context” has traditionally been dealt
                                                                   with in a rather drastic fashion: researchers forcibly
   Much recent eye-tracking research has demonstrated that         ignore it. If context does not influence the primary
   visual perception plays an integral part in on-line spoken      functions of the process of interest (be it in language,
   language comprehension, in environments that closely            vision, memory, reasoning, or action), then that process
   mimic our normal interaction with our physical
   environment and other humans. To test for the inverse,
                                                                   can be thought of as an encapsulated module which will
   an influence of language on visual processing, we               permit dissection via a nicely limited set of theoretical
   modified the standard visual search task by introducing         and methodological tools. For example, prominent
   spoken linguistic input. In classic visual search tasks,        theories of visual perception and attention posit that the
   targets defined by only one feature appear to “pop-out”         visual system is functionally independent of other
   regardless of the number of distractors, suggesting a           cognitive processes (Pylyshyn, 1999; Zeki, 1993). This
   parallel search process. In contrast, when the target is        kind of modularity thesis has been applied to accounts
   defined by a conjunction of features, the number of             of language processing as well (Chomsky, 1965; Fodor,
   distractors in the display causes a highly linear increase      1983). As a result, a great deal of progress has been
   in search time, suggesting a more serial search process.
   However, we found that when a conjunction target was
                                                                   made toward developing first approximations of how
   identified by a spoken instruction presented concurrently       vision may function and how language may function.
   with the visual display, the effect of set size on search          However, recent eye-tracking studies have shown
   time was dramatically reduced. These results suggest that       evidence that visual perception constrains real-time
   the incremental linguistic processing of the two spoken         spoken language comprehension.             For example,
   target features allows the visual search process to,            temporary ambiguities in word recognition and in
   essentially, conduct two nested single-feature parallel         syntactic parsing are quickly resolved by information in
   searches instead of one serial conjunction search.              the visual context (Allopenna, Magnuson, &
                                                                   Tanenhaus, 1998; Spivey & Marian, 1998; Tanenhaus,
                                                                   Spivey-Knowlton, Eberhard, & Sedivy, 1995).
                        Introduction                               Findings like these are difficult for modular theories of
For a psycholinguist studying spoken language                      language to accommodate.
comprehension, the visual environment would be                        The present experiment demonstrates the converse:
considered “context”. However, for a vision researcher,            that language processing can constrain visual
the visual environment is the primary target of study,             perception. In a standard visual search task, a target
and auditory/linguistic information would be                       object is typically defined by a conjunction of features,
considered the “context”. Clearly, this variable use of            and reaction time increases linearly with the number of
the label “context” is due to differences in perspective,          distractors, often in the range of 15-25 milliseconds per
not due to any objective differences between language              item (Duncan & Humphreys, 1989; Treisman &
and vision. In everyday perceptual/communicative                   Gelade, 1980; Wolfe, 1994). However, when we
circumstances, humans must integrate visual and                    presented the visual display first, and then provided the
linguistic information extremely rapidly for even the              spoken target features incrementally, we found that
simplest of exercises. Consider the real-time dance of             reaction time was considerably less sensitive to the
linguistic, visual, and even gestural events that takes            number of distractors.
place during a conversation about the weather. This                   With conjunction search displays, increased reaction
continuous coreferencing between visual and linguistic             times as a linear function of set size were originally
signals may render the very idea of labeling something             interpreted as evidence for serial processing of the
as “context” arbitrary at best, and perhaps even                   objects in the display, and contrasted with the near-flat
misleading.                                                        function of reaction time by set size observed with

feature search displays -- where a single feature is       Wolfe, 1995; Motter & Holsapple, 2000), and
sufficient to identify the target object. It was argued    subsequently search for the target object in that subset
that the early stages of the visual system process         upon hearing the second-mentioned feature, then this
individual features independently and in parallel          initial immediate group selection should reduce the
(Livingstone & Hubel, 1988), allowing the target object    effective set size to only those objects in the display
to "pop out" in the display if it is discriminable by a    that share the first-mentioned feature – effectively
single feature, but requiring application of an            cutting the search slope in half.
attentional window to the individual objects, one at a        At least two concerns remain before this basic
time, if the target object is discriminable only by a      finding can be extended and tested in the many
conjunction of features (Treisman & Gelade, 1980).         different variations of visual search displays. First,
This categorical distinction between parallel search of    since a slope of 8 ms per item is clearly in the range of
single feature displays and serial search of conjunction   what has traditionally been considered “parallel
displays has been supported by PET scan evidence for a     search”, it is somewhat unclear whether the result is in
region in the superior parietal cortex that is active      fact a halving of the effective set size or a near
during conjunction search for motion and color, but not    elimination of the effect of set size. Essentially, the
during single feature search for motion or for color       question is whether the first feature extraction is a
(Corbetta, Shulman, Miezin, & Petersen, 1995).             genuine “pop-out” effect and the second is a genuine
   However, several studies have discovered particular     serial search of those “popped out” objects (half of the
conjunctions of features that do not produce steeply       set size), or are both searches “practically parallel”. A
sloped reaction-time functions by set size (e.g.,          replication of the study may provide some insight into
McLeod, Driver & Crisp, 1988; Nakayama &                   this question. Second, the experiments reported by
Silverman, 1986). Additionally, it is possible to observe  Spivey et al. (in press b) ran participants in separate
the phenomenology of 'pop-out' while still obtaining a     blocks of control trials and trials with concurrent
significant (albeit, small) effect of set size on reaction auditory/visual input. It is in principle possible that
time (Bridgeman & Aiken, 1994). Moreover, it has           practice was somehow more effective in the
been argued that steeply sloped reaction-time functions    auditory/visual concurrent condition, or that subjects
may not reflect serial processing of objects in the        developed some unusual strategy in that condition that
display, but rather noise in the human visual system       they didn’t use in the control condition. To be confident
(Eckstein, 1998; Palmer, Verghese, & Pavel, 2000).         in the result, it is necessary to replicate it with a mixed
Overall, a wide range of studies have suggested that the   (instead of blocked) design, where the control trials and
distinction between putatively "serial" and "parallel"     the A/V concurrent trials are randomly interspersed.
search functions is continuous rather than discrete, and
should be considered extremes on a continuum of
search difficulty (Duncan & Humphreys, 1989;                                      Experiment
Nakayama & Joseph, 1998; Olds, Cowan, Jolicoeur,
2000; Wolfe, 1994, 1998).                                  Method
   In a recent study, Spivey, Tyler, Eberhard, and
                                                           Participants Eighteen Cornell undergraduate students
Tanenhaus (in press b) demonstrated that the
                                                           were recruited from various Psychology classes.
incremental processing of linguistic information could,
                                                           Participants were reimbursed 1 point of course extra
essentially, convert a difficult conjunction search into a
                                                           credit for participating in the study.
pair of easier searches. When target identity was
provided via recorded speech presented concurrently
                                                           Procedure The experiment was composed of two types
with the visual display, displays that typically produced
                                                           of trials presented in random mixed order within one
search slopes of 19 ms per item produced search slopes
                                                           continuous block of 192 trials. Participants were
of 8 ms per item. It was argued that if a spoken noun
                                                           instructed to take breaks between trials when they felt it
phrase such as "the red vertical" is processed
                                                           was necessary. In one type of trial, the participant was
incrementally (cf. Altmann, & Kamide, 1999; Eberhard,
                                                           auditorily informed of the target identity before
Spivey-Knowlton, Sedivy, & Tanenhaus, 1995;
                                                           presentation of the visual display (‘Auditory First’
Marslen-Wilson, 1973, 1975), and there is extremely
                                                           control condition). In the other type of trial, the
rapid integration between partial linguistic and visual
                                                           participant was auditorily informed of the two defining
representations, then one might predict that the listener
                                                           feature words of the target concurrently with the onset
should be able to search items with the first-mentioned
                                                           of the visual display (‘A/V Concurrent’ condition) (see
feature before even hearing the second one. If the
                                                           Figure 1) Of the 192 trials, 96 were ‘Auditory First’,
observer can immediately attend to the subset of objects
                                                           and 96 were ‘A/V concurrent.’
sharing that first-mentioned feature, such as the target
color (Egeth, Virzi, & Garbart, 1984; Friedman-Hill &

                                                                                                Legend
                                                                                                  = red
                                                                                                  = green
  Auditory-First                                            A/V Concurrent
  Control Condition                                         Condition
Figure 1. Schematic diagram of the two conditions. In the Auditory-First condition, the search display is presented
after the entire spoken query is heard, whereas in the A/V Concurrent condition, the search display is presented
immediately before the two target features are heard. Reaction time is measured from the point of display onset.
   Trials began with a question delivered in the format       and all had normal color perception. The objects
of a speech file. The same female speaker recorded all        comprising the visual display appeared in a grid-like
speech files with the same preamble recording, “Is there      arrangement positioned centrally in the screen (see
a…” being spliced onto the beginning of each of the           Figure 1). Set sizes of objects comprising the visual
four types of target query types (“…red vertical?”,           displays were 5, 10, 15, and 20.
“…red horizontal?”, “…green vertical?”, and “…green
horizontal?”). Each of the four types of speech files         Results
were edited to be almost identical in length, and with        Mean accuracy was 95% and did not differ across
almost identical auditory spacing of defining feature         conditions. Figure 2 shows the reaction time by set size
words. Participants were instructed to press a ‘yes’ key      functions for target-present trials (filled symbols) and
on a computer keyboard if the queried object was              target-absent trials (open symbols) in the A/V
present in the display, and the ‘no’ key if it was absent.    Concurrent condition and the Auditory-First condition.
It was stressed to participants that they should do this as   The best-fit linear equations are accompanied by their r2
quickly and accurately as possible. An initial fixation       values indicating the percentage of variance accounted
cross preceded the onset of the visual display in order to    for by the linear regression.
direct participants’ gaze to the central region of the           Overall mean reaction time was slower in the A/V
display. Each stimulus bar subtended 2.8 degrees X 0.4        Concurrent condition as a result of the complete
degrees of visual angle, and neighboring bars were            auditory notification of target identity being delayed by
separated from one another by an average of 2 degrees         approximately 1.5 seconds relative to the Auditory-First
of visual angle. Trials with red vertical bars as targets     control condition.      However, since spoken word
and trials with green vertical bars as targets, as well as    recognition is incremental, participants were able to
red and green horizontal bars as targets, were equally        begin processing before both target feature words had
and randomly distributed throughout the session. All          been presented, and overall reaction time was only
participants had normal or corrected-to-normal vision,        delayed by about 600 milliseconds.

                                                                                              identical, results indicated shallower slopes for
                        2200
                                       A/V Concurrent                                         reaction-time functions in the A/V Concurrent
                                             Target Present                                   condition compared to the Auditory-First control
                                             Target Absent
                                                                                              condition (Figure 2).
                        2000                                                                     To specifically test whether the mean slope was
                                    y = 1627.0 + 16.56x
                                                                                              significantly shallower in the A/V Concurrent
                                        R^2 = 0.979                                           condition, participants' individual set size slopes from
                        1800                                                                  the two conditions were compared via paired t-tests,
                                                                                              and revealed significantly shallower slopes for the A/V
                                                                                              Concurrent condition in target-present trials [3.78 vs.
Reaction Time (ms)
                                                                                              15.42 ms per item; t(16)=2.09, P<.05] and in target-
                        1600                                                                  absent trials [16.56 vs. 28.12 ms per item; t(16)=3.39,
                                y = 1595.5 + 3.78x                                            P<.01]. These results indicate that adjusting the timing
                                    R^2 = 0.295                                               of the spoken query (e.g., "Is there a red vertical?"),
                        1400                                                                  such that the two target feature words were presented
                                         Auditory First                                       while the visual display was visible, allowed
                                                Target Present
                                                Target Absent                                 participants to find the target object in a manner that
                                                                                              was substantially less affected by the total number of
                        1200 y = 838.5 + 28.12x
                                      R^2 = 0.956
                                                                                              distractors. In fact, the mean slope of 3.78 ms per item
                                                                                              in the target-present trials of the A/V Concurrent
                                                                                              condition was not significantly greater than zero
                        1000                                          y = 842 + 15.42x        [t(16)=1.12, p>.25], whereas the mean slope of 15.42
                                                                        R^2 = 0.820
                                                                                              ms per item in the target-present trials of the Auditory-
                                                                                              First control condition was robustly greater than zero
                          800                                                                 [t(16)=4.47, p<.001].
                                0           5           10       15         20           25      Hence it appears that, in the Auditory-First condition,
                                                         Setsize                              the search process may employ a conjunction template
                                                                                              to find the target, thus forcing a serial-like process akin
                          Figure 2: Reaction time as a function of set size.
                                                                                              to sequentially comparing each object to the target
                                                                                              template. However, in the A/V concurrent condition, it
                        Repeated-measures analysis of variance revealed
                                                                                              appears that the incremental nature of the speech input
                     significant main effects of Condition [F(1, 16)=230.27,
                                                                                              allows the search process to begin when only a single
                     p<.01], Target Presence/Absence [F(1,16)=27.97,
                                                                                              feature of the target identity has been heard. This initial
                     p<.01], and Set Size [F(3, 48)=22.83, p<.01]. The
                                                                                              single-feature search proceeds in a more parallel
                     effect of Condition was simply that overall reaction
                                                                                              fashion (with the second-mentioned target feature being
                     times were slower when the delivery of target identity
                                                                                              used to find the target amidst the attended subset), thus
                     was delayed in the A/V Concurrent condition. The
                                                                                              dramatically improving the efficiency of search.
                     effect of Target Presence/Absence was the common
                     finding that it takes longer to determine that the target is
                     absent than to determine that it is present. Essentially,
                                                                                                                    Discussion
                     the target-present case involves something akin to a                     The results suggest that, due to the incremental nature
                     self-terminating search, and the target-absent case                      of spoken language comprehension (Allopenna et al.,
                     requires something like an exhaustive search. The main                   1998; Altmann & Kamide, 1999; Cooper, 1974;
                     effect of Set Size simply showed that, when Condition                    Eberhard et al., 1995; Marslen-Wilson, 1973, 1975;
                     and Target Presence/Absence are ignored, having more                     Tanenhaus et al., 1995) the listener/observer can
                     distractors increased reaction time. There was also an                   selectively attend to the subset of objects that exhibit
                     interaction      between     Set    Size     and     Target              the target feature which is mentioned first in the speech
                     Presence/Absence, showing that the effect of Set Size                    stream. Upon hearing even just a portion of the second-
                     was more pronounced in the target-absent trials than in                  mentioned target feature a few hundred milliseconds
                     the target-present trials [F(3,48)=4.36, p<.01].                         later, the observer can then locate the conjunction target
                        The important result for the purposes of our inquiry                  object amidst this attended (spatially noncontiguous)
                     was the significant interaction between Condition and                    subset. These results highlight the incremental
                     Set Size, indicating that the effect of Set Size was more                processing of spoken language comprehension, and
                     pronounced in the Auditory-First control condition than                  demonstrate the human brain's ability to seamlessly
                     in the A/V Concurrent condition [F(3, 48)=4.92,                          cross-index partial linguistic representations (of a noun
                     p<.01]. Despite the fact that the visual displays were                   phrase, for example) with partial visual representations
                                                                                              (of a cluttered visual display).

   A more detailed question remains, concerning            ‘candy’ equally partially active during the first couple
whether the improved efficiency in visual search is due    hundred milliseconds of the word (Allopenna et al.,
to the first-mentioned target feature initiating an        1998). When only the candy was in the display, the eye-
instantaneous parallel search and the second-mentioned     movement data suggested that word recognition was
target feature initiating a serial search among the        faster, involving less competition from partially active
attended items (thus cutting the search slope in half) or  alternatives.
to both spoken target features initiating parallel            Similar findings were reported for syntactic
searches (causing the search slope to look like that of a  ambiguity resolution. When the visual context
single-feature search). In Spivey et al. (in press b,      contained a referential ambiguity (e.g., two apples for
Experiments 1 and 2), the target-present search slopes     the instruction “Put the apple…”) that was best resolved
of 7.7 and 8.9 ms per item in the A/V Concurrent           by pursuing the correct parse of a syntactic ambiguity
conditions were roughly consistent with both of those      (“Put the apple on the towel in the box.”), participants
interpretations. When parallel and serial search were      eye-movement patterns suggested a fast and correct
conceived of as discrete categories, any set-size          interpretation of the instruction. When there was no
function of less than 10 ms per item was generally         such referential ambiguity (e.g., only one apple),
interpreted as “parallel search.” However, Spivey et       participants produced eye-movement patterns indicating
al.’s target-present Auditory First conditions produced    a mis-parse of the instruction (Spivey, Tanenhaus,
search slopes of 19.8 and 18.6 ms per item --              Eberhard, & Sedivy, in press a; Tanenhaus et al., 1995).
approximately twice the A/V Concurrent slopes.                Those effects of visual context immediately
   The present results, with a target-present search slope influencing language comprehension were initially met
of 3.78 ms per item in the A/V Concurrent condition,       with considerable resistance from a substantial portion
appear to support the “two parallel searches”              of psycholinguists. However, when we would discuss
alternative.    However, in all likelihood, the two        those findings with vision researchers, they were often
alternatives outlined above rely too much on the           appreciative but not terribly impressed. To them, it
discrete distinction between “parallel” and “serial”       made perfect sense that the visual system was important
search. If there is indeed a continuum of search           and powerful enough to occasionally tell the language
efficiency (Duncan & Humphreys, 1989; Nakayama &           system what to do very quickly. We are curious to see
Joseph, 1998; Olds, Cowan, Jolicoeur, 2000; Wolfe,         the reaction of the vision research community to the
1994, 1998), and conjunction search is not quite           present results.
accurately described as an object-by-object serial            Returning to our discussion of the notion of
process (Eckstein, 1998; Palmer, Verghese, & Pavel,        “context”, which began this paper, it seems that the
2000), then it might be safest to conclude that each       rapidity with which the visual system and the language
spoken target feature initiates a “relatively efficient”   system can coordinate their representations suggests
search that is not quite parallel and not quite serial.    that any attempt to label some signal as “context” is
Importantly, the second search appears to be able to       doomed to be an arbitrary choice – a choice that risks
work selectively on the output of the first one --         marginalizing important information sources as well as
compelling evidence for the continuous incrementality      opaquely lumping discriminable information sources.
with which linguistic and visual processing can            Essentially, the less we assume encapsulated modular
coordinate.                                                processes in language and in vision, the less use we
   Until now, there has been little or no evidence for     have for the notion of “context” in language and in
real-time visual perception being influenced by            vision. Instead of visual processing and linguistic
linguistic context. However, there is considerable work    processing, perhaps a considerable portion of our daily
reporting demonstrations of real-time language             mental life is made up of visuolinguistic processing.
processing being influenced by visual context (e.g.,          Since the human brain is neither a psycholinguist nor
Allopenna, Magnuson, & Tanenhaus, 1998; McGurk &           a vision researcher (indeed, it is much more than even
MacDonald, 1976; Spivey & Marian, 1998; Tanenhaus          the two of them combined), it is not susceptible to the
et al., 1995). Recent eyetracking research has shown in    biased perspectives they exhibit. As far as the brain is
a number of circumstances that the resolution of           concerned, no one incoming signal is the “primary
temporary ambiguities is immediately biased by             signal” with the others being “context”. Each time slice
information in the visual array. For example, when         of perceptual experience is a rich tapestry of multi-
participants were instructed to “pick up the candy”,       modal environmental inputs, all of which the brain
they often looked first at a candle before then fixating   integrates and incorporates simultaneously. Our results
the candy (Tanenhaus et al., 1995). A precise timing       suggest that, across the domains of language and vision,
analysis of the eye movements suggested that, when the     it is surprisingly good at doing that job immediately and
candle and candy are in the visual display, participants   continuously.
had mental representations for both ‘candle’ and

                 Acknowledgments                           McLeod, P., Driver, J., & Crisp, J. (1988). Visual
                                                             search for conjunctions of movement in visual
We thank Michael Tanenhaus, Kathy Eberhard, and
                                                             search. Nature, 332, 154-155.
Julie Sedivy for helpful discussions, and Quinn            Motter, B. C. & Holsapple, J. W. (2000). Cortical
Hamilton for assistance with data collection. Supported      image density determines the probability of target
by a Sloan Fellowship in Neuroscience (M.J.S.).              discovery during active search. Vision Research, 40,
                                                             1311 1322.
                      References                           Motter, B. C. (1993). Focal attention produces spatially
Allopenna, P. D., Magnuson, J., & Tanenhaus, M. K.           selective processing in visual cortical areas V1, V2,
  (1998). Tracking the time course of spoken word            and V4 in the presence of competing stimuli. Journal
  recognition using eye-movements: evidence for              of Neurophysiology, 70, 909-919.
  continuous mapping models. Journal of Memory and         Nakayama, K. & Silverman, G. H. (1986). Serial and
  Language, 38, 419-439.                                     parallel processing of visual feature conjunctions.
Altmann, G. T. M. & Kamide, Y. (1999). Incremental           Nature, 320, 264 265.
  interpretation at verbs: Restricting the domain of       Nakayama, K. & Joseph, J. (1998). Attention, pattern
  subsequent reference. Cognition, 73, 247-264.              recognition, and pop-out in visual search. In R.
Bridgeman, B. & Aiken, W. (1994). Attentional                Parasuraman (Ed.), The Attentive Brain. Cambridge,
  "popout" and parallel search are separate phenomena.       MA: MIT Press. pp. 279-298.
  Investigative Ophthalmology & Visual Science, 35,        Olds, E. S., Cowan, W. & Jolicoeur, P. (2000). The
  1623.                                                      time-course of pop-out search. Vision Research, 40,
Chomsky, N. (1965). Aspects of the Theory of Syntax.         891-912.
  Cambridge, MA: MIT Press.                                Palmer, J., Verghese, P., & Pavel, M. (2000). The
Corbetta, M., Shulman, G., Miezin, F., & Petersen, S.        psychophysics of visual search. Vision Research, 40,
  (1995). Superior parietal cortex activation during         1227-1268.
  spatial attention shifts and visual feature conjunction. Pylyshyn, Z. (1999). Is vision continuous with
  Science, 270, 802-805.                                     cognition? The case of impenetrability of visual
Duncan, J. & Humphreys, G. (1989). Visual search and         perception. Behavioral and Brain Sciences, 22, 341-
  stimulus similarity. Psychological Review, 96, 433-        423.
  458.                                                     Spivey, M. J. & Marian, V. (1999). Cross talk between
Eberhard, K. M., Spivey-Knowlton, M., Sedivy, J. &           native and second languages: Partial activation of an
  Tanenhaus, M. (1995). Eye movements as a window            irrelevant lexicon. Psychological Science, 10, 281-
  into real-time spoken language comprehension in            284.
  natural contexts.        Journal of Psycholinguistic     Spivey, M. J., Tanenhaus, M. K., Eberhard, K. M., &
  Research, 24, 409.                                         Sedivy, J. C. (in press a). Eye movements and spoken
Eckstein, M. P. (1998). The lower visual search              language comprehension: Effects of visual context on
  efficiency for conjunctions is due to noise and not        syntactic     ambiguity       resolution.    Cognitive
  serial attentional processing. Psychological Science       Psychology.
  9, 111.                                                  Spivey, M. J., Tyler, M. J., Eberhard, K. M., &
Egeth, H. E., Virzi, R., & Garbart, H. (1984).               Tanenhaus, M. K. (in press b). Linguistically
  Searching for conjunctively defined targets. Journal       mediated visual search. Psychological Science.
  of Experimental Psychology: Human Perception and         Tanenhaus, M. K., Spivey-Knowlton, M. J., Eberhard,
  Performance, 10, 32.                                       K. M., & Sedivy, J. C. (1995). Integration of visual
Fodor, J. A. (1983). Modularity of Mind. Cambridge,          and linguistic information during spoken language
  MA: MIT Press.                                             comprehension. Science, 268, 1632-1634.
Friedman-Hill, S. & Wolfe, J. (1995). Second-order         Treisman, A. & Gelade, G. (1980). A feature
  parallel processing: Visual search for the odd item in     integration theory of attention.             Cognitive
  a subset. Journal of Experimental Psychology:              Psychology, 12, 97-136.
  Human Perception and Performance, 21, 531.               Wolfe, J. M. (1994). Guided Search 2.0: A revised
Livingstone, M. & Hubel, D. (1988). Segregation of           mode of visual search. Psychonomic Bulletin &
  form, color, movement, and depth: Anatomy,                 Review, 1, 202-238.
  physiology, and perception. Science, 240, 740-749.       Wolfe, J. M. (1998). What can 1 million trials tell us
Marslen-Wilson, W. (1973). Linguistic structure and          about visual search? Psychological Science, 9, 33-39.
  speech shadowing at very short latencies. Nature,        Zeki, S. (1993). A Vision of the Brain. Oxford:
  244, 522-523.                                              Blackwell Scientific (1993).
Marslen-Wilson, W. (1975). Sentence perception as an
  interactive parallel process. Science, 189, 226-228.
McGurk, & MacDonald (1976). Hearing lips and seeing
  voices. Nature, 264, 746-748.

