UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Clustering Using the Contrast Model
Permalink
https://escholarship.org/uc/item/9cm3x7cg
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 23(23)
Authors
Navarro, Daniel J.
Lee, Michael D..
Publication Date
2001-01-01
Peer reviewed
 eScholarship.org                              Powered by the California Digital Library
                                                                University of California

                                   Clustering Using the Contrast Model
                                             Daniel J. Navarro and Michael D. Lee
                             daniel.navarro,michael.lee @psychology.adelaide.edu.au
                                       Department of Psychology, University of Adelaide
                                              South Australia, 5005, AUSTRALIA
                           Abstract                               ric, featural, alignment-based, and transformational. Of
                                                                  these, the two most widely used approaches are the ge-
   An algorithm is developed for generating featural rep-
   resentations from similarity data using Tversky’s (1977)       ometric, where stimuli are represented in terms of their
   Contrast Model. Unlike previous additive clustering ap-        values on different dimensions, and the featural, where
   proaches, the algorithm fits a representational model that     stimuli are represented in terms of the presence or ab-
   allows for stimulus similarity to be measured in terms of      sence of weighted features. The geometric approach
   both common and distinctive features. The important is-        is most often used in formal models of cognitive pro-
   sue of striking an appropriate balance between data fit and
   representational complexity is addressed through the use       cesses, partly because of the ready availability of tech-
   of the Geometric Complexity Criterion to guide model           niques such as multidimensional scaling (e.g., Kruskal
   selection. The ability of the algorithm to recover known       1964; see Cox & Cox 1994 for an overview), which gen-
   featural representations from noisy data is tested, and it is  erate geometric representations from similarity data. The
   also applied to real data measuring the similarity of kin-
   ship terms.                                                    featural approach to stimulus representation, however, is
                                                                  at least as important as the geometric approach, and war-
                                                                  rants the development of techniques analogous to multi-
                        Introduction                              dimensional scaling.
Understanding human mental representation is necessary               Accordingly, this paper describes an algorithm that
for understanding human perception, cognition, decision           generates featural representations from similarity data.
making, and action. Mental representations play an im-            The optimization processes used in the algorithm are
portant role in mediating adaptive behavior, and form the         standard ones, and could almost certainly be improved.
basis for the cognitive processes of generalization, infer-       In this regard, we draw on Shepard and Arabie’s (1979)
ence and learning. Different assumptions regarding the            distinction between the psychological model that is be-
nature and form of mental representation lead to different        ing fit, and the algorithm that does the fitting. We make
constraints on formal models of these processes. For this         no claims regarding the significance of the algorithm it-
reason, Pinker (1998) argues that “pinning down men-              self (and certainly do not claim it is a model of the way
tal representation is the route to rigor in psychology” (p.       humans learn mental representations), but believe that
85). Certainly, it is important that cognitive models use         the psychological representational model that it fits has
principled mental representations, since the ad hoc defi-         three important properties. First, it allows for the arbi-
nition of stimuli on the basis of intuitive reasonableness        trary definition of features, avoiding the limitations of
is a highly questionable practice (Brooks 1991, Komatsu           partitioning or hierarchical clustering. Second, it uses a
1992, Lee 1998).                                                  more general model of featural stimulus similarity than
   One appealing and widely used approach for deriving            has previously been considered. Third, it generates feat-
stimulus representations is to base them on measures of           ural representations in a way that balances the competing
stimulus similarity. Following Shepard (1987), similarity         demands of data-fit and representational complexity.
may be understood as a measure of the degree to which
the consequences of one stimulus generalize to another,                         Featural Representation
and so it makes adaptive sense to give more similar stim-         Within a featural representation, stimuli are defined by
uli mental representations that are themselves more sim-          the presence or absence of a set of saliency weighted fea-
ilar. For a domain with n stimuli, similarity data take the       tures or properties. Formally, if a stimulus domain con-
form of an n × n similarity matrix, S = si j , where si j         tains n stimuli and m features, a featural representation is
is the similarity of the ith and jth stimuli. The goal of         given by the n × m matrix F = fik , where
similarity-based representation is then to define stimulus
representations that, under a given similarity model, cap-                           1   if stimulus i has feature k
ture the constraints implicit in the similarity matrix by                   fik =                                            (1)
                                                                                     0   otherwise,
approximating the data.
   Goldstone’s (in press) recent review identifies four           together with a vector w = (w1 , . . . , wm ) giving the (pos-
broad model classes for stimulus similarity: geomet-              itive) weights of each of the features.

The Contrast and Ratio Models                                     hyper-parameters α and θ can be incorporated into one
Tversky’s (1977) Contrast Model and Ratio Model of                parameter, ρ = θ (θ α), which represents the relative
stimulus similarity provide a rich range of possibilities         weighting of common and distinctive features, with
for generating featural representations that have been sig-       0 ρ 1. Setting the functional form F ( ) using the
nificantly under-utilized. Using the assumption that the          same ‘sum of saliency weights’ approach as additive
similarity between two stimuli is a function of their com-        clustering yields the similarity model
mon and distinctive features, the Contrast Model mea-
sures stimulus similarity as:
                                                                                                 1−ρ                              
                                                                     ŝi j = ρ ∑ wk fik f jk −              ∑ wk fik      1 − f jk
     ŝi j = θF (fi f j ) − αF (fi − f j ) − βF (f j − fi ) , (2)                  k               2         k
where fi f j denotes the features common to the ith and
 jth stimuli, fi − f j denotes the features present in the                         ∑ wk (1 − fik ) f jk      c.                      (4)
                                                                                     k
ith, but not the jth, stimulus, and F ( ) is some mono-
tonically increasing function. By manipulating the posi-          It is this symmetric version of the Contrast Model that
tive weighting hyper-parameters θ, α and β, different de-         is used in this paper to develop general featural repre-
grees of importance may given to the common and dis-              sentations. It allows for any relative degree of empha-
tinctive components. In particular, Tversky (1977) em-            sis to be placed on common and distinctive features and,
phasizes the two extreme alternatives obtained by setting         in particular, subsumes the additive clustering model
θ = 1, α = β = 0 (common features only), and θ = 0, α =           (ρ = 1) and the distance-based feature-matching similar-
β = 1 (distinctive features only). A different approach           ity model (ρ = 0). Technically, it is worth noting that the
is given by the Ratio Model, where similarity takes the           additive constant c used in additive clustering, which is
form:                                                             added to all pairwise similarity estimates in both addi-
                                                                  tive clustering and Contrast Model clustering representa-
                            θF (fi f j )                          tions, is not treated as a cluster, and thus is not weighted
     ŝi j =                                                . (3)
              θF (fi f j )  αF (fi − f j )   βF (f j − fi )       by ρ.
    While the Contrast Model and the Ratio Model pro-                  Limiting Representational Complexity
vide great flexibility for measuring similarity across fea-       Shepard and Arabie (1979) have noted that the ability to
tural representations, the only established techniques for        specify large numbers of features and set their weights
generating the representations from similarity data are           allows any similarity matrix to be modeled perfectly by
additive clustering algorithms (e.g., Arabie & Carroll            a featural representation using the common features ver-
1980; Lee 1999, in press; Mirkin 1987; Shepard & Ara-             sion of the Contrast Model. The same is true for the ma-
bie 1979; Tenenbaum 1996), which rely exclusively on              jority of Tversky’s (1977) similarity models, and is cer-
the common features version of the Contrast Model. This           tainly true for Eq. (4). While the representational power
means that only one special case of one of these ap-              to model data is desirable, the introduction of uncon-
proaches has been used as the basis of a practical tech-          strained feature structures with free parameters detracts
nique for generating representations.                             from fundamental modeling goals, such as the achieve-
    The paucity of available techniques is serious, given         ment of interpretability, explanatory insight, and the abil-
the recognition (e.g., Goodman 1972; Rips 1989; see               ity to generalize accurately beyond given information
Goldstone 1994 for an overview) that similarity is not            (Lee 2001a).
a unitary phenomenon, and the way in which it is mea-                This means that techniques for generating featural rep-
sured may change according to different cognitive de-             resentations from similarity data must balance the com-
mands. Direct empirical evidence that featural similar-           peting demands of maximizing accuracy and minimizing
ity judgments can place varying emphasis on common                complexity, following the basic principle of model selec-
and distinctive features is provided by the finding that          tion known as ‘Ockham’s Razor’ (Myung & Pitt 1997).
items presented in written form elicit common feature-            Data precision must also be considered, since precise
weighted judgments, whereas pictures tend to be rated             data warrants a representation being made more detailed
more in terms of distinctive features (Gati & Tversky             to improve data-fit, while noisy data does not.
1984; Tversky & Gati 1978).                                          In practice, this means that featural representations
A Symmetric Contrast Model                                        should not be derived solely on the basis of how well
                                                                  they fit the data, as quantified by a measure such as the
Although the Contrast Model has three hyper-                      variance accounted for,
parameters, α and β remain distinct only when
si j = s ji . While it is certainly the case that real world                                   ∑i< j (si j − ŝi j )2
domains display asymmetric similarity, modeling tech-                             VAF = 1 −                           ,              (5)
                                                                                                ∑i< j (si j − s̄)2
niques based on similarity data generally assume that
similarity is symmetric. Further, if the similarity ratings       where s̄ is the arithmetic mean of the similarity data.
are assumed to lie between 0 and 1, the remaining                 Rather, some form of complexity control must be used

to balance data-fit with model complexity. Most es-            More interestingly, Figure 1 shows that the level of
tablished algorithms strike this balance in unsatisfactory     fit for the entirely common features data deteriorates
ways, either pre-determining a fixed number of clusters        more rapidly than for the entirely distinctive features
(e.g., Shepard & Arabie 1979; Tenenbaum 1996), or pre-         data when the wrong ρ value is assumed. Similarly,
determining a fixed level of representational accuracy         for the evenly balanced data, the fit is greater when
(e.g., Lee 1999).                                              too much emphasis is placed on common features in
   Recently, Lee (in press) has applied the Bayesian In-       the assumed similarity model. These results imply that
formation Criterion (BIC: Schwarz 1978) to limit the           common features-weighted models are more able to fit
complexity of additive clustering representations. Un-         data when they are wrong than are distinctive features-
fortunately, an important limitation of the BIC is that it     weighted models. In the language of model complex-
equates model complexity with the number of parame-            ity, the common features functional form is more flexible
ters in the model. While this is often a reasonable ap-        than the distinctive features functional form, and this ex-
proximation, it neglects what Myung and Pitt (1997)            tra complexity improves the fit of incorrect models. For
term the ‘functional form’ component of model com-             this reason, it is important to derive featural representa-
plexity. For featural representations, parametric com-         tions using a measure that is sensitive to functional form
plexity is simply the number of features used in a rep-        complexity.
resentation. Functional form complexity, however, con-
siders the feature structure F, and is sensitive to the pat-   A Geometric Complexity Criterion
terns with which stimuli share features (see Lee 2001a),       Myung, Balasubramanian, and Pitt (2000) have recently
as well as any difference arising from the relative empha-     developed a measure called the Geometric Complexity
sis given to common and distinctive features.                  Criterion (GCC) that constitutes the state-of-the-art in
   It is important to account for functional form complex-     accounting for both fit and complexity in model selec-
ity with featural representational models that can vary        tion. The basic idea is to define complexity in terms
their emphasis on common and distinctive features. Fig-        of the number of distinguishable data distributions that
ure 1 shows the results of fitting featural representations,   the model can accommodate through parametric varia-
assuming different levels of ρ, on similarity data that        tion, with more complicated models being able to index
were generated using either entirely common features           more distributions than simple ones. Using Tenenbaum’s
(ρ = 1), entirely distinctive features (ρ = 0), or an even     (1996) probabilistic formulation of the data-fit of a featu-
balance of the two (ρ = 0.5). These results are averaged       ral model, and extending Lee’s (2001a) derivation of the
across five different similarity matrices, each based on a     Fisher Information matrix for the common features case
five-feature representation, and show one standard error       of the Contrast Model, it is a reasonably straightforward
about the mean level of fit.                                   exercise to derive a GCC for the current similarity model.
                                                               The final result is:
      100%
                                                                              1                          m       1        n(n − 1)
                                                                GCC     =        ∑ (si j − ŝi j )2
                                                                             2s2 i<                          2
                                                                                                                     ln
                                                                                                                            4πs2
                                                                                    j
                                                                               1
      90%                                                                        ln det G,                                           (6)
                                                                               2
VAF
                                                               where s denotes an estimate of the inherent precision of
                                                               the data (see Lee 2001b), m is the number of features, n
                                                               is the number of stimuli, and G denotes the m × m com-
      80%
                                                               plexity matrix for the feature structure. The xy-th cell of
                                                               the complexity matrix is given by,
                                                                                         ∑ ei jx ei jy                               (7)
      70%                                                                                i< j
             0                  0.5                     1
                          Clustering ρ                         where ei jx equals ρ if x is a common feature, − (1 − ρ) 2
                                                               if x is a distinctive feature, and 0 if neither i nor j pos-
                                                               sesses the feature x.
Figure 1: The change in VAF value, as a function of the
                                                                  An interesting aspect of the complexity matrix, and the
assumed balance between common and distinctive fea-            GCC measure as a whole, is that it is independent of the
tures, for the entirely common (dotted line), entirely dis-    parameterization of the model. That is, the complexity of
tinctive (dashed line) and balanced (solid line) similarity    a featural representation is dependent only on the feature
data.                                                          structure, and not the saliencies assigned to the features.
                                                               We should make two technical points about the GCC.
  As expected, the best-fitting featural representations       First, this derivation is based on the assumption that ρ
have ρ values matching those that generated the data.          is a fixed property of a model, and not a free parameter.

An alternative would be to modify the GCC so that it ac-             38
commodated ρ as a model parameter. Second, since the
additive constant is not weighted by ρ, the terms in the             36
complexity matrix corresponding to the additive constant
behave as if ρ = 1.                                                  34
                      Algorithm                                      32
In developing an algorithm to fit featural representa-         GCC
tions using the Contrast Model, we were guided by the                30
successful additive clustering algorithm reported by Lee
(submitted). Basically, the algorithm works by ‘grow-                28
ing’ a featural representation, starting with a one-feature
model, and continually adding features while this leads              26
to improvements in the GCC measure. For any fixed
number of features, the search for an appropriate as-                24
signment of stimuli to features is done using stochastic                  0     0.25         0.5          0.75          1
hill-climbing, with the best-fitting weights being deter-                              Clustering ρ
mined using a standard non-negative least squares algo-
rithm (Lawson & Hanson 1974). The algorithm termi-             Figure 3: The change in GCC value, as a function of the
nates once the process of adding features leads to rep-
                                                               assumed balance between common and distinctive fea-
resentations with GCC values that are more than a pre-
specified constant above the best previously found, and        tures, for the entirely common (dotted line), entirely dis-
the featural representation with the minimum GCC value         tinctive (dashed line) and balanced (solid line) similarity
is returned.                                                   data.
                                                                  The algorithm was applied to this similarity data un-
                                                               der different assumptions regarding the balance between
                                                               common and distinctive features, using ρ values of 0,
                                                               0.25, 0.5, 0.75 and 1. In calculating the GCC measure, a
                                                               data precision value of σ = 0.05 was assumed, in accor-
                                                               dance with the known level of noise. Figure 3 summa-
                                                               rizes the results of 10 runs of the algorithm for each of
                                                               the three similarity conditions, across all of the assumed
                                                               ρ values. The mean GCC value of the 10 derived rep-
                                                               resentations is shown, together with error bars showing
                                                               one standard error in both directions.
                                                                  Figure 3 shows that the GCC is minimized at the cor-
                                                               rect ρ value for all three similarity conditions. An exami-
                                                               nation of the derived representation revealed that the cor-
                                                               rect featural representation was recovered 25 times out
Figure 2: The artificial featural representation containing    of 30 attempts: nine times out of ten for the entirely dis-
seven stimuli and four features.                               tinctive data, and eight times out of ten for the evenly
                                                               balanced and the entirely common data. It is interesting
   To test the ability of this optimization algorithm to       to note that Figure 3 is far more symmetric than Figure 1,
fit similarity data, we examined its ability to recover a      suggesting that the GCC has successfully accounted for
known featural representation. This representation had         the differences in functional form complexity between
seven stimuli and four features, and included partition-       the common and distinctive feature approaches to mea-
ing, nested, and overlapping clusters, as shown in Figure      suring similarity.
2. Using this representation, similarity data were gen-
erated assuming entirely common features, entirely dis-           Additional Monte Carlo simulations with other feat-
tinctive features, or an even balance between the two.         ural representations, based on particular structures re-
Feature weights were chosen at random subject to the           ported by Tenenbaum (1996, Table 1) and Lee (1999,
constraint that they resulted in positive similarity values.   Table 5), also suggested that the algorithm is capable of
Each of the similarity values was perturbed by adding          recovering known configurations when more stimuli or
noise that was independently drawn from a Normal dis-          more features are involved, although problems with lo-
tribution with mean 0 and standard deviation 0.05.             cal minima are encountered more frequently.

                        Table 1: Representation of Rosenberg and Kim’s (1975) kinship terms domain.
                                               S TIMULI IN C LUSTER                               W EIGHT
                         aunt uncle niece nephew cousin                                            0.319
                         granddaughter grandson grandmother grandfather                            0.291
                         mother daughter grandmother granddaughter aunt niece sister               0.222
                         sister brother cousin                                                     0.221
                         father son grandfather grandson uncle nephew brother                      0.208
                         mother father daughter son sister brother                                 0.163
                         mother father daughter son                                                0.136
                         daughter son granddaughter grandson niece nephew sister brother           0.128
                         mother father grandmother grandfather aunt uncle sister brother           0.091
                         additive constant                                                         0.563
                                           VARIANCE ACCOUNTED F OR                                 92.7%
                 An Illustrative Example                              In terms of future work, it should be acknowledged
To demonstrate the practical application of the algorithm,         that the symmetric version of the Contrast Model is cer-
we used the averaged similarity data reported by Rosen-            tainly not the only possibility for combining common
berg and Kim (1975), which measures similarity of En-              and distinctive features approaches to measuring similar-
glish kinship terms. A data precision estimate of s = 0.09         ity. Tenenbaum and Griffiths (in press) provide a com-
was made based on the sample standard deviation of the             pelling argument for the use of the Ratio Model in the
individual matrices. Since the data was obtained by hav-           context of their Bayesian theory of generalization. It
ing participants sort items into different stacks, we might        would also be worthwhile to examine featural represen-
expect a model that provides a weighting of common and             tations where each feature is assumed to operate using
distinctive features to provide a better fit than one allow-       entirely an distinctive or an entirely common approach.
ing only for common features. Using ρ values of 0, 0.1,            The distinctive similarity features would be those that
0.2, . . . , 1.0, the representation with the minimum GCC          globally partition the entire stimulus set, as for the fea-
was found at ρ = 0.4.                                              ture ‘male’, which implies the existence of the comple-
   This representation contained the nine features de-             mentary feature ‘female’. The (more prevalent) common
tailed in Table 1, and explained 92.7% of the variance             similarity features would be those that captured shared
in the data. Interpreting most of the features in Ta-              properties, such as eye or hair color, where no broader
ble 1 is straightforward, since they essentially capture           implications are warranted.
concepts such as ‘male’, ‘female’, ‘nuclear family’, ‘ex-
tended family’, ‘grandparents’, ‘descendants’, and ‘pro-                              Acknowledgments
genitors’. While this representation is very similar to the        This article was supported by a Defence Science and
nine-feature representation generated by additive cluster-         Technology Organisation scholarship awarded to the first
ing (Lee submitted, Figure 2), it explains more of the             author. We wish to thank several referees for helpful
variance in the data, suggesting that participants did in-         comments on an earlier version of this paper.
deed use both common and distinctive features in assess-
ing similarity.
                                                                                           References
                          Conclusion                               Arabie, P., & Carroll, J.D. (1980). MAPCLUS: A mathe-
We have developed, tested, and demonstrated an al-                    matical programming approach to fitting the ADCLUS
gorithm that generates featural stimulus representations              model. Psychometrika 45(2), 211–235.
from similarity data. Unlike previous additive clustering          Brooks, R.A. (1991). Intelligence without representa-
approaches, the algorithm uses a symmetric version of                 tion. Artificial Intelligence 47, 139–159.
Tversky’s (1977) Contrast Model that measures similar-
ity in terms of both common and distinctive features. A            Cox, T.F., & Cox, M.A.A. (1994). Multidimensional
particular strength of the algorithm is its use of the Geo-           scaling. London: Chapman and Hall.
metric Complexity Criterion to guide the generation pro-
                                                                   Gati, I., & Tversky, A. (1984). Weighting common and
cess, which allows the desire for data-fit to be balanced
                                                                      distinctive features in perceptual and conceptual judg-
with the need to control representational complexity. Im-
                                                                      ments. Cognitive Psychology 16, 341–370.
portantly, this criterion is sensitive to the functional form
complexity of the similarity model, preventing an over-            Goldstone, R.L. (1994). The role of similarity in cat-
emphasis on the inherently more complicated common                    egorization: Providing a groundwork. Cognition 52,
features approach.                                                    125–157.

Goldstone, R.L. (in press). Similarity. In R.A. Wilson     Schwarz, G. (1978). Estimating the dimension of a
  & F.C. Keil (Eds.) MIT Encyclopedia of the Cognitive       model. Annals of Statistics, 6, 461–464.
  Sciences. Cambridge, MA: MIT Press.                      Shepard, R.N. (1987). Toward a universal law of gen-
Goodman, N. (1972). Seven strictures on similarity. In       eralization for psychological science. Science, 237,
  N. Goodman (Ed.), Problems and Projects, pp. 437–          1317–1323.
  446. New York: Bobbs-Merrill.                            Shepard, R.N., & Arabie, P. (1979). Additive cluster-
Lawson, C.L., & Hanson, R.J. (1974).Solving Least            ing representations of similarities as combinations of
  Squares Problems. Englewood Cliffs, NJ: Prentice-          discrete overlapping properties. Psychological Re-
  Hall.                                                      view 86(2), 87–123.
Lee, M.D. (1998). Neural feature abstraction from judg-    Tenenbaum, J.B. (1996). Learning the structure of simi-
  ments of similarity. Neural Computation, 10 (7),           larity. In D.S. Touretzky, M.C. Mozer, & M.E. Has-
  1815-1830.                                                 selmo (Eds.), Advances in Neural Information Pro-
                                                             cessing Systems, Volume 8, pp. 3–9. Cambridge, MA:
Lee, M.D. (1999). An extraction and regularization ap-
                                                             MIT Press.
  proach to additive clustering. Journal of Classifica-
  tion, 16 (2), 255-281.                                   Tenenbaum, J.B., & Griffiths, T.L. (in press). General-
                                                             ization, similarity, and Bayesian inference. Behavioral
Lee, M.D. (2001a). On the complexity of additive clus-
                                                             and Brain Sciences 24(2).
  tering models. Journal of Mathematical Psychology,
  45 (1), 131-148.                                         Tversky, A. (1977). Features of similarity. Psychological
                                                             Review 84(4), 327–352.
Lee, M.D. (2001b). Determining the dimensionality of
  multidimensional scaling models for cognitive mod-       Tversky, A., & Gati, I. (1978). Studies of similarity. In
  eling. Journal of Mathematical Psychology, 45 (1),         E. Rosch and B.B. Lloyd (Eds.), Cognition and Cate-
  149-166.                                                   gorization, pp. 79–98. Hillsdale, NJ: Wiley.
Lee, M.D. (in press). A simple method for generating ad-
  ditive clustering models with limited complexity. Ma-
  chine Learning.
Lee, M.D. (submitted). Generating additive clustering
  models with limited stochastic complexity. Manuscript
  submitted for publication.
Komatsu, L.K. (1992). Recent views of conceptual struc-
  ture. Psychological Bulletin 112(3), 500–526.
Kruskal, J.B. (1964). Multidimensional scaling by opti-
  mizing goodness of fit to a nonmetric hypothesis. Psy-
  chometrika, 29 (1), 1–27.
Mirkin, B.G. (1987). Additive clustering and qualitative
  factor analysis methods for similarity matrices. Jour-
  nal of Classification 4, 7–31.
Myung, I.J., Balasubramanian, V., & Pitt, M.A. (2000).
  Counting probability distributions: Differential geom-
  etry and model selection. Proceedings of the National
  Academy of Sciences USA 97, 11170–11175.
Myung, I.J., & Pitt, M.A. (1997). Applying Occam’s ra-
  zor in modeling cognition: A Bayesian approach. Psy-
  chonomic Bulletin and Review 4(1), 79–95.
Pinker, S. (1998). How the mind works. Great Britain:
  The Softback Preview.
Rips, L.J. (1989). Similarity, typicality, and categoriza-
  tion. In S. Vosniadou & A. Ortony (Eds.), Similarity
  and Analogical Reasoning, pp. 21–59. New York:
  Cambridge University Press.
Rosenberg, S., & Kim, M.P. (1975). The method of sort-
  ing as a data-gathering procedure in multivariate re-
  search. Multivariate Behavioral Research 10, 489–
  502.

