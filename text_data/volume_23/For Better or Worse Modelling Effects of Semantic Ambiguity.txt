UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
For Better or Worse: Modelling Effects of Semantic Ambiguity
Permalink
https://escholarship.org/uc/item/0b76j93g
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 23(23)
Authors
Rodd, Jennifer
Gaskell, Gareth
Marslen-Wilson, William
Publication Date
2001-01-01
Peer reviewed
 eScholarship.org                              Powered by the California Digital Library
                                                                University of California

                  For Better or Worse: Modelling Effects of Semantic Ambiguity
                                            Jennifer Rodd (jrodd@csl.psychol.cam.ac.uk)
                                Centre for Speech and Language, Department of Experimental Psychology
                                                              Cambridge University
                                                                 Cambridge, UK
                                            Gareth Gaskell (g.gaskell@psych.york.ac.uk)
                                                           Department of Psychology
                                                         University of York, York, UK
                           William Marslen-Wilson (william.marslen-wilson@mrc-cbu.cam.ac.uk)
                                                   MRC Cognition and Brain Sciences Unit
                                                      15 Chaucer Road, Cambridge, UK
                                Abstract                                  ample, although there are important differences between what
                                                                          it means to twist an ankle compared with to twist the truth,
   Several studies have reported an advantage in lexical deci-            these different senses of the word twist are closely related to
   sion for words with multiple meanings. More recently, Rodd,            each other, both etymologically and semantically. This rela-
   Gaskell, and Marslen-Wilson (in press) have reported a more
   complex pattern of ambiguity effects. While there is a process-        tionship is quite unlike the ambiguity for a word like bark.
   ing advantage for words that have many highly related word                All standard dictionaries respect this distinction between
   senses (e.g., twist), there is a disadvantage for words that have      word meanings and word senses; lexicographers routinely de-
   more than one meaning (e.g., bark). Here we show that these            cide whether different usages of the same spelling should cor-
   two apparently opposite effects of ambiguity can both emerge
   from the competition to activate a coherent semantic represen-         respond to different lexical entries or different senses within a
   tation in an attractor network. Ambiguity between unrelated            single entry. However, although this distinction appears easy
   meanings delays recognition because of interference between            to formulate, people will sometimes disagree about whether
   the two possible stable patterns of semantic activation, that cor-     two usages of a word are sufficiently related that they should
   respond to separate attractor basins. In contrast, the patterns of     be taken as senses of a single meaning rather than different
   semantic activation that correspond to different senses of the
   same word meaning all lie within a single attractor basin, and         meanings. However, even if there is not always clear distinc-
   the semantic flexibility associated with these words results in        tion between these two different types of ambiguity, it is im-
   a widening of the attractor basin, thus produces a processing          portant to remember that words that are described as ambigu-
   advantage relative to unambiguous words.                               ous can vary on a continuum between these two extremes.
                                                                             Rodd et al. (in press) support the psychological impor-
The Ambiguity Disadvantage and Sense Benefit                              tance of this distinction in a set of lexical decision experi-
                                                                          ments which show that while multiple related word senses
Models of word recognition often make the simplifying as-                 do produce a processing advantage, multiple unrelated mean-
sumption that each word in the language has a single, well-               ings delay recognition. Here we report a series of simulations
defined meaning. However, many words refer to more than                   which investigate whether these two apparently opposite ef-
one concept. For example, bark can refer either to a part of              fects of ambiguity can both emerge from the competition to
a tree, or to the sound made by a dog. Other words, such as               produce a coherent distributed semantic representation within
twist, have a range of systematically related dictionary defi-            an attractor network.
nitions including to make into a coil or spiral, to operate by
turning, to alter the shape of, to misconstrue the meaning of,            Semantic Competition Models of the Ambiguity
to wrench or sprain, and to squirm or writhe. To understand
such words, we select the appropriate interpretation, normally                                      Advantage
on the basis of the context in which the word occurs. In this             Joordens and Besner (1994) and Borowsky and Masson
paper we review the literature on how semantic ambiguity af-              (1996) have tried to model effects of ambiguity using a two-
fects the recognition of single words, and report a series of             layer Hopfield network (Hopfield, 1982) to learn the mapping
network simulations that examine the implications of these                between orthography and semantics. The models show an
results for models of word recognition                                    advantage for words that are ambiguous between unrelated
   Several studies in the literature report faster lexical deci-          meanings. The authors argue that this advantage arises be-
sion times for ambiguous words, compared with unambigu-                   cause, when the orthography of a word is presented to the
ous words (Azuma & Van Orden, 1997; Borowsky & Mas-                       network, the initial state of the semantic units is randomly de-
son, 1996; Millis & Button, 1989). There have been various                termined. The network must move from this state to a valid
explanations for why it might be easier to recognise words                finishing state corresponding to the meaning of the word. For
with multiple meanings. Typically it is assumed that ambigu-              ambiguous words there are multiple valid finishing state, and
ous words benefit from having more than one competitor in                 on average, the initial state of the network will be closer to
the race for recognition. More recently, this view that there             one of these states than for an unambiguous word, where
is a simple advantage for semantic ambiguity has been chal-               there is only one valid finishing state. However, as discussed
lenged. Rodd et al. (in press) argue that a distinction should            above, it is now apparent that ambiguity between unrelated
be made between the accidental ambiguity of words like bark               meanings produces an disadvantage, so that there is a discrep-
which, by chance, have two unrelated meanings, and the sys-               ancy between the data and the behaviour of these models.
tematic ambiguity of words that have multiple senses. For ex-                One limitation of these models is that their performance on

the task was surprisingly poor. Joordens and Besner (1994)            Method
report an error rate of 74%. These errors often result from           Network Architecture The network has 300 units: 100
the network settling into blend states, which are a mixture           (orthographic) input units and 200 (semantic) output units.
of the word’s meanings. Gaskell and Marslen-Wilson (1999)             The network is fully connected; each unit is connected to all
have shown that blends between unrelated semantic represen-           other units. All units are bipolar; they are either on [+1] or
tations can be relatively meaningless, and may be closer to a         off [−1].
different word in the lexicon than to either of the components
of the blend. In the Borowsky and Masson (1996) study, these          Learning Algorithm All connection strengths were ini-
blend states are not considered to be errors; the authors argue       tially set to 0. During each learning trial, the network
that to perform lexical decision it is not necessary to resolve       was presented with a single training pattern, and an error-
the ambiguity successfully in order for there to be sufficient        correcting learning algorithm was used to change the con-
familiarity to make a successful lexical decision. Although           nection strengths. The change in connection strength from a
this approach may be appropriate for modelling the specific           given unit i to a unit j is given by
task of lexical decision, this would severely limit the model in
being extended to be a more general model of word recogni-                         ∆wi j = xi (x j − ∑ wk j xk )/3n, i = j,      (1)
tion. It is the case that, given an ambiguous word in isolation,                                     k
we are able to retrieve one of its meanings. In contrast, the
model would predict that without a contextual bias to direct          where xi is the activation of unit i and n is the number of
us to one of the meanings we would get stuck in a blend state         units in the network. The learning rate parameter, 1/3, was
that may be quite unlike either of the meanings.                      selected to provide good performance after a relatively small
                                                                      amount of training.
   It is possible that the observed ambiguity advantage may
be an artefact of this tendency to settle into blend states. In-      Training Unambiguous word representations were created
deed, Joordens and Besner (1994) report that as the size of           by randomly assigning values of +1 and −1 to each of the
their network is increased, and performance improves, the             100 input and 200 output units, such that half the units in
ambiguity advantage is eliminated. However, even in these             each part of the network were assigned +1, and the other half
larger networks, the problem of blend states is still present;        −1. For the ambiguous words, a single, randomly generated
Joordens and Besner (1994), report a maximum performance              input pattern was paired with two different output patterns.
level of 48.8% for ambiguous words. In the following simu-               In the Joordens and Besner (1994) simulations, the net-
lation, we attempt to improve the overall performance of the          work was trained on only one unambiguous word and one
network, and investigate how ambiguity affects performance            ambiguous word. Here the training set varies between 1 and
in a network that is able to successfully retrieve the meanings       16 pairs of ambiguous and unambiguous words, (i.e., 2 to 32
of ambiguous words.                                                   words or 3 to 48 unique semantic patterns). The number of
                                                                      times that each word was presented to the network was varied
   Simulation 1: The Ambiguity Disadvantage                           between 2 and 64 times. The unambiguous and ambiguous
                                                                      words were matched for overall frequency of the orthographic
Introduction                                                          pattern. For each combination of training set size and length
                                                                      of training, the network was trained, and its performance was
While Hopfield networks are known to have limited capacity,           tested on 200 independent passes; for each pass, a different,
the networks discussed above are performing well below the            independently generated set of training items was used.
theoretical capacity limit. Hopfield (1982, pg 2556) stated
that “About 0.15 N states can be simultaneously remembered            Testing Each input pattern was presented to the network,
before error in recall is severe”, where N is the number of           and the output units were all set randomly to [+1] or [−1].
units in the network. Therefore the Joordens and Besner               Retrieval of the semantic patterns was the result of an asyn-
(1994) network should be able to learn 45 patterns, and yet           chronous updating procedure. A unit was selected at random,
the network cannot reliably learn 4 words. This poor perfor-          and its activation was updated by summing the weighted in-
mance is because the patterns corresponding to the different          put to that unit. If this input was greater than zero, then the
meanings of ambiguous words share the orthographic part               unit was set to +1, otherwise the unit was set to −1. This up-
of their pattern. Hopfield (1982) noted that these networks           dating continued until a sequence of 1500 updates produced
have a particular difficulty with correlated patterns. There-         no change in the state of any unit. The network was consid-
fore, the simple Hebbian learning rule, which captures the            ered to have settled correctly only if the activation of all its
correlational structure of the training set, may not be suitable      units was correct when it reached a stable state.
for learning ambiguous words.
   Simulation 1 uses instead the least mean-square error-
                                                                      Results
correcting learning algorithm, which adjusts the weights be-          For the unambiguous words, the network settled into the cor-
tween units to reduce any error in the activation patterns pro-       rect semantic pattern for over 99.8% of the words, for all the
duced by the current sets of weights. This may therefore al-          levels of training and set sizes. For the ambiguous words,
leviate the problem of blend states, as the learning algorithm        performance was more variable. The percentage of trials on
will change the weights such that these states are not stable.1       which the network settled into a correct training pattern for
                                                                      these words is shown in Figure 1 for different amounts of
    1 Kawamoto, Farrar, and Kello (1994) used this algorithm to learn training, and for different sizes of the training set. Impor-
ambiguous words, but they do not report error rates.                  tantly, under some conditions, the network was able to settle

correctly into the semantic pattern corresponding to one of       operating more effectively; alternatively, the number of spu-
the word’s two meanings on 98% of the trials. Therefore, the      rious stable attractors may increase for small training sets be-
LMS error-correcting algorithm performed substantially bet-       cause of the small number of learned attractor basins.
ter than the Hopfield algorithm on this task.                        There is an interesting effect of training on performance:
                                                                  initially, training improves performance, in terms of both er-
                     100                                          ror rates, and settling times. However, for some training-set
                      90                                          sizes, performance reduces if the training set is presented
                                                                  more than 16 times. This suggests that over-learning of the
   Percent Correct
                      80
                      70
                                                      16epochs    training set produces poor performance for the test items (in
                      60                              8 epochs    which only a subset of the training features are activated).
                      50                              4 epochs
                      40                              2 epochs
                                                      1 epoch     Discussion
                      30
                      20                                          This simulation shows that the introduction of an error-
                      10                                          correcting learning algorithm improved performance on the
                       0                                          ambiguous words to a level where it is reasonable to inves-
                           2   4      8    16    32
                                                                  tigate the effects of ambiguity on performance. In all condi-
                               Number of Words                    tions where performance exceeded 90%, there was a signif-
                                                                  icant disadvantage for the ambiguous words in terms of the
Figure 1: Simulation 1, Performance for Ambiguous Words           number of cycles taken for the network to settle.
                                                                     Therefore, this simulation suggests that the ambiguity ad-
   Despite this improvement, the ambiguous words were still       vantage found by Joordens and Besner (1994) is atypical for
difficult to learn, compared with the unambiguous words. For      a network of this type. When performance is improved such
the unambiguous words, the network always reached near-           that the network reliably settles into a stable semantic rep-
perfect performance, having been presented with the training      resentation that corresponds to one of the word’s meanings,
set only once. For the ambiguous words, only when the train-      the interference between the multiple patterns of ambigu-
ing set had been presented to the network four times, did per-    ous words delays their recognition, relative to unambiguous
formance ever rise above 90%. The number of cycles taken          words. Therefore, a simple semantic competition network of
by the network to settle was also generally greater for the       this type can simulate the ambiguity disadvantage seen by
ambiguous words than for the unambiguous words. Table 1           Rodd et al. (in press). The question that remains is whether
shows the difference between the settling times for the two       this type of network can also produce the benefit for words
types of words; positive numbers indicate faster settling for     with multiple, related word senses.
the unambiguous words. For the smallest training-set size,
the difference between the two types of words was small and         Simulation 2: Word Senses as Random Noise
variable, but for larger training sets, a consistent ambiguity
disadvantage emerges. Crucially, for all the networks where       Introduction
performance on the ambiguous words was greater than 90%,          We have now shown that semantic competition between word
there was a significant ambiguity disadvantage (all significant   meanings delays the settling of the network for ambiguous
using the Bonferroni correction for multiple comparisons).        words, relative to unambiguous words. How then are we to
                                                                  explain the advantage reported by Rodd et al. (in press) for
                                                                  words with multiple senses? One difference between these
Table 1: Simulation 1, Percentage Benefit in Settling Times       two forms of ambiguity is the degree of semantic overlap be-
for Unambiguous Words                                             tween the alternative semantic patterns. However, although
                   2       4       8       16       32            an increase in the similarity of the two meanings of an am-
    Training Words Words Words Words Words                        biguous words may reduce the level of semantic competition
           2      -1       4       38       87        -           (and therefore the ambiguity disadvantage), this can only im-
           4       0       6       23       65      105           prove performance to the level of the unambiguous words; it
           8      -1       3      16*      41*       94
          16       2       5      11*      25*       64           cannot produce a benefit.2
          32      -2       3      12*      32*      57               In this simulation, we explore the hypothesis that the vari-
          64      -1       4      11*      32*      49            ation in the meanings of words such as twist and flash, which
    Notes. * performance on ambiguous words exceeded 90%,         are listed as having many word senses, should be viewed
                                                                  not in terms of ambiguity, but in terms of flexibility. We
   The change in the performance as a function of the size        assume that the multiple senses of these words are not dis-
of the training set was somewhat surprising. At all levels of     tinct, but that their meaning is flexible or vague, such that it
training, the network settled more quickly when it had been       has a slightly different interpretation in different contexts. In
trained on fewer patterns. However, the effect of training-set    particular, we assume that these words can be represented as
size on error rates for the ambiguous words is more complex       having a single base pattern that represents the core meaning
(see Figure 1). It is not altogether clear why the network per-   of the word. Then, every time this pattern is presented to the
forms so poorly for a very small training sets. It is possible        2 This has been confirmed in a set of simulations identical to Sim-
that the increased error produced by the other words in the       ulation 1 except that the semantic relationship between the meanings
training set results in the error-correcting learning algorithm   of the words was systematically varied (Rodd, 2000).

network, random noise is added to this base pattern, such that     for the noisy patterns. These data show complex interactions
each time the network sees the word, it is slightly different      between the effects of noise and training, but crucially, while
from other instances of the word.                                  low levels of noise have no stable influence on performance,
   Although this idea that words with many senses should be        as the level of noise increases, a reliable disadvantage for
characterized as words whose meanings are flexible about a         noise emerges. This disadvantage for noise is greatest at low
core meaning does not reflect how these words are listed in        levels of training, and increases with the level of noise.
dictionaries, there is support for this idea that the classifica-
tion of the meanings of such words into distinct senses is arti-
ficial. For example, Sowa (1993) states that “for polysemous       Table 2: Simulation 2, Cycles to Settle for Unambiguous and
words, different dictionaries usually list different numbers of    Noisy Words
meanings, with each meaning blurring into the next”.
   The reason that we might expect this characterization of                Units                          Training Presentations
word senses to produce the processing benefit seen in the hu-           Changed                          16     32     64     128
man data is that, as we saw in Simulation 1, if an identical                   1    Unambiguous         603    617    615     588
pattern is repeatedly presented to the network, it can develop                 1    Noisy               611    622    614     593
a very deep attractor basin that can be difficult for the network              1    Difference          +8      +5     -1     +5
to settle into when it is given only the orthographic input. It                3    Unambiguous         587    598    564     515
is possible that adding a small amount of noise to the net-                    3    Noisy               617    614    583     539
work might prevent this over-learning, and might allow the                     3    Difference          +30    +16    +19     +24
network to develop broader attractor basins, that are easier                   5    Unambiguous         578    573    536     481
for the network to enter.                                                      5    Noisy               623    609    568     509
                                                                               5    Difference          +45    +36    +32     +28
Method
Network Architecture, Learning Algorithm and Process-
ing The architecture and learning algorithm used in this           Discussion
simulation were identical to those used in Simulation 1. How-      Contrary to the idea that noise during training might improve
ever, to reduce the length and variability of the settling times,  performance, the network was slower to settle into those
a different updating procedure was used. Updating now con-         training patterns that had noise added to them during train-
sisted of a series of update sequences in which all the seman-     ing, compared with the unambiguous patterns. Therefore, this
tic units were updated once in random order.                       simulation suggests that even if we characterize the ambiguity
Training The networks were each trained on 64 words.               between multiple senses as being noise about a base pattern,
Half these words were unambiguous, and were presented to           the ambiguity still produces a processing disadvantage. This
the network in exactly the same form on each presentation.         is, of course, the reverse of the pattern seen in the human data.
The other words had noise added to them; each time these              In this simulation, the noise that was added to the semantic
words were presented to the network, a small number of the         representations was random; on each training trial, the acti-
semantic units were randomly changed from the original base        vation of a given number of units in the semantic pattern was
pattern. The number of units that were changed varied from         changed. However, this is not a realistic characterization of
1 to 5 across different simulations. The number of times that      how the senses of words differ; it is not the case that a new
these words were presented to the network was varied from          sense of a word can be created from the core meaning of the
16 to 128. For each level of training and noise, 100 networks      word by simply changing arbitrary features. Rather, it is that
were trained on independently generated sets of patterns.          case that these words have sets of possible semantic features
                                                                   which are sometimes, but not always, present. Therefore,
Results                                                            rather than modelling word senses as the addition of random
                                                                   noise, it might be better to assume that each word has a range
For the unambiguous words, the network settled correctly in        of possible semantic features, but that not all these features
over 99.5% of trials, in all conditions. For the words that        are always turned on. For example, the word twist may in
had noise added to them, it is less clear what it means for the    some contexts not activate the features relating to pain, but it
network to settle correctly; as the level of noise increased, the  will never arbitrarily gain a feature such as has legs.
percentage of trials on which the network settled into the base       To model word senses in this way, we need to move away
pattern decreased. However, those trials on which the net-         from semantic representations in which half the units are set
work did not settle into the base pattern should not all be con-   to +1 and the other half set to −1. Instead, for any given
sidered as errors. If the network settles into a pattern that does semantic representation, most of the units will be turned off,
not differ from the base pattern by more than the amount of        and only a subset will be turned on. Noise is added such that
noise that was added to the patterns during training, this can     only those features that should be turned on may be turned
be thought of as the network settling into one of the word’s       off, but there is no arbitrary addition of semantic features.
senses rather than the core meaning, and should not be con-
sidered to be an error. Using this approach, the percentage                Simulation 3: Sparse Representations
correct for these words was always above 99.5%
   Table 2 shows settling times for the unambiguous words          Method
and the words with the added noise, and the differences be-        Network Architecture, Learning Algorithm and Process-
tween these scores. Positive numbers reflect a disadvantage        ing The architecture used in this simulation was identical to

that used in Simulations 1 and 2. The training patterns used                                                               25
were sparse, such that only 10% of the units were set to +1
and the remainder were set to 0. The change in connection                                                                  20
                                                                                                   Semantic Units Active
strength from a given unit i to a unit j is given by
                                                                                                                           15
                           ∆wi j = 5xi (x j − ∑ wk j xk )/n,             i = j          (2)                                                                   Unambiguous
                                                                                                                                                               Noisy
                                                    k                                                                      10
Training As in Simulation 2, the network was trained on                                                                     5
64 words; half of the words had noise added to the seman-
tic representations during training, and the other half did not.                                                            0
Again, the number of units that were changed for the noisy                                                                      1   2    3       4     5   6
words varied from 1 to 5; however noise was added only to                                                                               Update Cycle
units that were set to +1 in the base pattern.
                                                                                                       Figure 3: Simulation 3, Activation of Semantic Units
Results
Figure 2 shows the performance of the network at different
levels of training and noise.3 At all levels of noise, perfor-                                    Interestingly, at the end of the first update cycle, the net-
mance is better for the words that had noise added during                                      work is significantly more active for the noisy words (p <
training; the network is able to correctly produce the seman-                                  .001). For the unambiguous words, on average 12 units are
tic representations for these words at lower levels of training.                               switched on; for the noisy words, 16 are activated. Therefore,
                                                                                               if we assume that lexical decisions are made before the ac-
                100                                                                            tivation of the semantic units has become completely stable,
                 90                                                                            there will be an advantage for the noisy words. It is worth not-
                 80                                                                            ing that, if this network is presented with a novel word that
                 70
                                                                                               was not in the training set, the activation of the semantic units
    % Correct
                 60                                                            Noisy
                 50                                                            Unambiguous     very rarely rises above 10. If an activation threshold were set
                 40                                                                            at this level, there would be an advantage for the noisy words.
                 30
                 20
                                                                                                  The later advantage for unambiguous words reflects an as-
                 10                                                                            sumption built into the training set that the total number of
                  0                                                                            semantic features that are ever activated for words with many
  Training:           16   32   64 128   16   32   64 128   16   32   64 128
                                                                                               senses is equivalent to the total number of features for the
   Noise:             1 Unit Changed     3 Units Changed    5 Units Changed
                                                                                               unambiguous words. In other words, we have assumed that
                                                                                               the individual senses of words with many senses have fewer
                                                                                               semantic features than those with only a single sense. This
                            Figure 2: Simulation 3, Error rates                                assumption is probably incorrect; it is more likely that words
                                                                                               with many senses have a larger set of possible semantic fea-
   We then looked in detail at the settling behaviour of the net-                              tures than words with few senses. It may have been more re-
work, which was presented with each word 128 times, with a                                     alistic to assume that the groups of words should be equated
level of noise of 5 units. This network successfully retrieved                                 on the average number of features that are activated for each
the meaning in over 99.7% of trials for both types of words.                                   individual sense. If this had been the case then the two types
This network settled significantly more quickly for the un-                                    of words would settle to the same mean activation level, and
ambiguous words than for the noisy words; the unambiguous                                      the noise advantage would be larger, and extend later in the
words took on average 407 updates before they were stable,                                     settling of the network.
the noisy words took 435 (t(99) = 8.9, p < .001). However,
a more interesting picture emerges if we look at how the ac-                                   Discussion
tivation of the semantic representations built up over time for                                This simulation shows that if the activation of the semantic
this network. Figure 3 shows the total number of semantic                                      features is used as a metric of lexical decision, then there is
units that are switched on at the end of each update of the 200                                an advantage for words to which noise is added during train-
semantic units. If the network activates 20 units, this corre-                                 ing. The advantage is only present early in the settling of the
sponds to the activation of a complete semantic pattern. For                                   network. This suggests that, as predicted, the noise acts to
the noisy words, however, the network tends to activate only                                   ensure that the attractor basins are sufficiently wide to allow
a subset of the 20 units; this corresponds to the activation of                                the activation of the networks to enter the basin quickly. Later
a sense of the word that does not contain all the possible se-                                 in the processing, however, there is a disadvantage for these
mantic features for that word.                                                                 words; this may be because of competition between multiple
    3 Unambiguous words are considered to have settled correctly if                            stable states (within the large attractor) that correspond to the
they settled into the exact training pattern. Noisy words are consid-                          different senses of the words.4
ered to have settled correctly if they do not differ from their base
pattern by more than the amount of noise that was added during                                     4 Additional simulations, not reported here, show that the low er-
training. In a separate analysis, not reported here, this tolerance was                        ror rates for ambiguous words and ambiguity disadvantage seen in
also used for the unambiguous patterns; In this analysis, no the error                         Simulation 1 is also seen in the rate of activation of semantic units
rates differed from those reported here by more than 0.2%.                                     when these sparse representations are used (Rodd, 2000).

                         Conclusions                              in word senses is often systematic across words. Although the
                                                                  simulations reported here demonstrate important principles
The simulations reported here show that networks using the
                                                                  about how extreme forms of ambiguity can affect processing,
same architecture and learning rule can accommodate the two
                                                                  further work needs to be done using more realistic semantic
apparently opposite effects of semantic ambiguity reported
                                                                  representations. These issues are important if we are to fully
by Rodd et al. (in press). While the semantic competition
                                                                  understand the implications of ambiguity effects for theories
associated with the ambiguity between unrelated meanings
                                                                  about the representation and access of word meanings.
delays recognition, the flexibility around the base pattern seen
in words with many senses can produce a benefit.
                                                                                           References
   The ambiguity disadvantage shown in Simulation 1 is im-
portant because previous simulations of ambiguity effects us-     Azuma, T., & Van Orden, G. C. (1997). Why safe is better
ing networks of this type have shown an ambiguity advan-                than fast: The relatedness of a word’s meanings affects
tage. We argue that these earlier results were atypical, and            lexical decision times. Journal of Memory and Lan-
relied on using networks that were not able to disambiguate             guage, 36, 484–504.
between the different meanings of ambiguous words. Simula-        Borowsky, R., & Masson, M. E. J. (1996). Semantic ambi-
tions 2 and 3 show that a network of this type can also show a          guity effects in word identification. Journal of Experi-
benefit for words whose meanings are flexible between differ-           mental Psychology: Learning Memory and Cognition,
ent word senses, but only when their semantic features vary             22, 63–85.
within a limited set of possible features. This limitation fits
in with our intuitions about how the semantic representations     Gaskell, M. G., & Marslen-Wilson, W. D. (1999). Ambi-
of words senses vary.                                                   guity, competition and blending in speech perception.
   These contrasting effects of ambiguity can best be viewed            Cognitive Science, 23, 439–462.
in terms of the attractor structure of the network. The de-
lay in activating the meaning of an ambiguous word is due to      Hopfield, J. J. (1982). Neural networks and physical systems
competition between the two stable attractors that correspond           with emergent collective computational abilities. Pro-
to the two different meanings of the word. The initial state            ceedings of the National Academy of Sciences of the
of the semantic units produced by the orthographic input will           United States of America–Biological Sciences, 79(8),
correspond to an unstable blend of the two meanings; the at-            2554–2558.
tractor structure of the network will then move the activation
                                                                  Joordens, S., & Besner, D. (1994). When banking on mean-
of the units away from this blend state towards one of the sta-
                                                                        ing is not (yet) money in the bank - explorations in con-
ble attractors. This disambiguation process takes time, and
                                                                        nectionist modeling. Journal of Experimental Psychol-
is responsible for the observed ambiguity disadvantage. In
                                                                        ogy: Learning Memory and Cognition, 20, 1051–1062.
contrast, the different senses of a words all lie within a sin-
gle attractor basin. Further, the semantic flexibility associated Kawamoto, A. H., Farrar, W. T., & Kello, C. T. (1994). When
with these words results in a widening of the attractor basin,          two meanings are better than one: Modeling the ambi-
thus producing a processing advantage relative to unambigu-             guity advantage using a recurrent distributed network.
ous words. There may, however, be a disadvantage for these              Journal of Experimental Psychology: Human Percep-
words later in processing, due to the existence of multiple sta-        tion and Performance, 20, 1233–1247.
ble attractors within the large basin that corresponds to the set
of different senses of the word.                                  Millis, M. L., & Button, S. B. (1989). The effect of polysemy
   In summary, these simulations show that it is possible that          on lexical decision time: now you see it, now you don’t.
the pattern of ambiguity effects reported by Rodd et al. (in            Memory & Cognition, 17, 141–147.
press) can be explained in terms of the effects of these two
types of ambiguity on the competition to activate a coherent      Rodd, J. M. (2000). Semantic representation and lexical
semantic representation within an attractor network. The next           competition: Evidence from ambiguity. Unpublished
stage is to determine whether these explanations are correct.           doctoral dissertation, University of Cambridge.
   First, we have assumed that words with many senses             Rodd, J. M., Gaskell, M. G., & Marslen-Wilson, W. D. (in
should be characterised as words whose meanings are flex-               press). Making sense of semantic ambiguity: Semantic
ible about a core meaning; this assumption must be validated            competition in lexical access. Journal of Memory and
on the basis of detailed analysis of the stimuli used in the ex-        Language.
periments. Second, it needs to be confirmed that flexibility is
the key property responsible for the sense benefit. As noted      Sowa, J. F. (1993). Lexical structure and conceptual struc-
by Rodd et al. (in press), words with many senses differ from           tures. In J. Pustejovsky (Ed.), Semantics and the lexi-
words with few senses on a range of dimensions, including               con (pp. 223–262). Dordrecht/Boston/London: Kluwer
semantic richness and contextual predictability.                        Academic Publishers.
   Finally, these simulations investigate two extreme cases of
ambiguity; we have compared words with two completely un-
related meanings, with words whose different senses corre-
spond to all the possible combinations of a set of permitted
features. This is clearly unrealistic - most words with multi-
ple senses do have some level of structure, and the variation

