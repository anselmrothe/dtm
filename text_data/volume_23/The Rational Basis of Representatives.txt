UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
The Rational Basis of Representatives
Permalink
https://escholarship.org/uc/item/0mn5d090
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 23(23)
Authors
Tenenbaum, Joshua B.
Griffiths, Thomas
Publication Date
2001-01-01
Peer reviewed
 eScholarship.org                              Powered by the California Digital Library
                                                                University of California

                               The Rational Basis of Representativeness
                                           Joshua B. Tenenbaum & Thomas L. Griffiths
                                                     Department of Psychology
                                                        Stanford University
                                                  Stanford, CA 94305-2130 USA
                                              jbt,gruffydd @psych.stanford.edu
                            Abstract                               reasoning. We will first point out some shortcomings
                                                                   of previous accounts based on likelihood or similarity,
   Representativeness is a central explanatory construct in        and show how a Bayesian approach can overcome those
   cognitive science but suffers from the lack of a principled     problems. We will then compare the quantitative predic-
   theoretical account. Here we present a formal definition
   of one sense of representativeness – what it means to be        tions of Bayesian, likelihood, and similarity models on
   a good example of a process or category in the context          two sets of representativeness judgments.
   of Bayesian inference. This analysis clarifies the rela-
   tion between representativeness as an intuitive statistical                     Previous approaches
   heuristic and normative principles of inductive inference.
   It also leads to strong quantitative predictions about peo-     Likelihood. In trying to relate intuitions about repre-
   ple’s judgments, which compare favorably to alternative         sentativeness to rational statistical inferences, a natural
   accounts based on likelihood or similarity when evaluated       starting point is the concept of likelihood. Let d denote
   on data from two experiments.                                   some observed data, such as a sequence of coin tosses,
                                                                   and h denote some hypothesis about the source of d,
   Why do people think that Linda, the politically ac-             such as flipping a fair coin. The probability of observ-
tive, single, outspoken, and very bright 31-year-old, is           ing d given that h is true, P(d h), is called a likelihood.
more likely to be a feminist bankteller than to be a bank-         Let R(d, h) denote representativeness – how representa-
teller, even though this is logically impossible? Why do           tive the observation d is of the generative process in h.
we think that the sequence                 is more likely than        Gigerenzer & Hoffrage (1995) have proposed that rep-
the sequence              to be produced by flipping a fair        resentativeness, to the extent that it can be defined rig-
coin, even though both are equally likely? The standard            orously, is equivalent to likelihood: R(d, h) = P(d h).
answer in cognitive psychology (Kahneman & Tversky,                This proposal is appealing in that, other factors aside, the
1972) is that our brains are designed to judge “represen-          more frequently h leads to observing d, the more repre-
tativeness”, not probability: Linda is more representative         sentative d should be of h. It is also consistent with some
of feminist banktellers than of banktellers, and               is  classic errors in probability judgment, such as the con-
more representative of flipping a fair coin than is             ,  junction fallacy: a person is almost certainly more likely
despite anything that probability theory tells us.                 to match Linda’s description given that she is a bankteller
   Not only errors in probabilistic reasoning, but numer-          and a feminist than given only that she is a bankteller.
ous other phenomena of categorization, comparison, and                While likelihood and representativeness seem related,
inference have been attributed to the influence of repre-          however, they are not equivalent. Two observations with
sentativeness (or prototypicality or “goodness of exam-            equal likelihood may differ in representativeness. Know-
ple”; Mervis & Rosch, 1981; Osherson, Smith, Wilkie,               ing that         and          are equally likely to be pro-
Lopez, & Shafir, 1990; Rips, 1975). However, a princi-             duced by a fair coin does not change our judgment that
pled account of representativeness has not been easy to            the latter is the more representative outcome. Tversky
come by. Its leading proponents (Kahneman & Tversky,               & Kahneman (1983) provide several examples of cases
1996; Mervis & Rosch, 1981) have asserted that rep-                in which a more representative outcome is actually less
resentativeness should be defined only operationally in            likely. Any sequence of fair coin flips, such as           ,
terms of people’s judgments; an a priori, analytic defini-         is less likely than one of its subseqences, such as or
tion need not be given. Critics have countered that this                , but may easily be more representative. More color-
concept is too vague to serve as an explanation of intu-           fully, “being divorced four times” is more representative
itive probability judgment (Gigerenzer, 1996).                     of Hollywood actresses than is “voting democratic”, but
   This paper presents a framework for constructing ra-            the former is certainly less likely.
tional models of representativeness, based on a Bayesian              Figure 1 illustrates a simple version of the dissoci-
analysis of what makes an observation a good example               ation between representativeness and likelihood. Each
of a category or process. The goal is to identify pre-             panel shows a sample of three points from a Gaussian
cisely one sense of representativeness and show that it            distribution. With independent sampling, the total likeli-
has a rational basis in normative principles of inductive          hood of a sample equals the product of the likelihoods for

                                                               of similarity. That is, an observation d is representative
      p(X|h)
                                                               of a category or process h to the extent that it is similar
                                                               to the set of observations h typically generates.
                                                                  Similarity seems to avoid some of the problems that
                                                               likelihood encounters.             may be more representa-
                         X                         X           tive of a fair coin than          because it is more similar
                                                               on average to other coin flip sequences, based on such
Figure 1: Given a normal distribution, the left sample has     features as the number of heads or the number of alter-
                                                               nations. Likewise, someone who has been divorced four
greater likelihood but the right is more representative.
                                                               times may be more similar to the prototypical Hollywood
                                                               actress than someone who votes democratic, if marital
                                                               status is weighted more heavily than political affiliation
each item in the sample. Thus the left sample has much         in computing similarity to Hollywood actresses.
greater likelihood, because each point is much closer to          However, the explanatory power of a similarity-based
the peak of the distribution than in the right sample. Yet     account hinges on being able to specify what makes two
the more spread-out sample on the right seems more rep-        stimuli more or less similar, what the relevant features
resentative. We tested this intuition in a survey of 138       are and how are they weighted. Similarity unconstrained
Stanford undergraduates. They were first shown a nor-          is liable to lead to circular explanations: having had mul-
mally distributed set of thirty “widgets” produced by a        tiple divorces is more representative of Hollywood ac-
factory. The widgets were simple drawings resembling           tresses because marital status is more highly weighted in
nuts or bolts, varying only in their sizes. They were then     computing similarity to Hollywood actresses, but why is
shown three different samples, each with three widgets,        marital status so highly weighted, if not because having
and asked to rate on a scale of 1-10 how representative        multiple divorces is so typical of Hollywood actresses?
each sample was of the widgets produced by this factory.          Equating representativeness with similarity also runs
Each sample contained a point at the mean of the original      into a problem when evaluating the representativeness
distribution, and points at z = 2.85 (“broad sample”),         of a set of objects, as in Figure 1. Similarity is usu-
z = 1 (“intermediate sample”), or z = 0.05 (“narrow            ally defined as a relation between pairs of stimuli, but
sample”). The intermediate sample, with a standard de-         here we require a judgment of similarity between two
viation similar to the population, received a significantly    sets of stimuli, the sample and the population. It is not
higher rating than did the much more likely narrow sam-        immediately obvious how best to extend similarity from
ple (7.1 vs. 5.2, p <. 05). The broad sample, with lowest      a pairwise to a setwise measure. The individual elements
likelihood of all, also received a lower rating (6.9) than     of the left sample are certainly more similar to the av-
the intermediate sample, but not by a significant margin.      erage member of the population than are the elements
   We also tested whether intermediate-range samples           of the right sample. The left sample also comes closer to
are more representative for natural categories, using as       minimizing the average distance between elements of the
stimuli black-and-white pictures of birds. In a design         population and elements of the sample. If similarity be-
parallel to the widget study, 135 different Stanford un-       tween sets is defined according to one of these measures,
dergraduates saw three samples of birds, each contain-         it will fail to match up with representativeness.
ing three members, and rated how representative they              Finally, and most problematic for our purposes here,
were of birds in general. The samples consisted of either      a definition in terms of similarity fails to elucidate the
three robins (“narrow”); a robin, an eagle, and a seagull      rational basis of representativeness, and thus brings us
(“intermediate”); or a robin, an ostrich, and a penguin        no closer to explaining when and why representativeness
(“broad”). Although the robins were individually rated         leads to reasonable statistical inferences. Hence we seem
as more representative than the other birds (by a sepa-        to be left with two less-than-perfect options for defining
rate group of 100 subjects), the set of three robins was       representativeness: the simple, rational, but clearly in-
considered the least representative of the three samples.      sufficient concept of likelihood, or the more flexible but
As with the widgets, the intermediate sample was rated         notoriously slippery concept of similarity.
more representative (6.3) than either the narrow (5.1) or
broad (5.3) samples (p <. 05 for both differences).                            A Bayesian analysis
   For natural categories as well as for the artificial wid-   In this section we present a Bayesian analysis of repre-
gets, a set of representative examples turns out not to be     sentativeness that addresses some of the shortcomings of
the most representative set of examples. Sample likeli-        the likelihood and similarity proposals. As with likeli-
hood, because it is merely the product of each example’s       hood, Bayesian representativeness takes the form of a
individual likelihood, cannot capture this phenomenon.         simple probabilistic quantity, which in fact includes like-
At best, then, likelihood may be only one factor con-          lihood as one component. But like the similarity ap-
tributing to the computation of representativeness.            proach, it can account for dissociations of representative-
Similarity. Most attempts to explicate the mechanisms          ness and likelihood, when a less probable feature of the
of representativeness, including that of Kahneman &            stimuli is also more diagnostic of the process or category
Tversky (1972), rely not on likelihood but on some sense       in question. Moreover, it applies just as well to evaluat-

ing the representativenes of a set of examples (e.g. Figure      the origins of             and         : a fair coin (hF ), a
1) as it does to individual examples.                            two-headed coin (hT ), and a weighted coin (hW ) that
   Our notion of a “good example” is defined in the con-         comes up heads with probability 3/5. The likelihoods
text of a Bayesian inductive inference task. As above, let       of the two sequences under these hypotheses are, for
d denote some observed data, and let H = h1 , . . . hn           the fair coin, P(           hF ) = P(        hF ) = (1 2)5 =
denote a set of n hypotheses (assumed to be mutually ex-         0.03125; for the two-headed coin, P(                   hT ) = 1
clusive and exhaustive) that might explain the observed          while P(           hT ) = 0; and for the weighted coin,
data. For each hi , we require both the likelihood P(d hi )      P(         hW ) = (3 5)5 = 0.0778 while P(               hW ) =
and a prior probability, P(hi ), which expresses the degree      (3 5)3 (2 5)2 = 0.0346. For concreteness, we choose
of belief in hi before d is observed. Let h̄i = h j H :          specific prior probabilities for these hypotheses: P(hF ) =
 j = i denote the negation of hypothesis hi , the asser-         0.9, P(hT ) = 0.05, and P(hW ) = 0.05.                  Substi-
tion that some hypothesis other than hi is the true source       tuting these numbers into Equation 4, we have
of d. Then we define our measure of representativeness                     , hF ) = log 1×0.05 0.10.03125
                                                                 R(                                  0.0778×0.05 0.1 = −2.85,
R(d, hi ) to be the logarithm of the likelihood ratio
                                                                 while R(           , hF ) = log 0×0.05 0.10.03125
                                                                                                              0.0346×0.05 0.1 =
                                  P(d hi )                       0.59. This result, that            is more representative of
                       L(d hi ) =           .                (1)
                                  P(d h̄i )                      a fair coin than          , accords with intuition and holds
                                                                 regardless of the prior probabilities we assign to the three
This definition is motivated by Bayes’ rule, which pre-          alternative hypotheses. In a later section, we go be-
scribes a degree of belief in hypothesis hi after observing      yond a qualitative reconstruction of intuitions to test a
d given by the posterior probability                             quantitative model of representativeness judgments for
                                P(d hi )P(hi )                   sequences of coin flips.
                    P(hi d) =                  .             (2)    The Bayesian approach also accounts for cases where
                                    P(d)
                                                                 a sample with lower likelihood appears more repre-
Defining the posterior odds O(hi d) = P(hi d) P(h̄i d) =         sentative. For instance, P(                     hF ) is strictly
P(hi d) (1 − P(hi d)), and the prior odds O(hi ) =               lower than either P(              hF ) or P(          hF ), but
P(hi ) (1 − P(hi )), we can write Bayes’ rule in the form:                      is no less representative than            . The
                                                                 Bayesian account also offers an intuitively compelling
             log O(hi d) = log L(d hi )     log O(hi ).      (3) definition of representativeness for a set of examples,
Equation 3 shows why the log likelihood ratio,                   such as the widgets in Figure 1. We demonstrate by
log L(d hi ), provides a natural measure of how good an          computing the representativeness for a sample X from
example d is of hi : it indicates the extent to which ob-        a Gaussian population h1 . Let x1 , . . . , xN be the N ex-
serving d increases or decreases the posterior odds of           amples in X, m be the mean of X, and S = ∑i (xi −m)2 the
hi relative to the prior odds. Researchers in statistics         sum-of-squares. Let h1 have mean µ and variance σ2 . We
(Good, 1950), artificial intelligence (Pearl, 1988), and         take the hypothesis space H to include all possible Gaus-
philosophy of science (Fitelson, 2000) have previously           sian distributions in one dimension – each a conceivable
considered log L(d hi ) as the best measure for the weight       alternate explanation for the sample X. Because H is
of evidence that d provides for hi , because it captures the     an uncountably infinite set, the sum in the denominator
unique contribution that d makes to our belief in hi inde-       of Equation 4 becomes an integral. Assuming an unin-
pendently of all other knowledge that we have (reflected         formative Jeffreys prior on µ, σ (Equation 3 of Minka,
in P(hi )).                                                      1998), our expression for Bayesian representativeness in
   To compute R(d, hi ) in the presence of more than one         Equation 4 then reduces to
alternative hypothesis, we express it in the form                                                  1                  
                                                                         R(X, h1 ) = N log S − 2 N(m − µ)2 S ,                (5)
                                     P(d hi )                                                     σ
            R(d, hi ) = log                             .    (4)
                            ∑h j H P(d h j )P(h j h̄i )          plus a term that depends only on N and σ2 .
                                                                    Equation 5 is maximized when m = µ and S N =
P(h j h̄i ) is the prior probability of h j given that hi is not σ2 , that is, when the mean and variance of the sam-
the true explanation of d: 0 when i = j and P(h j ) (1 −         ple X match the mean and variance of the population
P(hi )) otherwise. Equation 4 shows that d is representa-        h1 . This result is intuitive, and it accounts for why peo-
tive of hi to the extent that its likelihood under hi exceeds    ple preferred intermediate samples of widgets or birds
its average likelihood under alternative hypotheses.             over broad or narrow samples in the surveys described
   To illustrate the analysis concretely, consider the sim-      above: the N log S term penalizes narrower samples and
ple case of two coinflip sequences,                 and        . the −S σ2 penalizes broader samples. Yet this result
Unlike the likelihood model, we cannot compute how               is also not particularly surprising. More interestingly,
representative an observation is of a hypothesis with-           Equation 5 gives a general metric for scoring the rep-
out specifying the alternative hypotheses that an ob-            resentativeness of any sample from a Gaussian distribu-
server might consider. In the interests of simplic-              tion, which we will test quantitatively against people’s
ity, we consider just three relevant hypotheses about            judgments in the following section.

                  Quantitative modeling                      hypothesis based on the mean values of α and β over
In this section, we present quantitative models of repre-    the whole distribution of sequences generated by that hy-
sentative judgments for two kinds of stimuli: sequences      pothesis. For example, for h2 , α = 4 and β = 7; for h3
of coin flips and sets of animals. For each data set, we     (again assuming “mostly” means with probability 0.85),
compare the predictions of Bayesian, likelihood-based,       α 6.8 and β 1.8. Lastly, we define the represen-
and similarity-based models.                                 tativeness of sequence i for hypothesis j as R(di , h j ) =
                                                             sim(di , h j ) ∑k sim(di , hk ). The dimensional weights wα
Coin flips                                                   and wβ are free parameters optimized to fit the data, giv-
                                                             ing wα = 1, wβ = 0.4.
Methods. 278 Stanford undergraduates rated the rep-
resentativeness of four different coin flip sequences for    Results. To compensate for nonlinear transformations
each of four hypothetical generative processes, under the    that might affect the 1-7 rating scale used by subjects,
cover story of helping a casino debug a new line of gam-     the predictions of each model were first transformed ac-
bling machines. The sequences were d1 =                    , cording to a power function with a power γ chosen to op-
d2 =                , d3 =          , and d4 =             . timize each model’s fit, and then mapped onto the same
The generative processes were h1 = “A fair coin”, h2 =       interval spanned by the data. This gives both the likeli-
“A coin that always alternates heads and tails”, h3 = “A     hood model and the Bayesian model one free parameter
coin that mostly comes up heads”, and h4 = “A coin that      plus two constrained parameters (corresponding to the
always comes up heads”. The orders of both sequences         meanings of “mostly” and “always”), while the similar-
and hypotheses were randomized across subjects. Rep-         ity model has three free parameters (wα , wβ , and γ) and
resentativeness judgments were made on a scale of 1-7.       the same two constrained parameters. All three models
                                                             correlate highly with subjects’ representativeness judg-
Bayesian model. While people could construct an ar-          ments, although the Bayesian model has a slight edge
bitrarily large hypothesis space for this task, we make      with r = 0.94, versus 0.87 for the likelihood model and
the simplifying assumption that their hypothesis space       0.92 for the similarity model. Figure 2 presents an item-
can be approximated by just the four hypotheses that they    by-item analysis, showing that the Bayesian model cap-
are asked to make judgments about. We constructed sim-       tures virtually all of the salient patterns in the data.
ple probabilistic models for each hypothesis hi to gener-
ate the necessary likelihoods P(d j hi ). Priors for all hy- Animals
potheses were assumed to be equal. To model h1 , “a fair
coin”, all likelihoods were set equal to their true values   Methods. We used data reported by Osherson, Smith
of 1 28 . To model h3 , “mostly heads”, and h4 , “always     et al. (1990; Tables 3 and 4) in a study of category-
heads”, we used binomial distributions with p = 0.85         based induction. They asked one group of subjects to
and p = 0.99, respectively. In some sense, these p val-      judge pairwise similarities for a set of 10 mammals, and
ues represent free parameters of the model, but their val-   a second group of subjects to judge the strengths of 45
ues are strongly constrained by the meaning of the words     arguments of the form x1 has property P, x2 has prop-
“mostly” and “always”. Their exact values are not cru-       erty P, x3 has property P, therefore all mammals have
cial to the model’s performance, as long as “always” is      property P , where x1 , x2 and x3 are three different kinds
taken to mean something like “almost but not quite al-       of mammals and P is a blank biological predicate. Such
ways” (i.e. p < 1.0). To model h4 , “always alternates       judgments of argument strength are not the same thing
heads and tails”, we used a binomial distribution over the   as judgments of representativeness, but for now we take
seven possible state transitions in each sequence, again     them as a reasonable proxy for how representative the
with “always” translated into probability as p = 0.99. All   sample X = x1 , x2 , x3 is of the set of all mammals.
model predictions were then given by Equation 4.             Bayesian model. We assume that people’s hypothesis
Likelihood model. This model treats representative-          space includes the category of all mammals (hM ), as well
ness judgments simply as P(d j hi ), as specified above.     as an infinite number of alternative hypotheses. For sim-
                                                             plicity, we model all hypotheses as Gaussian distribu-
Similarity model. We defined a simple similarity-            tions in a two-dimensional feature space obtained from a
based model in terms of two intuitively relevant fea-        multidimensional scaling (MDS) analysis of the similar-
tures for comparing sequences: the number of heads in        ity judgments in Osherson et al. (1990). This allows us to
each sequence and the number of alternations in each se-     apply essentially the same analysis used in the previous
quence. Let α j be the number of heads in sequence j,        section to compute the representativeness of a sample
and β j be the number of alternations. Then the similarity   from a Gaussian distribution (Equation 5), and also par-
of sequences di and d j is defined to be                     allels the original approach to modeling category-based
                                                           induction of Rips (1975). The MDS space for animals is
   sim(di , d j ) = exp −wα αi − α j − wβ βi − β j ,     (6) shown in Figure 3. The large gray oval indicates the one-
                                                             standard-deviation contour line of hM , which we take to
where wα and wβ are the weights given to these two fea-      be the best fitting Gaussian distribution for the set of all
tures. To compute similarity between a sequence and a        ten mammals. We assume the set H of alternative hy-
generating hypothesis, we construct a prototype for each     potheses includes all Gaussians in this two-dimensions

                         Sequence: HHTHTTTH                                 Sequence: HTHTHTHT                                  Sequence: HHTHTHHH                            Sequence: HHHHHHHH
                                          data
                     7                                                  7                                                   7                                             7
                                          model
Representativeness                                 Representativeness                                  Representativeness                            Representativeness
                     5                                                  5                                                   5                                             5
                     3                                                  3                                                   3                                             3
                     1                                                  1                                                   1                                             1
                         h     h    h      h                                h     h    h    h                                    h    h    h    h                              h    h    h    h
                          1     2    3      4                                1     2    3    4                                    1    2    3    4                              1    2    3    4
Figure 2: Representativeness judgments for coin flip sequences. Each panel shows subjects’ mean judgments and
the Bayesian model predictions for the representativeness of one sequence with respect to four different generating
hypotheses: h1 = “A fair coin”, h2 = “A coin that always alternates heads and tails”, h3 = “A coin that mostly comes
up heads”, and h4 = “A coin that always comes up heads”.
space, and we again use the uninformative Jeffreys’ prior                                              Other models. We also compare the predictions of a
P(h) (Minka, 1998; Equation 3). How representative a                                                   simple likelihood model, which equates representative-
sample X (e.g. horse, cow, squirrel ) is of all mammals                                                ness with P(X hM ), and Sloman’s (1993) feature-based
can then be computed from a multidimensional version                                                   model. Heit (1998) also presented a Bayesian model of
of Equation 5 (ignoring terms equal for all samples):                                                  category-based induction tasks, but because his model
                                                                                                       depends heavily on the choice of priors, it does not make
                     R(X, hm ) = N log S − N(m − µ)T V−1 (m − µ)                                       strong quantitative predictions that can be evaluated here.
                                         −trace(SV−1 ),                                          (7)
                                                                                                       Results. Figure 3 plots the argument strength judg-
                                                                                                       ments for 45 arguments versus the representativeness
where m is the mean of X, S = ∑i (xi − m)T (xi − m), xi                                                predictions of the probabilistic and similarity-based
are the MDS coordinates of example i, N is the number                                                  models. Both the Bayesian and max-similarity models
of examples in X, and µ and V are the mean and covari-                                                 predict the data reasonably well (r = 0.80 vs. r = 0.88),
ance matrix of hM (Minka, 1998). Equation 7 measures                                                   with no significant difference between them (p > .2).
the representativeness of any sample X of N mammals                                                    Neither of these models has any free numerical param-
in terms of the distance between the best fitting Gaus-                                                eters. With one free parameter, the feature-based model
sian for the sample (mean m, covariance S/N) and the                                                   performs slightly worse (r = 0.71). Interestingly, both
best fitting Gaussian for the set of all mammals (mean                                                 the likelihood and sum-similarity models show a weak
µ, covariance V). Figure 3 illustrates this graphically, by                                            negative correlation with the data (r = −.31, r = −.26).
plotting one-standard-deviation contours for three sam-                                                This discrepancy directly embodies the insight of Fig-
ples that vary in how representative they are of the set of                                            ure 1: high likelihood can yield low representativeness
all mammals. Observe that the more representative the                                                  when the sample is tightly clustered near the mean, as in
sample, the greater the overlap between its best-fitting                                               the sample of horse, cow, rhino (ellipse C in Figure 3).
Gaussian and the best-fitting Gaussian for the whole set.                                              Sum-similarity performs as poorly as likelihood because
Similarity-based models. Osherson et al. (1990) re-                                                    it is essentially a nonparametric estimate of likelihood;
port pairwise similarity judgments for the animals, but to                                             likewise, max-similarity performs well because it corre-
construct a similarity-based model of this representative-                                             lates highly with Bayesian representativeness.
ness task, we need to define a setwise measure of simi-
larity between any sample of three animals and the set of                                              Discussion
all mammals. The similarity-coverage model proposed                                                    Overall, the Bayesian models provide the most satisfy-
by Osherson et al. defines this quantity as the sum of                                                 ing account of these two data sets. On the coinflip data,
each category instance’s maximal similarity to the sam-                                                not only does Bayes obtain the highest correlation, but it
ple: R(X, hM ) = ∑ j maxi sim(i, j), where j ranges over                                               does so with the minimal number of free parameters. On
all mammals and i ranges over just those in the sample X.                                              the animals data, Bayes obtains a correlation competitive
A more traditional similarity-based model might replace                                                with the best of the other models, max-similarity, even
the maximum with a sum: R(X, hM ) = ∑ j ∑i sim(i, j).                                                  though it is based on less than half as much input data (20
Osherson et al. (1990) consider both max-similarity and                                                MDS coordinates versus 45 raw similarity judgments)
sum-similarity models but favor the former as it is more                                               and may be hindered by information lost in the MDS pre-
consistent with their phenomena. However, there seems                                                  processing step. Most importantly, the Bayesian models
to be little a priori reason to prefer max-similarity, and                                             are based on a rational analysis, which provides a sin-
indeed most similarity-based models of classification are                                              gle principled definition of representativeness applicable
closer to sum-similarity, so we consider both here.                                                    across the two quite different domains of coinflips and

                        B                                          1                                1
                                                                            r=0.80        A                            A r=−0.31
                  cow
                                                           Data
                                      A
      rhino          horse                                        0.5                 B           0.5                  B
              C     elephant
                                          seal                              C                                              C
                                                 dolphin           0                                0
                                                                          Bayesian model                 Likelihood model
                                                                   1                                1
                                                                           r=0.88         A                            A r=−0.26
    mouse                   gorilla
                                                                  0.5                B            0.5                  B
      squirrel
                            chimp                                               C                                      C
                                                                   0                                0
            MDS space for animals                                       Max−Similarity model         Sum−Similarity model
Figure 3: Modeling representativeness for sets of mammals. Ellipses in the MDS space of animals (left) mark one-
standard-deviation contours for the set of all mammals (thick), a representative sample ( horse, chimp, seal , A), a
somewhat representative sample ( horse, mouse, rhino , B), and a less representative sample ( horse, cow, rhino , C).
Scatter plots (right) compare strength judgments for 45 arguments with the predictions of four models (see text).
animals. In contrast, the similarity-based models have                                         References
no rational grounding and take on very different forms                   Anderson, J. R. (1990). The adaptive character of thought.
in the two domains. They achieve high correlations, but                     Erlbaum, Hillsdale, NJ.
only through the introduction of multiple free parame-                   Fitelson, B. (2000).          A Bayesian account of inde-
                                                                            pendent evidence with applications.              Available at
ters, such as the feature weights on the coin flip data, or                 http://philosophy.wisc.edu/fitelson/psa2.pdf.
ad hoc assumptions, such as the choice of max-similarity                 Gigerenzer, G. and Hoffrage, U. (1995). How to improve
over sum-similarity on the animal data. On the other                        bayesian reasoning without instruction: frequency formats.
hand, similarity-based models do have the advantage of                      Psychological Review, 102:684–704.
requiring only simple computations. Thus both Bayesian                   Good, I. J. (1950). Probability and the weighing of evidence.
                                                                            Charles Griffin & Co., London.
and similarity-based models may have something to of-                    Heit, E. (1998). A Bayesian analysis of some forms of induc-
fer, but at different levels of analysis. Similarity may                    tive reasoning. In M. Oaksford and N. Chater, editors, Ratio-
provide a reasonable way to describe the psychologi-                        nal models of cognition, pages 248–274. Oxford University
cal mechanisms of representativeness, while a Bayesian                      Press, Oxford.
                                                                         Kahneman, D. and Tversky, A. (1972). Subjective probability:
analysis may provide the best explanation of why those                      A judgment of representativeness. Cog. Psych., 3:430–454.
mechanisms work the way they do: why different fea-                      Kahneman, D. and Tversky, A. (1996). On the reality of cog-
tures of sequences are weighted as they are in the coinflip                 nitive illusions. Psychological Review, 103:582–591.
example, or why max-similarity provides a better model                   Mervis, C. B. and Rosch, E. (1981). Categorization of natural
for inductive reasoning than does sum-similarity.                           objects. Annual review of psychology, 32:89–115.
                                                                         Minka, T. P. (1998).        Inferring a gaussian distribution.
                                                                            http://www-white.media.mit.edu/ tpminka/papers/gaussian.html.
                        Conclusion                                       Oaksford, M. and Chater, N. (1998). Rational models of cog-
We have argued that representativeness is best under-                       nition. Oxford University Press, Oxford.
                                                                         Osherson, D. N., Smith, E. E., Wilkie, O., Lopez, A., and
stood as a Bayesian computation, rather than as a judg-                     Shafir, E. (1990). Category-based induction. Psychological
ment of similarity or likelihood. Our analysis makes pre-                   Review, 97:185–200.
cise one core sense of representativeness – the extent to                Pearl, J. (1988). Probabilistic reasoning in intelligent systems:
which something is a good example of a category or pro-                     Networks of plausible inference. Morgan Kaufmann.
                                                                         Rips, L. J. (1975). Inductive judgments about natural cate-
cess – and exposes its underlying rational basis. Ratio-                    gories. J. Verbal Learning and Verbal Behav., 14:665–681.
nal models have been successfully applied to a number                    Shepard, R. N. (1987). Toward a universal law of generaliza-
of cognitive capacities (Shepard, 1987; Anderson, 1990;                     tion for psychological science. Science, 237:1317–1323.
Oaksford & Chater, 1998) but not previously to analyz-                   Sloman, S. A. (1993). Feature-based induction. Cog. Psych.,
ing representativeness, which is traditionally thought of                   25:231–280.
                                                                         Tversky, A. and Kahneman, D. (1983). Extensional vs. intu-
as an alternative to normative probabilistic judgment. By                   itive reasoning: The conjunction fallacy in probability judg-
clarifying the relation between our intuitive sense of rep-                 ment. Psychological Review, 90:293–315.
resentativeness and normative principles of statistical in-              Acknowledgements. Supported by Mitsubishi Electric
ference, our analysis may lead to a better understanding                 Research Labs and a Hackett studentship to TLG. N. Davi-
of those conditions under which human reasoning may                      denko, M. Steyvers, and M. Strevens gave helpful comments.
actually be rational or close to rational, as well as those
situations in which it truly deviates from a rational norm.

