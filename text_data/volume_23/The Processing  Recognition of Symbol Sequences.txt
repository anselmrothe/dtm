UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
The Processing &amp; Recognition of Symbol Sequences
Permalink
https://escholarship.org/uc/item/7bz2v2rn
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 23(23)
Author
Andrews, Mark W.
Publication Date
2001-01-01
Peer reviewed
 eScholarship.org                              Powered by the California Digital Library
                                                                University of California

                       The Processing & Recognition of Symbol Sequences
                                           Mark W. Andrews (mwa1@cornell.edu)
                                               Department of Psychology; Uris Hall
                                                       Ithaca, NY 14853 USA
                            Abstract                                den Markov Models are limited in their generality due
                                                                    to their fundamental inability to handle patterns above a
                                                                    certain complexity. This absence of general models for
   It is proposed that learning a language (or more gener-          temporal pattern recognition is evident in the study of
   ally, a sequence of symbols) is formally equivalent to           human language processing, which traditionally has es-
   reconstructing the state-space of a non-linear dynamical
   system. Given this, a set of results from the study of           chewed serious consideration of statistical learning and
   nonlinear dynamical systems may be especially relevant           pattern recognition.
   for an understanding of the mechanisms underlying lan-              This paper aims to introduce a general framework for
   guage processing. These results demonstrate that a dy-           the study of temporal pattern recognition. This is devel-
   namical system can be reconstructed on the basis of the
   data that it emits. They imply that with minimal assump-         oped in the context to language processing, but it could
   tions the structure of an arbitrary language can be inferred     be extended in a straightforward manner to most other
   entirely from a corpus of data. State-Space reconstruc-          cases of temporal patterns. First, a general characteriza-
   tion can implemented in a straightforward manner in a            tion of the problem of language learning and language
   model neural system. Simulations of a recurrent neural           processingis proposed. Then, some recent results in
   network, trained on a large corpus of natural language,
   are described. Results imply that the network sucessfully        the study of nonlinear dynamical systems are described.
   recognizes temporal patterns in this corpus.                     These are seen as being especially relevant for an under-
                                                                    standing of the mechanisms underlying temporal pattern
                                                                    recognition, especially with regard to language process-
                        Introduction                                ing. Finally, simulations with a recurrent neural network
Complex pattern recognition is often characterized by               are described, which suggest successful pattern recogni-
means of a simple geometric analogy. Any object or                  tion of English sentences.
pattern may be described as a single point in a high-
dimensional space. For example, a square grayscale im-                     The processing of symbol sequences
age that is 256 pixels in length, may be described as a
point in the 2562 dimensional space of all possible im-             A paradigm for the study temporal pattern processing,
ages. A collection of such images is a set of points in             especially language processing, has developed as a re-
this space. If these patterns are not entirely random, this         sult of the deep relationship between formal languages
set will reside in a subspace of lower dimensionality. To           and abstract automata (Chomsky 1963)1. Any language
learn the structure of these images, an organism or ma-             (or more generally, any sequence of symbols), can be de-
chine must discover a compact parametric representation             scribed as the product of a particular automaton. By this
of this subspace. This might take the form of, for exam-            account, learning a language is equivalent to identifying
ple, finding a reasonably small set of basis vectors that           a particular automaton on the basis of a sample of the
will span the subspace and projecting each image onto               language that it generates. More formally, an automa-
these vectors. Having done this, each image can be clas-            ton A is specified by the quadruple X  Y  F  G . X and
sified in terms of a new and more meaningful coordinate             Y are sets known as the state and the output spaces, re-
system. You effectively describe ’what is there’ in terms           spectively. The functions F : X  X and G : X  Y
of ’what is known’.
                                                                        1 The correspondence between formal languages and ab-
   This geometric approach is routinely employed in the
study of visual object recognition, but may easily be               stract automata can be summarized by the so-called Chomsky
                                                                    hierarchy: Classes of automata that are increasingly restrictive
extended to a wide range of categorization and classi-              versions of the Turing machine produce classes of languages
fication tasks. In almost all cases, however, the pat-              described by increasingly restrictive generative grammars. The
terns under study have been multi-dimensional static pat-           regular languages R are produced by strictly finite automata,
terns. In contrast, the study of temporal pattern recogn-           the context-free languages CF are produced by pushdown stack
                                                                    automata, the context-sensitive languages CS are produced by
tion using this or related approaches has not been well-            linear bounded automata and the recursively enumerable lan-
developed. For example, one of the most widely em-                  guages RN are produced by unrestricted Turing machines. R 
ployed techniques for temporal pattern recognition, Hid-            CF  CS  RN, and likewise for their corresponding automata.

are the state-transition and the output functions, respec-          By introducing A ¼ = x y f  g, the language gener-
tively. Beginning at time t0 and continuing until t∞ , the     ating process is being explicitly defined as a nonlinear
sentence-generator A constantly changes from one state         dynamical system. For example, the system may be de-
in X to the next, according to its state-transition func-      scribed by a set of coupled differential equations
tion F . At each transition, a symbol from the set Y is
emitted, according to its output function G .                                             ẋi  f x δ
    A langage learner attempts to identify the nature of
this automaton on the basis of a sample of the language        where x is the system’s state and ẋi is a vector-field de-
that it generates. That is to say, the language learner is     fined on an m-dimensional manifold M . δ is a unspec-
exposed to a finite sequence of Y and from this must           ified stochastic element in the system. The language
attempt to identify A  X  Y  F  G . Having attained      being produced by this system is a result of the coarse-
knowledge of A , the learner is said to have full knowl-       coding function
edge of the structure of the language. The learner has
the capability to produce all the sentences of the lan-                                     y  gx
guage, including the infinite number of sentences that
were never seen. Likewise, the learner has the capability      where y is a variable representing the symbols of the lan-
to parse the syntactic form of any sentence of the lan-        guage. However, there are still formal similarities be-
guage. This ability also extends to the infinite number of     tween the discrete automaton A  X  Y  F  G  and its
never-encountered sentences. As syntactic parsing is a         continuous counterpart A ¼  x y f  g. x is the state-
necessary precondition for the interpretation of language,     space of A ¼ and y is a variable representing its output.
it is said that the language has been learned once knowl-      The function f : x  x describes the state evolution of
edge of its grammar has been attained.                         the system while g : x  y is an output function. In fact,
                                                               the only essential distinction between A  X  Y  F  G 
                                                               and   x y f  g is that in the latter case the state-space
    While the correspondence between formal languages
and automata has allowed the problem of language learn-
ing to be given an explicit characterization, these au-       x is continuous, rather than discrete, and the evolution of
tomata A  X  Y  F  G  have always been taken to be       the system described by f is smooth, preventing discon-
discrete parameter systems. To continue this paradigm          tinuous leaps through space.
it is useful to demonstrate the correspondence between
generative grammars and continuous as well as discrete                      State-Space Reconstruction
automata. Within nonlinear dynamical systems theory,           A language learner can be said to be attempting to iden-
the study of symbolic dynamics has made apparent the re-       tify the process generating the language. If this generat-
lationships between formal languages, generative gram-         ing process is described as a continuous dynamical sys-
mars and continuous dynamical systems. Symbolic dy-            tem, the objective is to model the dynamical system ẋ 
namics refers to the practice of coarse-coding the ambi-        f x on the basis of the language it outputs, yt0      yt .
ent state-space of a dynamical system into a finite set of     Prima facie, this problem is widely intractable. The sym-
subspaces and assigning a symbol to each. Whenever the         bols to which the learner is exposed do not identify the
system enters a partition, the assigned symbol is emitted.     state of the system. They are a product of the composi-
In this way, the trajectories of the dynamical system can      tion of two unknown and probably non-linear functions,
be represented as strings of symbols. Unless the system         f and g. However, it may be fruitful to consider the anal-
is entirely stochastic, only a certain subset of strings will  ogy between this problem and a more general problem
occur. It can be shown that these strings define a lan-        encountered in the experimental analysis of complex sys-
guage and the system producing them can be described           tems. For example, a scientist observing a sequence of
by a generative grammar (Bai-Lin & Wei-Mou 1998).              individual measurements from a complex physical pro-
    The relationship between languages, grammars and           cess (e.g. a fluid in turbulent motion) may be interested
dynamical systems has been further described by Tabor          in understanding the properties of the underlying sys-
(1998). In that work, and in Tabor (2000), the compu-          tem. In the absence of prior knowledge and without loss
tational capacities of a pushdown stack automaton were         in generality, the system can be taken to be a stochastic
identified with those of a stochastic dynamical system,        dynamical system, whose functional form is completely
based on an iterated function system. This was used to         unknown. The scientist must infer its functional form
demonstrate the recognition of context-free languages by       on the basis of the measurement data alone. One of the
a simple 2dimensional dynamical system. Following             more remarkable outcomes of dynamical systems theory
Tabor’s approach, it is reasonable to propose that any         is that in many general cases this problem is tractable. In
language (or any symbol sequence) generating process           virtue of the analogy, the manner by which this is done
may be legitimately described as a continuous as well as       may also elucidate the problem of language learning.
a discrete system. Accordingly, and by keeping a strict             Packard, Crutchfield, Farmer & Shaw (1980) were first
analogy with the automaton A  X  Y  F  G  described      to demonstrate that a dynamical system could be recon-
above, it is possible to introduce the corresponding con-      structed entirely on the basis of its output. They pro-
tinuous system, A ¼ defined by the quadruple x y f  g    posed that any time-series of quantities measured from a
.                                                              dynamical system may be sufficient to construct a model

that preserves its essential structure. Takens (1981) de-                       delay coordinate map D ¼ : M      Ês , where s is an integer
                                                                                arbitrarily greater than 2m  1, and a smooth transforma-
                                                                                                          
veloped and clarified the mathematical evidence for this
proposal. This was considerably generalized by Sauer,                           tion of this map, φ : D ¼    Ê2m1 . In the spirit of Takens
                                                                                                               Æ          
Yorke & Casdagli (1991), and more recently Stark,                               (1981), Sauer et al. (1991) demonstrate that the set of
Broomhead, Davies & Huke (1997) have extended these                             these composite functions φ D ¼ : M          Ê2m1 that are
results to the more general case of stochastic dynamical                        also diffeomorphism is open and dense in the function
systems.                                                                        space.
   Sauer et al. (1991) have suggested that the foundations                         The theorems of Takens (1981) and Sauer et al. (1991)
of these ideas are to be found in differential topology. For                    apply to deterministic dynamical systems. These are sys-
example, a seminal theorem in this field (Whitney 1936)                         tems whose entire future evolution can be be determined
is that any m-dimensional manifold M can be mapped                              from precise knowledge of the system’s state. As real
by a diffeomorphism2 into Euclidean space Êd if d                              world systems are inevitably coupled with sources of ex-
2m  1. Moreover, the subset of all possible smooth maps                        ternal noise, the generality of these theorems may seem
from M to Ê2m1 that are also diffeomorphisms is both                           limited. Stark et al. (1997) have shown, however, that the
open and dense in the function space. As Sauer et al.                           embedding theorems can be generalized to a much less
(1991) point out a single measurement of a dynamical                            restricted class of stochastic dynamical systems. They
                                                                                                                                     
system is a map from the system’s state to the real line.                       consider a discrete time system where at each time step
As such, the significance of Whitney’s result is that al-                       one of k different discrete-time maps fω : M             M is
most every3 set of 2m  1 independent measurements of                           chosen, where ω  1  k. As in Takens (1981), they
a dynamical taken simultaneously is sufficent to recon-                         define the delay-coordinate map, D : M          Ê2m1 and
struct the dynamical system in the measurement-space.                           show that in the stochastic systems under consideration
The manifold M and its vector-field ẋ are embedded in                          the set of maps D that are also diffeomorphism is open
the measurement-space.                                                          and dense in the function space.
   The more recent result by Takens (1981) may be un-
                                                                     
derstood in in terms of this embedding theorem. Takens                                State-Space reconstruction in neural
considers the case of a dynamical system f x δ : M
M and the delay-coordinate map, D : M Ê2m1 . This                                                      systems
                                           
map D is defined as simply a time-series of scalar mea-                         While these results are obviously important for the gen-
surements z  yt  yt 1  yt 2m obtained from this                        eral problem of nonlinear time-series analysis, their rele-
system, where y  gx. It is clear that                                       vance for the problem of language learning may be lim-
                                gx  g Æ f x   g Æ f    xt  
                                                                                ited. The problem of language learning does not fit neatly
z  yt  yt 1   yt 2m            t            t
                                                                    2m
                                                                                into the scenarios considered by Takens (1981), Sauer
                                                                                et al. (1991) and Stark et al. (1997). This is primarily
where f n is the composition of f n-times. In other words,                      due to the fact that the output of the language generat-
the sequence z of 2m  1 measurements y  gx is in fact                      ing dynamical system is a sequence of symbols rather
a function of a single point or state x of the hidden dy-                      than a real-valued scalar. In addition, the stochastic sys-
namical system. The delay-coordinate map D maps each                            tem considered by Stark et al. (1997) might not be gen-
state x of the hidden system to a point in Ê2m1 . Tak-                        eral enough to describe the arbitrary stochastic dynami-
ens (1981) demonstrated that with minimal assumptions                           cal system that is here taken to be the language generator.
about the hidden dynamical system4 , the set of delay-                          More importantly, these theorems consider and explain
coordinate maps D that are also diffeomorphisms is both                         certain sufficient conditions and do not lead naturally to
open and dense in the space of maps D . In almost every                         a general algorithmic procedure for reconstructing state-
case, the hidden dynamical system is embedded within                            space. For example, Taken’s theorem demonstrates that
the delay-coordinate measurement space.                                         the coordinate space of 2m  1 scalar measurements is
   Sauer et al. (1991) have considerably elaborated the                         sufficient to embed the generating dynamical system of
Takens (1981) embedding theorem. They define both a                             dimensionality m. Practically, however, this just means
    2A   diffeomorphism from M to N is a one to one map,                        that the coordinate space of a finite number of scalar mea-
where the map and its inverse are diferentiable.                                surements is sufficient for the embedding. It does not in-
    3 The fact that the set of maps that are also diffeomorphisms               dicate how it can be known that an embedding has in fact
is an open subset of the function space means that any arbitrar-                occurred. What is necessary, therefore, is an objective
ily small perturbation of a diffeomorphism is also a diffeomor-                 function that may be optimized to produce a reconstruc-
phism. The fact that the set is dense means that every point
in the function space is arbitrarily close to a diffeomorphism.                 tion of the dynamical systems from its outputs.
In addition, Sauer et al. (1991) have shown that almost every                      Crutchfield & Young (1989) introduce ε-machines as
map in the function space is a diffeomorphism, in that the com-                 a general procedure for state-space reconstruction. They
plement to this subset is of measure zero. In other words, the                  propose that the state of the ε-machine uniquely corre-
likelihood of an arbitrary map also being a diffeomorphism is
probability one, or infinitely likely.                                          sponds to the state of a dynamical system emiting a sym-
    4 In particular, it is assumed that the dynamical does not con-             bol sequence if it can be shown that its state renders the
tain periodic orbits that are exactly equal to (or exactly twice)               future of the symbol sequence conditionally independent
the sampling rate of the measurement function y  gx.                        of its past. In other words, if the probability distribu-

tion over future sequences of symbols is independent of               system f x δ then it is clear that
the past symbols given the state of the ε-machine, then
the ε-machine uniquely labels the state of the dynamical             yt   ψ It0      It   ψ
                                                                                                       
                                                                                                                    Æ                    Æ           
                                                                                                         gxt0  g f xt0      g f t xt0  
system generating the symbols. The ε-machine can then
be taken as a model of the unseen symbol generating dy-               where f t is the composition of f t times. The state y of
namical system5 .                                                     the neural system is a function of the state x of the hidden
     On the basis of the embedding theorems and the ε-                dynamical system.
machine of Crutchfield & Young (1989), an objective                       The neural system ẏi on N is a diffeomorphism of the
function for state-space reconstruction may be intro-                 dynamical system ẋ on M , if the state y smoothly and
                                                                      uniquely labels the state x. If the future inputs to the
                     
duced. The objective is to model the dynamical system
 f x δ : M           M , and this can be defined as learning a    neural system are stochastically independent of the past
structure-preserving map from the manifold M to a sec-                inputs, given the state y of the system then N and M
ond topological model-space N . If the probability dis-               are diffeomorphically equivalent. If the probability of
tribution over sequences of symbols emitted by the dy-                the future inputs to the neural system, conditioned on its
namical system defined on M is independent of its past                state y, is not further sharpened by acquiring information
symbols given the state of the model-space N , then N                 about the previous inputs to the system then there is a
smoothly and uniquely labels the state of the dynamical               structure preserving map between the two systems.
system generating the symbols. The trajectory of states
on N can then be taken as a model of the unseen sym-                                        Network simulations
bol generating dynamical system N . This idea may be                  In this paper, it is taken that a language (or a sequence
illustrated by means of a neural system.                              of symbols) is produced by a continuous dynamical sys-
     A system of cortical neurons can be minimally mod-               tem. To learn this dynamical is to learn the statistical
eled by a set of n coupled nonlinear differential equa-               structure of the language. By hypothesis, this can be ac-
tions,                                                                complished by embedding the hidden dynamical system
                                                                      in a second model space. To maximize prediction of fu-
                                   j n
                            
                                                                      ture states given present ones is effectively to seek such
                       ẏi  yi  ∑ wi j σ yi   Ii                an embedding. As such, it should be the case that if a
                                   j 1                               recurrent neural network is trained on a corpus of natural
                                                                      language (in the now familiar style introduced initially
where σ is a smooth and monotonic transfer function,                  by Elman (1990)) it should develop a state space that is
yi is the soma potential of neuron i, resulting from a                a model of the generating process of the language. One
weighted sum of its inhibitory and excitatory inputs. I               manifestation of this would be that sentences, judged (by
is the external input to the system. Clearly, this system is          human observers) to be structurally similar, should also
a dynamical system defined on a n-dimensional manifold                be clustered in the state space of the neural system.
N . In addition, the state of this system yt at a given time             To explore this hypothesis further, a simulation of an
t is a function of both its present input It and, through             idealized neural system was performed by implementing
                               
the action of its recurrent synapses, the history of pre-             the system of coupled equations,
vious input, It0      It . In other words, the system’s
state at any given time is a smooth function of an entire                                 
                                                                                  ẏi  yi  ∑ wi j σ yi   θi  ∑ wik Ik 
sequence of inputs. This can be represented by the cor-                                              j                        k
respondence yt  ψ It0      It . If the sequence of inputs
                  
   It0      It represents the output It  gxt  of dynamical                                                    
      5 In a dynamical system, the entire evolution of the system                                Ol  σ      ∑ wli yi     
is described by its trajectory from t ∞ through t0 to t∞ . The                                                 i
future trajectories of the system are conditionally independent
of the past, given the present state of the system. In the ideal                                                       1
case of a deterministic and autonomous system, the future tra-                                                       ζ
jectory of the system, X t0  t∞ , can in principle be determined                              σ ζ  1  e              
from the present state of the system, X t0 . Absolute knowl-
edge of the system’s state X at t0 provides absolute knowledge        where yi is the state of the neuron and can be viewed as
of the future trajectory X t0  t∞ . No information about the sys-  representing its mean soma potential, θi is a bias term
tem’s prior trajectory X t ∞  t0  is necessary. In a stochastic
dynamical system (where, for example, at irregular points in          and Ii is external input. Ol is the output of the system
time there is coin toss of an k-sided coin to choose between k        which ”reads off” the recurrent network. There were 120
different set of differential equations), a similar situation oc-     neurons in the recurrent network. The input was a 250
curs. While the future is not entirely predictable on the basis       dimensional bit vector, described below. The output was
of the present state in this system, no increase in information
about the future is gained by knowing the past. In other words,       likewise a 250 dimensional vector. For the purposes of
the future trajectory of the system is stochastically independent     computer simulation, a difference equation was used,
                                                                                      ∆t y
of the past, given the state of the system. The case of a stochas-
tic system can be seen to generalize to the case of a dynamical        yti∆t  1             i  ∆t
                                                                                                 t
                                                                                                         ∑ wi j σ yi   ∆tθti  ∆t ∑ wik Ikt 
system driven by external input.                                                                          j                                k

   This was obtained by an approximation of its contin-        be to predict the same value for the future as is obtained
uous counterpart. ∆t was a variable parameter which            at the present. Thus, if the ratio is greater than 1.0 the
could be manipulated for finer approximations of the un-       network is performing worse than a chance model. At
derlying continuous system.                                    values less the 1.0, the network is performing better than
   The data-set used for network learning was a cor-           a chance model. A value approaching 0, would indicate
pus of natural language amounting to over 10 million           perfect predictive accuracy.
words. The corpus comprised 14,000 documents, the av-             On the final pass through the corpus, the mean perfor-
erage length of each document being approximately 700          mance ratio for the training data was .4767. Furthermore,
words. All documents were in a plain-text and untagged         a validation set which comprised 1000 unseen documents
format. They were obtained from publicly available elec-       was prepared. The mean performance ratio on this set
tronic text archives on the internet6 . No explicit crite-     was .4989. These values indicate substantial predictive
ria were used when selecting documents other than that         performance and generalization abilities by the network.
cover a wide range of subject matters such as science,         They compare very favorably to mean performance ra-
social science, literature, children’s stories, history, law   tios usually obtained in non-linear time series prediction
and politics.                                                  tasks (Weigend & Gershenfeld 1993).
   Altogether, the entire corpus contained a vocabulary
of 115,000 words. Of these, a set of 50,000 accounted          Discriminant function analysis
for over 99% of the total number of words in the cor-          If a neural network learns the statistical structure of the
pus. Only the members of this set were used for training       language , its state space should have topological orga-
the network, the infrequent words having been deleted.         nization based on a similarity principle. For example,
Each of these 50,000 words was coded by being ran-             sentences that are similar in content should cluster in
domly assigned to a unique bit vector of 247 zeros and         compact neighborhoods of the state space. An ideal ex-
3 ones (there are over 2.5 million possible combinations       perimental test of this would be to have reliable human
to choose from). While this random coding scheme in-           judges classify a large set of sentences on the basis of
troduced some spurious correlations between words, the         their content, and then to compare this with a network’s
average correlation between words was close to zero7 .         classification of the same set of sentences. To the extent
   The network was presented with the entire corpus as         that the network’s classifications are close to those of hu-
a sequence of words, one word at a time. The net-              man judges, the network would have met a behavioral
work was trained to predict its future word-input given        criterion for language comprehension.
its present word-input. The synaptic weight parame-               To adequately assess generalization abilities, a large
ters were adapted using the continuous version of back-        set of sentences would be required. Such an experiment
propagation through time due to Pearlmutter (1989). In         would be laborious to conduct. Fortunately, however,
this procedure, the minimum of the cross-entropy objec-        data-sets of labeled or categorized documents (rather
tive function was sought by calculating the derivatives        than sentences) are readily obtainable, as these are regu-
of this function with respect to each weight parameter at      larly used as benchmark tests of text categorization tech-
each time ”tick” ∆t of the 50 previous time steps.             niques. In the experiment conducted here, sentences
   With a learning rate parameter of .01, and a ∆t param-      were extracted from labeled documents. Sentences were
eter of .25, the network was trained for 46 passes through     then assigned to the semantic class of the document from
the corpus. At this time, the learning rate parameter was      which they came. For example, sentences taken from
annealed to .001, and the ∆t parameter was lowered to .1.      a document assigned to the class ’motorcycling’ would
Training was continued for another five passes through         themselves be assigned to the semantic class ’motorcy-
the entire corpus. The performance of the network at pre-      cling’. In this way, a large set of sentences could as-
dicting future words could be adequately assessed using        signed a plausible, although somewhat limited, interpre-
the a method of ratios between squared errors,                 tation. The data-sets were the Reuters-21578 newswire
                                                               data-set, the 20 newsgroups data-set8 , and then a third
                            ∑t dit   yti 2                   set which was compiled for the purpose of this experi-
                    Ri                                      ment from 6000 documents obtained from the Library of
                                          1 2
                          ∑t dit    dit                        Congress, which had been previously classified by their
                                                               Dewey Decimal categories
where dit is the target or to-be-predicted outcome for neu-       An appropriate test of the network’s representational
ron i at time t. The denominator of this ratio specifies the   capacities would be to assess the probability that a sen-
sum squared differences between the target outcome and         tence from a given semantic class would be assigned
the target at the previous step. This ratio is useful as the   correctly to that class. To do this a linear discriminant
best prediction a random-walk model can make would             function was used to divide the state space into (sim-
    6 The main sources of the electronic texts were, Project
                                                               ply connected and convex) sub-regions based on seman-
                                                               tic class. The discriminant function is a straightforward
Gutenberg, the Etext Archives, and archives.org.
    7 A more valid distributed code based the actual orthogra-     8 The two data sets are available on the inter-
phy of English words has been used by the author in previous   net.       See http://www.cs.cmu.edu/ textlearning and
simulations, but these will be reported here.                  http://www.research.att.com/ lewis

                                                                                   References
       Table 1: Accuracy of sentence classification.
                                                             Andrews, M. W. (2001), Language learning by state
                   Data Set       Accuracy                        space reconstruction. Manuscript in preparation.
                    Library         83%                      Bai-Lin, H. & Wei-Mou, Z. (1998), Applied Symbolic
                    Reuters         75%                           Dynamics and Chaos, World Scientific, Singapore.
                  Newsgroups        69%
                     Mean           76%                      Chomsky, N. (1963), Formal properties of grammars,
                                                                  in R. D. Luce, R. R. Bush & E. Galanter, eds,
                                                                  ‘Handbook of mathematical psychology’, Vol. 2,
                                                                  John Wiley and Sons, Inc., New York and London,
linear transformation of the state space, such that the           pp. 323–418.
centroids of ”training” sentences labeled by their class
are made maximally distant from one another. The net-        Crutchfield, J. P. & Young, K. (1989), ‘Inferring
work’s ability to categorize by semantic class can be as-         statistical complexity’, Physical Review Letters
sessed for a ”test” set of sentences by assessing the prob-       63(2), 105–108.
ability that a given sentence from a certain semantic class
would be correctly assigned to that class. The measure       Elman, J. L. (1990), ‘Finding structure in time’, Cogni-
                                                                  tive Science 14, 179–211.
used was Mahalanobis distance. This measure is approx-
imately proportional to an estimate of the posterior prob-   Nigam, K., Mccallum, A., Thrun, S. & Mitchell, T.
ability that a given sentence will correctly assigned to its      (2000), ‘Text classification from labeled and un-
appropriate class. 5000 sentences from each of the three          labeled documents using em’, Machine Learning
data-set were used in this test. The results are illustrated      39(2/3), 103–135.
in Table 1.
   These accuracy rates are suitably high, and in fact       Packard, N., Crutchfield, J. P., Farmer, J. & Shaw, R.
compare favorably to state-of-the-art text categorization         (1980), ‘Geometry from a time series’, Physical
methods which use similar or identical data-sets (Nigam,          Review Letters 45(9), 712–716.
Mccallum, Thrun & Mitchell 2000). It is reasonable to
conclude from this that the state space of a recurrent neu-  Pearlmutter, B. (1989), ‘Learning state space trajectories
ral network trained to predict word sequences becomes             in recurrent neural networks’, Neural Computation
organized on basis on semantic similarity. Sentences and          1, 263–269.
texts that are semantically similar are clustered into com-  Sauer, T., Yorke, J. A. & Casdagli, M. (1991), ‘Embedol-
pact neighborhoods which can be discriminated by a sim-           ogy’, Journal of Statistical Physics 65(3–4), 579–
ple linear function.                                              616.
                       Conclusion                            Stark, J., Broomhead, D. S., Davies, M. E. & Huke,
                                                                  J. (1997), ‘Takens embedding theorem for forced
Temporal pattern recognition is not as theoretically so-
                                                                  and stochastic systems’, Nonlinear Analysis, The-
phisticated as its multidimensional and static counter-
                                                                  ory, Methods and Applications 30(8), 5303–5314.
part. Here, an approach to temporal pattern learning is
introduced that is based on recent results from dynami-      Tabor, W. (1998), Dynamical automata, Technical report,
cal systems theory. It is proposed that the reconstruction        Department of Computer Science, Cornell Univer-
of system generating a language (or symbol sequence)              sity.
is adequate for learning the statistical structure of tem-
poral data. It is proposed that state-space reconstruction   Tabor, W. (2000), ‘Fractal encoding of context-free
can be carried out in a straightforward manner in a recur-        grammars in connectionist networks’, Expert Sys-
rent neural network. Results showing pattern recogni-             tems 17(1), 41–56.
tion of English sentences by the network are provided.
These results are similar in kind to those obtained by       Takens, F. (1981), Detecting strange attractors in turbu-
Elman (1990) and in the many works that followed this             lence, in D. Rand & L.-S. Young, eds, ‘Dynamical
paradigm. It is believed that the appropriate explanation         Systems and Turbulence’, Springer-Verlag, Berlin
of these now familar sets of results, is that the recurrent       and Heidelberg, pp. 366–381.
neural network has reconstructed the language generat-       Weigend, A. S. & Gershenfeld, N. A., eds (1993), Time
ing process. Sentences that were produced by similar              series prediction: Forecasting the future and under-
trajectories in the original systems are now modelled by          standing the past, Vol. 15, Addison-Wesley, Read-
similar trajectories in the recurrent neural network. It is       ing, MA 01867.
clear, however, that this is not a definitive demonstration
of state-space reconstruction and a more detailed analy-     Whitney, H. (1936), ‘Differentiable manifolds’, Annals
sis of temporal pattern learning using formal grammars            of Mathematics 37(3), 645–680.
is being currently undertaken (Andrews 2001).

