UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Modeling Cognition with Software Agents
Permalink
https://escholarship.org/uc/item/73m9m2pj
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 23(23)
Authors
Franklin, Stan
Graesser, Art
Publication Date
2001-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

                                Modeling Cognition with Software Agents
                                          Stan Franklin (franklin@memphis.edu)
                  Institute for Intelligent Systems, The University of Memphis, Memphis TN 38152, US
                                        Art Graesser (a-graesser@memphis.edu)
                  Institute for Intelligent Systems, The University of Memphis, Memphis TN 38152, US
                           Abstract                           heading of autonomous software agents (Franklin &
    We propose the use of autonomous software agents as       Graesser 1997). These agents are designed to implement a
   cognitive models that generate testable hypotheses         theory of cognition and attempt to automate practical
   about human cognition. While such agents are typically     tasks typically performed by humans. We have been
   produced to automate practical human tasks, they can
                                                              developing two such agents that implement global
   be designed within the constraints of a psychological
   theory. As an example we describe an agent designed        workspace theory Baars 1988), one with a relatively
   within global workspace theory that accommodates           simple clerical task (Zhang et al. 1998b) and the other
   several other theories as well. We discuss various         with a rather complex personnel assignment task
   resulting hypotheses, including a new interpretation of    (Franklin et al. 1998). These models do not merely
   the readiness potential data of Libet.                     produce output that solves a specific engineering problem,
                                                              as do typical software agents like web bots. They have
                      Introduction                            mechanisms that simulate human cognition and their
Computational models have long been a major, and              design decisions generate hopefully testable hypotheses
perhaps indispensable, tool in cognitive science.             (Franklin 1997), thus potentially providing research
Many of these model some psychological theory of a            direction for cognitive scientists and neuroscientists.
particular aspect of cognition, attempting to account             This paper briefly describes the architecture and
for experimental data. Others aspire to be a general          mechanisms of one such agent. In Table 1 we point out
computational model of cognition, such as the                 examples of relevant hypotheses that arise from our
construction-integration model (Kintsch 1998),                design decisions. It is beyond the scope of this article to
SOAR (Laird et al. 1987), and ACT-R (Anderson                 specify all of the hypotheses and associated support.
1990). Most of these computational models are
computer simulations of subjects in psychological                             Theoretical Frameworks
laboratories, and are capable of performing tasks at a        According to global workspace (GW) theory (Baars
fine-grain level of detail. The simulated data ideally        1988), one principal function of consciousness is to
fit the human data like a glove. The theories on              recruit the relevant resources needed for dealing with
which the simulations are based are periodically              novel or problematic situations. These resources may
revised so that new simulations conform more closely          include both knowledge and procedures. They are
to the data. The computational models are judged on           recruited internally, but partially driven by stimulus input.
how closely they predict the data. A model may also           GW theory postulates that human cognition is
be judged by the amount of change required in core,           implemented by a multitude of relatively small, special
as opposed to peripheral, parameters that are needed          purpose processes, almost always unconscious.
to fit the data. Alternatively, the models are evaluated      Communication between them is rare since they mostly
on a course-grain level, by observing whether a               communicate through working memory and over a
number of qualitative predictions (i.e., directional          narrow bandwidth. They are individually quite simple
predications, such as condition A > B) fit the data.          and incapable of dealing with complex messages.
And finally, all of the models have been evaluated by         Coalitions of such processes compete for access to a
observing how well they fit data in practical,                global workspace. This limited capacity workspace serves
everyday tasks in real-world environments. For                to broadcast the message of the coalition to all the
example, some such agents, based on SOAR,                     unconscious processors (bringing it to consciousness) in
simulate battlefield performers such as fighter pilots        order to recruit relevant processors to join in handling the
and tank commanders (Hirst & Kalus 1998). These               current novel situation, or in solving the current problem.
data fitting approaches to testing theories have been         Thus consciousness allows us to deal with novel or
hugely successful, and account for a large body of            problematic situations that cannot be dealt with
what is now known in cognitive science.                       efficiently, if at all, by automated unconscious processes.
     In this paper, we propose another class of               Consciousness recruits appropriately useful resources, and
computational models, which fall under the general            thereby manages to solve the relevance problem.

     An autonomous agent (Franklin & Graesser           these hypotheses are not directly addressed in this paper.
1997) is a system situated in, and part of, an          Others will be discussed in some detail.
environment, which senses that environment, and             IDA is intended to model a broad range of human
acts on it, over time, in pursuit of its own agenda. In cognitive function. Her architecture is comprised of
biological agents, this agenda arises from drives that  modules each devoted to a particular cognitive process.
evolve over generations; in artificial agents its       Table 2 lists most of these modules and gives pointers to
designer builds in the drives. Such drives, which act   the sources of their computational mechanisms, and to the
as motive generators (Sloman 1987), must be present,    psychological theories they support.
whether explicitly represented or derived from the          The processors postulated by GW theory are
processing trajectory. The agent also acts in such a    implemented by codelets, small pieces of code, each an
way as to possibly influence what it senses at a later  independent thread. These are specialized for some simple
time. In other words, it is structurally coupled to its task and often play the role of demons waiting for
environment (Maturana 1975). Examples include           appropriate conditions under which to act. From a
humans, most animals some mobile robots, and            biological point of view, these codelets may well
various computational agents, including artificial life correspond to Edelman’s neuronal groups (1987).
agents, software agents and many computer viruses.          Perception in IDA consists mostly of processing
Here we are immediately concerned with                  incoming email messages in natural language. In
autonomous software agents, designed for specific       sufficiently narrow domains, natural language
tasks, and ‘living’ in real world computing systems     understanding may be achieved via an analysis of surface
such as operating systems, databases, or networks.      features called complex, template-based matching (Allen
     A “conscious” software agent is one that           1995, Jurafsky & Martin 2000). Ida’s relatively limited
implements GW theory. In addition to modeling this      domain requires her to deal with only a few dozen or so
theory (Franklin & Graesser 1999), such “conscious”     distinct message types, each with relatively predictable
software agents should be capable of more adaptive,     content. This allows for surface level natural language
more human-like operations, including being capable     processing. Her language-processing module has been
of creative problem solving in the face of novel and    implemented as a Copycat-like architecture (Hofstadter &
unexpected situations. However, there is no claim       Mitchell 1994) with codelets that are triggered by surface
that the agent is a sentient being. What, if anything,  features. The mechanism includes a slipnet that stores
the agent truly feels or what the conscious experience  domain knowledge, a pool of codelets (processors)
actually is are not the relevant concerns.              specialized for recognizing particular pieces of text, and
     IDA (Intelligent Distribution Agent) is a          production templates for building and verifying
“conscious” software agent being developed for the      understanding. Together they allow her to recognize,
US Navy (Franklin et al. 1998). At the end of each      categorize and understand. IDA must also perceive
sailor's tour of duty, the sailor is assigned to a new  content read from databases, a much easier task. An
billet.. The Navy employs some 280 people, called       underlying hypothesis motivating our design decisions
detailers, to effect these new assignments. IDA's task  about perception appears in Table 1.
is to completely automate the role of detailer. IDA         Suppose, for example, that IDA receives a message
must communicate with sailors via email in natural      from a sailor saying that his projected rotation date (PRD)
language, by understanding the content and              is approaching and asking that a job be found for him.
producing life-like responses. Sometimes she will       The perception module would recognize the sailor’s name
initiate conversations. She must access several         and social security number, and that the message is of the
databases, again understanding the content. She must    please-find-job type. This information would then be
adher to some ninety Navy policies. She must hold       written to working memory. The hypothesis here is that
down moving costs, but also cater to the needs and      the contents of perception are written to working memory
desires of the sailor. This includes negotiating with   before becoming conscious. IDA employs sparse
the sailor and eventually writing the orders. A partial distributed memory (SDM) as her major associative
prototype of IDA with most of the functionality         memory (Kanerva 1988). SDM is a content addressable
described is now up and running. It should be           memory that, in many ways, is an ideal computational
complete before the beginning of the year.              mechanism for use as a long-term associative memory
                                                        (LTM). Any item written to working memory cues a
         Architecture and Mechanisms                    retrieval from LTM, returning prior activity associated
                                                        with the current entry. In our example, LTM will be
Table 1 specifies several of the underlying
                                                        accessed as soon as the message information reaches the
hypotheses that guided the design of IDA Many of
                                                        workspace, and the retrieved associations will be also
                                                        written to the workspace.

                                          Table 1. Hypotheses from Design Decisions
  Module                   Hypotheses from Design Decisions
  Perception               Much of human language understanding employs a combined bottom up/top down passing of
                           activation through a hierarchical conceptual net, with the most abstract concepts in the middle.
  Working Memory           The contents of perception are written to working memory before becoming conscious.
  Long-term Memory         Part, but not all, of working memory, the focus, is set aside as an interface with long-term associative
                           memory (LTM). Reads from LTM are made with cues taken from the focus and the resulting
                           associations are written there. Writes to LTM are also made from the focus.
  Consciousness            Human consciousness must have a mechanism for gathering processors (neuronal groups) into
                           coalitions, another for conducting the competition, and yet another for broadcasting
  Motivation               The hierarchy of goal contexts is fueled at the top by drives, that is by primitive motivators, and at the
                           bottom by input from the environment, both external and internal
  Goal Contexts            In humans, processors (neuronal groups) bring perceptions and thoughts to consciousness. Other
                           processors, aware of the contents of consciousness, instantiate an appropriate goal context hierarchy,
                           which motivates yet other processors to perform internal or external actions.
  Emotions                 Action selection will be influenced by emotions via their effect on drives. Emotions also influence
                           attention and the strength with which items are stored in associative memory.
  Voluntary Action         Voluntary action in humans is controlled by a timekeeper who becomes less patient as the time for a
                           decision increases. Each time a proposal or objection reaches consciousness, its chance of becoming
                           conscious again diminishes.
  Language Production      Much of human language production results from filling in blanks in scripts, and concatenating the
                           results.
     At a given moment IDA’s workspace may                              number of codelets that carry the information
contain, ready for use, a current entry from                            describing the situation. (In the example of our
perception or elsewhere, prior entries in various                       message, these codelets would carry the sailor’s
states of decay, and associations instigated by the                     name, his or her social security number, and the
current entry, i.e. activated elements of LTM.. IDA’s                   message type.) This association should lead to these
workspace thus consists of both short-term working                      information codelets, together with the attention
memory (STM) and something very similar to the                          codelet that collected them, becoming a coalition.
long-term working memory (LT-WM) of Ericsson                            Codelets also have activations measuring their
and Kintsch (1995).                                                     current relevance. The attention codelet increases its
     Since most of IDA’s cognition deals with                           activation in order that the coalition might compete
performing routine tasks with novel content, most of                    for the spotlight of “consciousness”. Upon winning
her workspace is structured into registers for                          the competition, the contents of the coalition is then
particular kinds of data. Part of the workspace, the                    broadcast to all codelets. This leads us to the
focus, is set aside as an interface with long-term                      consciousness hypothesis in Table 1.
LTM. Retrievals from LTM are made with cues taken                           Baars addresses the question of how content
from the focus and the resulting associations are                       arrives in consciousness (1988, pp. 98-99), offering
written to other registers in the focus. The contents of                two possible high-level mechanisms both consistent
still other registers in the focus are stored in (written               with neurophysiological timing findings. He also
to) associative memory. All this leads to the                           devotes an entire chapter (1988 Chapter 3) to
perception hypothesis in Table 1.                                       neurophysiological evidence consistent with the basic
     Not all of the contents of the workspace                           concept of a global workspace. Yet no mechanisms
eventually make their way into consciousness. The                       are proposed for the three distinct processes
apparatus for “consciousness” consists of a coalition                   identified as being needed in our hypothesis above.
manager, a spotlight controller, a broadcast manager,                   Here we have a good example of engineering, as well
and a collection of attention codelets who recognize                    as psychological, considerations giving direction to
novel or problematic situations (Bogner et al. 2000).                   neurophysiological research.
     Each attention codelet keeps a watchful eye out                        Summarizing our example, an attention codelet
for some particular situation to occur that might call                  will note the please-find-job message type, gather
for “conscious” intervention. In most cases the                         information codelets carrying name, ssn and message
attention codelet is watching the workspace, which                      type, be formed into a coalition, and will compete for
will likely contain both perceptual information and                     consciousness. If or when successful, its contents will
data created internally, the products of “thoughts.”                    be broadcast.
Upon encountering such a situation, the appropriate                         IDA depends on a behavior net (Maes 1989) for
attention codelet will be associated with the small                     high-level action selection in the service of built-in

drives. She has several distinct drives operating in                  codelets for their execution. A behavior net is
parallel that vary in urgency as time passes and the                  composed of behaviors, corresponding to goal
environment changes. Behaviors are typically mid-                     contexts in GW theory, and their various links. A
level actions, many depending on several behavior                     behavior looks very much like a production rule,
                     Table 2. IDA’s Modules and Mechanisms and the Theories they Accommodate
Module                   Computational Mechanism motivated by                   Theories Accommodated
Perception               Copycat architecture (Hofstadter & Mitchell 1994)      Perceptual Symbol System (Barsalou 1999)
Working Memory           Sparse Distributed Memory (Kanerva 1988)               Long-term Working Memory (Erricsson & Kintsch 1995)
Emotions                 Neural Networks (McCellland & Rumelhart 1986)          (Damasio 1999, Rolls 1999)
Associative Memory       Sparse Distributed Memory (Kanerva 1988)
Consciousness            Pandemonium Theory (Jackson 1987)                      Global Workspace Theory (Baars 1988)
Action Selection         Behavior Nets (Maes 1989)                              Global Workspace Theory (Baars 1988)
Constraint               Linear Functional (standard operations research)
Satisfaction
Deliberation             Pandemonium Theory (Jackson 1987)                      Human-Like Agent Architecture (Sloman 1999)
Voluntary Action         Pandemonium Theory (Jackson 1987)                      Ideomotor Theory (James 1890)
Language Generation      Pandemonium Theory (Jackson 1987)
Metacognition            Fuzzy Classifers                                       Human-Like Agent Architecture (Sloman 1999)
having preconditions as well as additions and                      long as it’s the most important activity going, this
deletions. It typically requires the efforts of several            process is continued until all the relevant personnel data
codelets to effect its action.                                     is written to the workspace. In a similar fashion,
    . Each behavior occupies a node in a digraph. As in            repeated runs through “consciousness” and the behavior
connectionist models (McClelland et al. 1986), this                net result in a course selection of possible suitable jobs
digraph spreads activation. The activation comes from              being made from the job requisition database.
that stored in the behaviors themselves, from the                       The process just described leads us to speculate that
environment, from drives, and from internal states.                in humans, like in IDA, processors (neuronal groups)
More relevant behaviors receive more activation from               bring perceptions and thoughts to consciousness. Other
the environment. Each drive awards activation to those             processors, aware of the contents of consciousness,
behaviors that will satisfy it. Certain internal states of         instantiate an appropriate goal context hierarchy, which
the agent can also activate behaviors. One example                 in turn, motivates yet other processors to perform
might be activation from a coalition of codelets                   internal or external actions.
responding to a “conscious” broadcast. Activation                       IDA is provided with a constraint satisfaction
spreads from behavior to behavior along both                       module designed around a linear functional. It provides
excitatory and inhibitory links and a behavior is                  a numerical measure of the suitability, or fitness, of a
chosen to execute based on activation. IDA’s behavior              specific job for a given sailor. For each issue (say
net produces flexible, tunable action selection. This              moving costs) or policy (say sea duty following shore
hierarchy of goal contexts is fueled at the top by                 duty) there’s a function that measures suitability in that
drives, that is, by primitive motivators, and at the               respect. Coefficients indicate the relative importance of
bottom by input from the environment, both external                each issue or policy. The weighted sum measures the
and internal.                                                      job’s fitness for this sailor at this time. The same
    Returning to our example, the broadcast is received            process, beginning with an attention codelet and ending
by appropriate behavior-priming codelets who know to               with behavior codelets, brings each function value to
instantiate a behavior stream for reading the sailor’s             “consciousness” and writes the next into the workspace.
personnel record. They also bind appropriate variables             At last, the job’s fitness value is written to the
with name and ssn, and send activation to a behavior               workspace.
that knows how to access the database. When that                        Since IDA’s domain is fairly complex, she requires
behavior is executed, behavior codelets associated                 deliberation in the sense of creating possible scenarios,
with it begin to read data from the sailor’s file into. the        partial plans of actions, and choosing between them
workspace. Each such write results in another round of             (Sloman 1999). In our example, IDA now has a list of a
associations, the triggering of an attention codelet, the          number of possible jobs in her workspace, together with
resulting information coming to “consciousness,”                   their fitness values. She must construct a temporal
additional binding of variables and passing of                     scenario for at least a few of these possible billets to see
activation, and the execution of the next behavior. As             if the timing will work out (say if the sailor can be

aboard ship before the departure date). In each           being one to be offered. If an objection or a new
scenario the sailor leaves his or her current post during proposal is made in a timely fashion, it will not do so.
a prescribed time interval, spends a specified length of      Two proposing attention codelets may alternatively
time on leave, possibly reports to a training facility on propose the same two jobs several times. Several
a certain date, uses travel time, and arrives at the new  mechanisms tend to prevent continuing oscillation.
billet with in a specified time frame. Such scenarios     Each time a codelet proposes the same job it does so
are valued on how well they fit the temporal              with less activation and, so, has less chance of coming
constraints (the gap) and on moving and training costs.   to “consciousness.” Also, the timekeeper loses patience
These scenarios are composed of scenes organized          as the process continues, thereby diminishing the time
around events, and are constructed in the workspace       span required for a decision. A job proposal may also
by the same process of attention codelet to               alternate with an objection, rather than with another
“consciousness” to behavior net to behavior codelets      proposal, with the same kinds of consequences. These
as described previously.                                  occurrences may also be interspersed with the creation
    We humans most often select actions                   of new scenarios. If a job is proposed but objected to,
subconsciously, but we also make voluntary choices of     and no other is proposed, the scenario building may be
action, often as a result of the kind of deliberation     expected to continue yielding the possibility of finding
described above. Baars argues that such voluntary         a job that can be agreed upon.
choice is the same a conscious choice (1997, p. 131).         Experimental work of neuroscientist Benjamin
We must carefully distinguish between being               Libet lends support to this implementation of voluntary
conscious of the results of an action and consciously     action as mirroring what happens in humans (Libet et
deciding to take that action, that is, of consciously     al. 1983). He writes, “Freely voluntary acts are
deliberating on the decision. The latter case constitutes preceded by a specific electrical change in the brain
voluntary action. William James proposed the              (the 'readiness potential', RP) that begins 550 ms before
ideomotor theory of voluntary action (James 1890).        the act. Human subjects became aware of intention to
James suggests that any idea (internal proposal) for an   act 350-400 ms after RP starts, but 200 ms. before the
action that comes to mind (to consciousness) is acted     motor act. The volitional process is therefore initiated
upon unless it provokes some opposing idea or some        unconsciously. But the conscious function could still
counter proposal. GW theory adopts James’ ideomotor       control the outcome; it can veto the act.” Libet
theory as is (1988, Chapter 7), and provides a            interprets the onset of the readiness potential as the time
functional architecture for it. The IDA model furnishes   of the decision to act. Suppose we interpret it, instead,
an underlying mechanism that implements that theory       as the time a neuronal group (attention codelet) decides
of volition and its architecture in a software agent.     to propose the action (job). The next 350-400 ms would
    Suppose that in our example at least one scenario     be the time required for the neuronal group (attention
has been successfully constructed in the workspace.       codelet) to gather its information (information codelets)
The players in this decision making process include       and win the competition for consciousness. The next
several proposing attention codelets and a timekeeper     200 ms would be the time during which another
codelet. A proposing attention codelet’s task is to       neuronal group (timekeeper) would wait for objections
propose that a certain job be offered to the sailor.      or alternative proposals from some third neuronal group
Choosing a job to propose on the basis of the codelet’s   (attention codelet) before initiating the action. This
particular pattern of preferences, it brings information  scenario gets the sequence right, but begs the question
about itself and the proposed job to “consciousness”      of the timing. Why should it take 350 ms for the first
so that the timekeeper codelet can know of it. Its        neuronal group (attention codelet) to reach
preference pattern may include several different issues   consciousness and only 200 ms for the next? Our model
(say priority, moving cost, gap, etc) with differing      would require such extra time during the first pass to set
weights assigned to each. For example, our proposing      up the appropriate goal context hierarchy (behavior
attention codelet may place great weight on low           stream) for the voluntary decision making process, but
moving cost, some weight on fitness value, and little     would not require it again during the second. The
weight on the others. This codelet may propose the        problem with this explanation is that we identify the
second job on the scenario list because of its low cost   moment of “consciousness” with the broadcast, which
and high fitness, in spite of low priority and a sizable  occurs before instantiation of the behavior stream. So
gap. If no other proposing attention codelet objects (by  the relevant question is whether consciousness occurs
bringing itself to “consciousness” with an objecting      in humans only after a responding goal structure is in
message) and no other such codelet proposes a             place? This leads us to the voluntary action hypothesis
different job within a prescribed span of time, the       in Table 1.
timekeeper codelet will mark the proposed job as

                      Future Work                         Franklin, S., and A. C. Graesser. 1997. Is it an Agent,
                                                            or just a Program?: A Taxonomy for Autonomous
Though the IDA model cuts a broad swath, human
                                                            Agents. In Intelligent Agents III. Berlin: Springer.
cognition is far too rich to be easily encompassed.
                                                          Franklin, S., and A. Graesser. 1999. A Software Agent
Still, we plan to extend the model in several ways. An
                                                            Model of Consciousness. Consciousness and
alteration to the behavior net will allow automation of
                                                            Cognition 8:285–305.
actions. A capacity for learning from conversations
                                                          Franklin, S., A. Kelemen, and L. McCauley. 1998.
with detailers is planned (Ramamurthy et al. 1998). A
                                                            IDA: A Cognitive Agent Architecture. In IEEE Conf
development/training period utilizing that ability is
                                                            on Systems, Man and Cybernetics. : IEEE Press.
also anticipated for IDA (Franklin 2000). We’re also
                                                          Hirst, T., and T. Kalus; 1998. Soar Agents for OOTW
working on giving her the ability to report “conscious”
                                                            Mission Simulation. 4th Int’l Command and Control
activity in natural language. Though IDA deals
                                                            Research and Technology Symposium. September.
intelligently with novel instances of routine situations,
                                                          Hofstadter, D. R., and M. Mitchell. 1994. The Copycat
she should be able to also handle unexpected, and
                                                            Project. In Advances in connectionist and neural
problematic non-routine situations. We’re working on
                                                            computation theory, Vol. 2: logical connections, ed.
it. In modeling human cognition, there’s always much
                                                            K. J. Holyoak, & J. A. Barnden. Norwood N.J.:
left to do.
                                                            Ablex.
                                                          James, W. 1890. The Principles of Psychology.
                  Acknowledgments                           Cambridge, MA: Harvard University Press.
The first author was supported in part by ONR grant       Jurafsky, D., and J. H. Martin. 2000. Speech and
N00014-98-1-0332. The authors wish to acknowledge           Language Processing. Englewood Cliffs, NJ:
essential contributions from the Conscious Software         Prentice-Hall.
Research Group: Ashraf Anwar, Ramesh Aitipamula , Arpad   Kanerva, P. 1988. Sparse Distributed Memory.
Kelemen, Ravikumar Kondadadi, Irina Makkaveeva , Lee
McCauley, Aregahegn Negatu, Uma Ramamurthy, Alexei
                                                            Cambridge MA: The MIT Press.
Stoliartchouk, and Zhaohua Zhang.                         Kintsch, W. 1998. Comprehension. Cambridge:
                                                            Cambridge University Press.
                                                          Laird, E. J., A. Newell, and Rosenbloom P. S. 1987.
                        References                          SOAR: An Architecture for General Intelligence.
Allen, J. J. 1995. Natural Language Understanding.          Artificial Intelligence 33:1–64.
   Redwood City CA: Benjamin/Cummings.                    Libet, B., C. A. Gleason, E. W. Wright, and
Anderson, J. R. 1990. The Adaptive Character of             D. K. Pearl. 1983. Time of Conscious Intention to
   Thought. Hillsdale, NJ: Erlbaum.                         Act in Relation to Onset of Cerebral Activity
Baars, B. J. 1988. A Cognitive Theory of                    (Readiness-Potential). Brain 106:623–642.
   Consciousness. Cambridge: Cambridge University         Maes, P. 1989. How to do the right thing. Connection
   Press.                                                   Science 1:291–323.
Barsalou, L. W. 1999. Perceptual symbol systems.          Maturana, H. R. 1975. The Organization of the Living.
   Behavioral and Brain Sciences 22:577–609.                International Journal of Man-Machine Studies
Bogner, M., U. Ramamurthy, and S. Franklin. 2000.           7:313–332.
   “Consciousness" and Conceptual Learning in a           McClelland, J. L., et al. 1986. Parallel Distributed
   Socially Situated Agent. In Human Cognition and          Processing, vol. 1. Cambridge: MIT Press.
   Social Agent Technology, ed. K. Dautenhahn.            Ramamurthy, U., S. Franklin, and A. Negatu. 1998.
   Amsterdam: John Benjamins.                               Learning Concepts in Software Agents. In From
Damasio, A. R. 1994. Descartes' Error. New York:            animals to animats 5, ed. R. Pfeifer, et al
   Gosset; Putnam Press.                                    Cambridge,Mass: MIT Press.
Edelman, G. M. 1987. Neural Darwinism. New York:          Sloman, A. 1987. Motives Mechanisms Emotions.
   Basic Books.                                             Cognition and Emotion 1:217–234.
Ericsson, K. A., and W. Kintsch. 1995. Long-term          Sloman, A. 1999. What Sort of Architecture is
   working memory. Psychological Review                     Required for a Human-like Agent? In Foundations of
   102:21–245.                                              Rational Agency, ed. M. Wooldridge, and A. Rao.
Franklin, S. 1997. Autonomous Agents as Embodied            Dordrecht, Netherlands: Kluwer Academic
   AI. Cybernetics and Systems 28:499–520.                  Publishers.
Franklin, S. 2000. Learning in "Conscious" Software       Zhang, Z., S. Franklin, B. Olde, Y. Wan, and
   Agents. In Workshop on Development and Learning.         A. Graesser. 1998b. Natural Language Sensing for
   Michigan State U.; East Lansing, USA: April 5-7.         Autonomous Agents. In Proceedings of IEEE
                                                            International Joint Symposia on Intellgence Systems
                                                            98.

