UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Rules for Syntax, Vectors for Semantics
Permalink
https://escholarship.org/uc/item/057457h4
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 23(23)
Authors
Wiemer-Hastings, Peter
Zipitria, Iraide
Publication Date
2001-01-01
Peer reviewed
 eScholarship.org                              Powered by the California Digital Library
                                                                University of California

                                      Rules for Syntax, Vectors for Semantics
                                    Peter Wiemer-Hastings (Peter.Wiemer-Hastings@ed.ac.uk)
                                               Iraide Zipitria (iraidez@cogsci.ed.ac.uk)
                                                           University of Edinburgh
                                                           Division of Informatics
                                                              2 Buccleuch Place
                                                       Edinburgh EH8 9LW Scotland
                               Abstract                                  for 200-word essay segments, LSA accounted for 60% of
                                                                         the variance in human scores. For 60-word essay segments,
   Latent Semantic Analysis (LSA) has been shown to perform              LSA scores accounted for only 10% of the variance.
   many linguistic tasks as well as humans do, and has been put             In work on judging the quality of single-sentence student
   forward as a model of human linguistic competence. But
   LSA pays no attention to word order, much less sentence               answers in an intelligent tutoring context, we have shown
   structure. Researchers in Natural Language Processing have            in previous work that although LSA nears the performance
   made significant progress in quickly and accurately deriv-            of intermediate-knowledge human raters, it lags far behind
   ing the syntactic structure of texts. But there is little agree-      expert performance (Wiemer-Hastings, Wiemer-Hastings,
   ment on how best to represent meaning, and the representa-            & Graesser, 1999b). Furthermore, when we compared
   tions are brittle and difficult to build. This paper evaluates a
   model of language understanding that combines information             LSA to a keyword-based approach, LSA performed only
   from rule-based syntactic processing with a vector-based se-          marginally better (Wiemer-Hastings, Wiemer-Hastings, &
   mantic representation which is learned from a corpus. The             Graesser, 1999a). This accords with unpublished results on
   model is evaluated as a cognitive model, and as a potential           short answer sentences from Walter Kintsch, personal com-
   technique for natural language understanding.
                                                                         munication, January 1999.
                                                                            In the field of Natural Language Processing, the eras of
                          Motivations                                    excessive optimism and ensuing disappointment have been
                                                                         followed by study increases in the systems’ ability to pro-
Latent Semantic Analysis (LSA) was originally developed                  cess the syntactic structure of texts with rule-based mecha-
for the task of information retrieval, selecting a text which            nisms. The biggest recent developments have been due to
matches a query from a large database (Deerwester, Du-                   the augmentation of the rules with corpus-derived proba-
mais, Furnas, Landauer, & Harshman, 1990)1 . More re-                    bilities for when they should be applied (Charniak, 1997;
cently, LSA has been evaluated by psychologists as a model               Collins, 1996, 1998, for example).
for human lexical acquisition (Landauer & Dumais, 1997).                    Unfortunately, progress in the area of computing the se-
It has been applied to other textual tasks and found to gen-             mantic content of texts has not been so successful. Two ba-
erally perform at levels matching human performance. All                 sic variants of semantic theories have been developed. One
this despite the fact that LSA pays no attention to word or-             is based on some form of logic. The other is represented
der, let alone syntax. This led Landauer to claim that syntax            by connections within semantic networks. In fact, the latter
apparently has no contribution to the meaning of a sentence,             can be simply converted into a logic-based representation.
and may only serve as a working memory crutch for sen-                      Such theories are brittle in two ways. First, they require
tence processing, or in a stylistic role (Landauer, Laham,               every concept and every connection between concepts to
Rehder, & Schreiner, 1997).                                              be defined by a human knowledge engineer. Multi-purpose
   The tasks that LSA has been shown to perform well on                  representations are not feasible because of the many techni-
can be separated into two groups: those that deal with sin-              cal senses of words in every different domain. Second, such
gle words and those that deal with longer texts. For exam-               representations can not naturally make the graded judge-
ple, on the synonym selection part of the TOEFL (Test of                 ments that humans do. Humans can compare any two
English as a Foreign Language), LSA was as accurate at                   things (even apples and oranges!), but aside from count-
choosing the correct synonym (out of 4 choices) as were                  ing feature overlap, logic-based representations have diffi-
successful foreign applicants to US universities (Landauer               culty with relationships other than subsumption and “has-
et al., 1997). For longer texts, Rehder et al (1998) showed              as-part”.
that for evaluating author knowledge, LSA does steadily                     Due to these various motivations, we are pursuing a two-
worse for texts shorter than 200 words. More specifically,               pronged research project. First, we want to evaluate the
    1 We do not describe the functioning of the LSA mechanism            combination of a syntactic processing mechanism with an
here. For a complete description, see (Deerwester et al., 1990;          LSA-based semantic representation as a cognitive model
Landauer & Dumais, 1997)                                                 of human sentence similarity judgements. Second, we are

implementing a computational system to automate the pro-        Although these numbers are relatively low for inter-rater
cessing of texts with this technique. This paper describes   reliability on similarity tasks in general (Tversky, 1977;
the human data we collected for the cognitive modeling as-   Goldstone, Medin, & Halberstadt, 1997; Resnik & Diab,
pect, the evaluation of our approach with respect to that    2000, for example), we have found this level of agreement
data, and our initial attempts to implement the computa-     in our other studies of sentence similarity. This task is ob-
tional system.                                               viously a difficult one for humans. In future work, we will
                                                             study the effects of varying the level of context that is avail-
                      Data collection                        able for making these decisions.
In (Wiemer-Hastings, 2000), we reported our initial at-
tempts in this direction. In that evaluation, we compared             Experiment 1: Part-of-speech tags
our technique (described more fully below) to human rat-     One way of deriving structural knowledge from text is by
ings that were previously collected as part of the AutoTutor performing part of speech tagging. This is one area in
project (Wiemer-Hastings, Graesser, Harter, & the Tutor-     which NLP systems have been fairly successful. Brill’s tag-
ing Research Group, 1998). To our surprise, we found that    ger (Brill, 1994) is trained on a marked corpus and uses
adding syntactic information actually hurt the performance   rules to assign a unique tag to each word. It first assigns
of an LSA-based approach. This could have been due to        the most common tag for each word, then applies a set
some problem with the approach, or due to some difficulty    of automatically-derived context-based rules to modify the
with the human data. The previous ratings had been based     original tagging.
on complete multi-sentence student answers and ideal good       When LSA is trained, it does not distinguish between
answers. The raters were asked to indicate what percentage   words which are used in multiple parts of speech. This may
of the content of the student answer matched some part of    have significant semantic ramifications. The word “plane”,
the ideal answer. In the current work, we are looking at     for example, has very different senses as a verb and as a
similarity ratings for specific sentences. Thus the previous noun. One way to add structural information to LSA would
data was not appropriate for our current goals.              be to allow it to distinguish the part of speech for each word
   To get more relevant human data, we started with text     when training and comparing sentences.
from the AutoTutor Computer Literacy tutoring domain so
that we could more directly compare the results with our     Approach
previous results, and because we had already trained an      Our approach to this task was to use the Brill tagger to as-
LSA space for it. AutoTutor “understands” student answers    sign a part-of-speech tag to every word in the training cor-
by comparing them to a set of target good answers with       pus and every word in the test set (which had been given to
LSA. For this evaluation, we randomly paired 300 student     the human raters). The tag for each word was attached to it
answer sentences with 300 target good answers from the       with an underscore so that LSA would view each word/tag
relevant questions. We split these into four booklets of 75  combination as a single term. For example:
pairs, and gave each booklet to four human raters. Because
                                                                ROM NNP is VBZ information NN the DT
we are also interested in the effect of knowledge on the re-
                                                                computer NN was VBD programmed VBN with IN
liability of ratings, we had previously asked the raters if
                                                                when WRB it PRP was VBD built VBN . .
they were proficient or not with computers. We gave each
booklet to two proficient and two non-proficient raters.        The original training corpus was 2.3 MB of text taken
   We told the raters that the sentence pairs were from the  from textbooks and articles on computer literacy. We
domain of computer literacy, and asked them to indicate      trained LSA on the tagged corpus at 100, 200, 300, and 400
how similar the items were on a 6-point scale, from com-     dimensions because these dimensions had shown reason-
pletely dissimilar to completely similar. Here is an example able levels of performance in previous evaluations. Then
item:                                                        we evaluated this approach by using the new LSA space to
                                                             calculate the cosines between the tagged versions of the test
   Sentence 1: ROM only reads information from the           sentences that had been given to the human raters. We cal-
   disk.                                                     culated the correlations between the cosines and the human
   Sentence 2: The central processing unit, CPU, can         ratings.
   read from RAM.
                                                             Results
We did not specify how the raters were to decide what sim-
ilarity means.                                               Figure 1 shows the correlations between the different LSA
   The averaged correlations between the human raters        models and the human ratings. The first bar depicts the cor-
were:                                                        relation using the standard LSA space (at 200 dimensions)
                                                             as applied to the untagged versions of the sentences.
   Non-Proficient: r = 0.35, P < 0.001
   Skilled: r = 0.45, P < 0.001                              Discussion
   Mean Non-Proficient with Mean Proficient: r = 0.53,       It is clear that the performance of the tagged models do
   P < 0.001                                                 not match human judgements as closely as the standard

                                                                    improve sentence similarity judgements.
                  Standard vs Tagged LSA
      0.4                                                           The approach: Structured LSA
     0.35                                                           In standard LSA, the input to the procedure is an entire text,
                                                                    represented as a string. The string is then tokenized into
      0.3
                                                                    words, and the vector for each word is accessed from the
     0.25
                                                         LSA200     trained vector space. LSA ignores words that it can not
                                                         tagLSA100
                                                         tagLSA200
                                                                    find, i.e. those that did not appear in more than one doc-
      0.2                                                tagLSA300  ument in the training corpus, or those that appear on the
                                                         tagLSA400  stop-word list, a list of 440 very common words, including
     0.15
                                                                    most function words. The vector for a text is constructed
      0.1                                                           by simply adding together the relevant word vectors. Two
                                                                    texts are compared by calculating the cosine between their
     0.05
                                                                    vectors.
        0                                                              In our approach which we call Structured LSA (SLSA),
                     Correlation with human ratings                 we preprocess input sentences to derive aspects of their
                                                                    structure. More specifically, for each sentence, we:
             Figure 1: The performance of tagged LSA
                                                                    • resolve pronominal anaphora, replacing pronouns with
                                                                       their antecedents,
LSA approach does. It is not clear why this is. This could          • break down complex sentences into simple sentences,
support Landauer’s claim with respect to the irrelevance of
structural information for determining meaning. Perhaps             • segment the simple sentences into subject, verb, and ob-
LSA somehow does manage to account for different senses                ject substrings.2
and uses of a word even though it does not have explicit
knowledge of the syntactic context of the word’s use.               .
     Alternatively, the relatively poor performance could be           For example, we transform the student answer: “RAM
due to some inadequacies of this particular approach. For           stores things being worked with, and it is volatile” into:
example, although the number of dimensions was in the
correct relative range with respect to non-tagged LSA pro-             (“stores” “RAM” “things being worked with”)
cessing, perhaps tagged LSA works better with more (or                 (“volatile” “RAM”)
fewer?) dimensions. It could also be that the perfor-
mance was hampered by mistagging of key words in the                This yields a verb string, subject string, and (optional) ob-
sentence. Because the Brill tagger is trained on the Wall           ject string for each sentence. Note that for copular sen-
Street Journal corpus, its tagging rules often lead it astray       tences as in the second simple sentence above, the “verb
when processing the colloquial and domain-specific student          string” is the description following the verb. Also note that
answers in our tutoring domain. For example, one student            our human data was collected not on the original sentences,
answered a question about a computer’s memory like this,            but on sentences on which the first two steps above were
“RAM stores things being worked with.” The Brill tagger             already completed.
mistagged the word “stores” as a plural noun, thus greatly
altering the overall meaning of the sentence.
                                                                    SLSA similarity rating
                                                                    To calculate a similarity score between two sentences with
               Experiment 2: Surface parsing                        the SLSA approach, the preprocessing is performed on the
                                                                    sentences. Then we separately pass the verb strings, sub-
Another obvious potential contribution of sentence struc-           ject strings, and object strings to LSA which computes the
ture to meaning is by providing information about the rela-         cosines between them. Then we average the three together
tionships and actions of the participants: the “who did what        to get an overall similarity rating between the sentences.3
to whom” information. Although some might claim that                   Note that this approach provides more information than
LSA is able to derive this information from its training cor-       the standard LSA approach. For each pair of sentences,
pus (because men rarely bite dogs, for example), this can           there are four separate similarity ratings instead of just one.
not always be the case. And with the exception of case-
                                                                        2 Passive sentences were normalized, putting the syntactic ob-
marked pronouns like “I” and constructions like “there is
. . . ”, it is difficult to think of any entity references that can ject as the subject, and vice versa.
                                                                        3 In (Wiemer-Hastings, 2000), we evaluated three different
not appear as both subject and object of a sentence. Thus,
                                                                    methods for aggregating the segment cosines, including a subject-
if we can separately determine the subject, object, and verb        predicate approach and given-new approach. In the current eval-
parts of a sentence, we should be able to provide informa-          uation, the simple average provided the best performance, so we
tion that, in addition to that which we get from LSA, will          do not present the others here.

                                                                   parsing techniques to automatically perform the prepro-
            Correlation with Human Raters                          cessing of input sentences for SLSA. This section describes
0.6
                                                                   different approaches that we are evaluating.
0.5                                                                Surface parsing
                                                                   Shallow parsing is currently an area of intense interest in
0.4                                                                the corpus-based natural language processing community.
                                                           LSA200  In fact, the 2001 Computational Natural Language Learn-
0.3                                                        SLSA200 ing workshop at the Association for Computational Lin-
                                                                   guistics conference will include a shared task which is to
0.2                                                                evaluate different techniques for clause splitting. Clause
                                                                   splitting is defined as separating a sentence into subject and
0.1
                                                                   predicate parts.
                                                                      We are currently evaluating the feasibility of using sev-
  0
                                                                   eral publicly available surface parsing tools: LTChunk,
          Novice           Skilled         Overall                 the SCOL parser, and the Memory-Based Shallow Parser
                                                                   (MBSP). LTChunk was developed by the Language Tech-
  Figure 2: The correlation between SLSA scores and human          nology Group at the University of Edinburgh (described
                                                                   at http://www.ltg.ed.ac.uk/software/chunk/). It
  ratings
                                                                   identifies noun phrases and verb groups (combinations of
                                                                   adverbs, auxiliaries, and verbs) in text. The SCOL parser
  In addition to the overall similarity score, SLSA produces       was developed by Abney (1996), and parses text using a
  separate measures of the similarity of the segments of the       set of cascaded rules which delay “difficult” decisions like
  sentences. This additional information could be very useful      where to attach prepositional phrases. MBSP (Daelemans,
  for dialog processing systems.                                   Buchholz, & Veenstra, 1999) is part of the Tilburg Memory
                                                                   Based Learner project (Daelemans, Zavrel, van der Sloot,
  Results                                                          & van den Bosch, 2000). It is also trained on Penn Tree-
  Because we were interested in evaluating this approach in        bank Wall Street Journal corpus, performs part-of-speech
  principle, and not with respect to any particular implemen-      tagging, and segments texts into subject, verbs, and objects.
  tation of the preprocessing technique, we preprocessed the       Current work
  entire test set by hand as described above. Then, the SLSA
  similarity scores were calculated and correlated with the        Each of these different methods has drawbacks. The corpus
  human ratings. Figure 2 compares the LSA and SLSA ap-            trained approaches have the same difficulty as that noted
  proaches with respect to the correlations to human ratings       above: the student answer texts differ sufficiently from the
  for the non-proficient, proficient, and averaged ratings.        Wall Street Journal to lead to many mistaggings, and there-
     SLSA performed better with respect to each subset of hu-      fore, misparses.
  man ratings than did the standard LSA approach. The cor-            Our current efforts are focussing on using the Brill Tag-
  relation with the mean of all four human raters was slightly     ger (adjusting its tags to be more appropriate for our do-
  better than the highest level of agreement among the human       main), and then the SCOL parser to identify sentence seg-
  raters.                                                          ments. We are developing a postprocessor to transform the
                                                                   output of the parser into the subject, verb, object segmen-
  Discussion                                                       tation that we need as input to SLSA. The postprocessor
  These results are not consistent with Landauer’s claim that      handles active, passive, and imperative constructions. We
  syntax does not convey additional semantic content beyond        are also working on a simple coreference resolution mech-
  the meanings of individual words. Human sentence simi-           anism to allow substitution of antecedents. Our set of hand-
  larity judgements are better modelled by an approach that        processed sentences gives us a useful gold standard against
  takes structural information into acccount. Although the         which to evaluate our approach.
  standard LSA approach does perform as well as humans                The process of matching the segments of the two sen-
  on longer texts, this may be because the information about       tences can be viewed as structure mapping of the type that
  who does what to whom in individual sentences is lost in         Gentner et al developed for processing analogies (Gen-
  the noise, or is constrained by the larger context.              tner, 1983; Forbus, Ferguson, & Gentner, 1994, for ex-
                                                                   ample). Ramscar and colleagues have developed a two-
           Toward a hybrid natural language                        stage model for processing analogy which first performs
                                                                   structure-mapping between two scenarios, and then uses
                        understander                               LSA to determine the similarity of the slot fillers between
  Now that we have validated the benefits of this approach,        the two structures (Yarlett & Ramscar, 2000). For SLSA,
  we have begun to develop a system that will use shallow          the proper treatment of syntactic structures like passives is

quite important. Even more difficult are alternations like       meaning.
“give” and “take” which can have the same syntactic struc-
ture, but completely different semantic role structures. Re-  • What is the best level of granularity to use in segmenting
solving such cases seems to require semantic information,        sentences? We have evaluated the use of subject, verb,
resulting in a chicken-and-egg situation. How can we use         and object segments, but a coarser or finer segmentation
SLSA to interpret the meaning of a sentence if we must           may perform better.
know the meaning in order to use SLSA? Our current re-        • How much semantic information can be derived from a
search involves treating the verbal and nominal parts of the     sentence without knowing its meaning? Inducing addi-
input sentences differently.                                     tional relationships between the parts of a sentence might
                                                                 improve the SLSA approach, but may require already
                       Conclusions                               knowing what the sentence is about.
Our findings do not support the claim that syntax provides a
                                                              Addressing these questions will be the focus of our future
negligible contribution to sentence meaning. Instead, a sen-
                                                              research.
tence comparison metric that combines structure-derived
information with vector-based semantics models human
                                                                                       References
similarity judgements better than LSA alone. As previously
mentioned, this approach provides a number of advantages.     Abney, S. (1996). Partial parsing via finite-state cascades.
Its overall fit to human data is not only better than stan-          In Proceedings of the ESSLLI ’96 Robust Parsing
dard LSA, but it provides additional information about the           Workshop.
similarity of the different parts of sentences. This could be Brill, E. (1994). Some advances in rule-based part of
used in a dialogue-based tutoring system to focus the stu-           speech tagging. In Proceedings of the Twelfth Na-
dent’s attention on some particular aspect of the target good        tional Conference on Artificial Intelligence. AAAI
answer.                                                              Press.
   With respect to traditional parsing techniques, SLSA has
three obvious advantages. First, it is fast, because it does  Charniak, E. (1997). Statistical Parsing with a Context-free
not deal with the combinatorial explosions from ambiguity            Grammar and Word Statistics. In Proceedings of the
that most parsers face. Second, it does not require a hand-          14th National Conference of the American Associa-
built semantic concept representation which is tedious to            tion for Artificial Intelligence, Providence, RI., July,
build and brittle. Third, LSA is (in a sense) grounded.              pp. 598–603.
Although it does not have direct experience of the world,
LSA does have indirect experience via its training corpus.    Collins, M. (1996). A New Statistical Parser Based on Bi-
The corpus provides a rich set of interconnections between           gram Lexical Dependencies. In Proceedings of the
terms which allows LSA to successfully model many as-                34th Annual Meeting of the Association for Compu-
pects of human linguistic competence.                                tational Linguistics, Santa Cruz, CA, pp. 184–191
   The limitation of SLSA as a natural language under-               San Francisco, CA. Morgan Kaufmann.
standing mechanism is that it is only appropriate for tasks   Collins, M. (1998). Head-Driven Statistical Models for
where understanding can be cast as computing the similar-            Natural Language Parsing. Ph.D. thesis, University
ity of an item to an expected utterance. For tutoring, the           of Pennsylvania.
approach is feasible because the tutor (whether computer
or human) normally determines the topic of conversation,      Daelemans, W., Buchholz, S., & Veenstra, J. (1999).
and has some idea of what the student should say. For other          Memory-Based Shallow Parsing. In Proceedings of
tasks where the input utterance is less constrained, this ap-        CoNLL-99.
proach might not be the best. On the other hand, if a natural
                                                              Daelemans, W., Zavrel, J., van der Sloot, K., & van den
language generation system could be used to generate a set
                                                                     Bosch, A. (2000). TiMBL: Tilburg Memory Based
of expected utterances in a particular domain, expectation-
                                                                     Learner, version 3.0, Reference Guide. Tech. rep.
based understanding might be feasible and effective.
                                                                     Technical Report 00-01, 2000, ILK, University of
   Although we have presented the syntactic analysis of this
                                                                     Tilburg. available at http://ilk.kub.nl/.
work as being derived from symbolic, rule-based mecha-
nisms, our analyses of SLSA as a cognitive model do not       Deerwester, S., Dumais, S. T., Furnas, G. W., Landauer,
depend on this. They would be equally applicable with a              T. K., & Harshman, R. (1990). Indexing by Latent
connectionist surface parsing technique.                             Semantic Analysis. Journal of the American Society
   These findings raise quite a few interesting questions for        for Information Science, 41, 391–407.
future research. For example:
                                                              Forbus, K., Ferguson, R., & Gentner, D. (1994). Incre-
• What exactly are humans measuring when they rate sen-              mental structure mapping. In Proceedings of the 16th
   tence similarity? Perhaps varying the instructions for hu-        Annual Conference of the Cognitive Science Society
   man raters will get them to focus on different aspects of         Mahwah, NJ. Erlbaum.

Gentner, D. (1983). Structure-mapping: A theoretical
      framework for analogy. Cognitive Science, 7, 155–
      170.
Goldstone, R., Medin, D., & Halberstadt, J. (1997). Simi-
      larity in context. Memory and Cognition, 25(2), 237–
      255.
Landauer, T. K., Laham, D., Rehder, R., & Schreiner, M. E.
      (1997). How well can passage meaning be derived
      without using word order? A comparison of Latent
      Semantic Analysis and humans. In Proceedings of
      the 19th Annual Conference of the Cognitive Science
      Society, pp. 412–417 Mahwah, NJ. Erlbaum.
Landauer, T., & Dumais, S. (1997). A solution to Plato’s
      problem: The latent semantic analysis theory of ac-
      quisition, induction, and representation of knowl-
      edge. Psychological Review, 104, 211–240.
Rehder, B., Schreiner, M., Laham, D., Wolfe, M., Lan-
      dauer, T., & Kintsch, W. (1998). Using Latent Se-
      mantic Analysis to assess knowledge: Some techni-
      cal considerations. Discourse Processes, 25, 337–
      354.
Resnik, P., & Diab, M. (2000). Measuring Verb Similarity.
      In Proceedings of the 22nd Annual Conference of the
      Cognitive Science Society Mahwah, NJ. Erlbaum.
Tversky, A. (1977). Features of similarity. Psychological
      Review, 84(4), 327–352.
Wiemer-Hastings, P. (2000). Adding syntactic information
      to LSA. In Proceedings of the 22nd Annual Confer-
      ence of the Cognitive Science Society, pp. 989–993
      Mahwah, NJ. Erlbaum.
Wiemer-Hastings, P., Graesser, A., Harter, D., & the Tu-
      toring Research Group (1998). The foundations and
      architecture of AutoTutor. In Goettl, B., Halff, H.,
      Redfield, C., & Shute, V. (Eds.), Intelligent Tutoring
      Systems, Proceedings of the 4th International Con-
      ference, pp. 334–343 Berlin. Springer.
Wiemer-Hastings, P., Wiemer-Hastings, K., & Graesser, A.
      (1999a). How Latent is Latent Semantic Analysis?.
      In Proceedings of the Sixteenth International Joint
      Congress on Artificial Intelligence, pp. 932–937 San
      Francisco. Morgan Kaufmann.
Wiemer-Hastings, P., Wiemer-Hastings, K., & Graesser, A.
      (1999b). Improving an intelligent tutor’s comprehen-
      sion of students with Latent Semantic Analysis. In
      Lajoie, S., & Vivet, M. (Eds.), Artificial Intelligence
      in Education, pp. 535–542 Amsterdam. IOS Press.
Yarlett, D., & Ramscar, M. (2000). Structure-Mapping
      Theory and Lexico-Semantic Information. In Pro-
      ceedings of the 22nd Annual Conference of the Cog-
      nitive Science Society, pp. 571–576 Mahwah, NJ.
      Erlbaum.

