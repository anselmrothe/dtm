                      A Bayesian model of knowledge and metacognitive control:
                                                 Applications to opt-in tasks
                                               Stephen T. Bennett (stbennet@uci.edu)
                                                      Department of Cognitive Science
                                                       University of California, Irvine
                                           Aaron S. Benjamin (asbenjam@illinois.edu)
                       Department of Psychology and Beckman Institute for Advanced Science and Technology
                                                University of Illinois at Urbana-Champaign
                                              Mark Steyvers (mark.steyvers@uci.edu)
                                                      Department of Cognitive Science
                                                       University of California, Irvine
                               Abstract                                   the data is missing in a nonrandom fashion (see Little & Ru-
                                                                          bin, 2014, for a description of other missing data scenarios).
   In many ecologically situated cognitive tasks, participants en-        Consequently, participants can only be compared and their
   gage in self-selection of the particular stimuli they choose to
   evaluate or test themselves on. This contrasts with a traditional      performance fairly evaluated if a model is specified for the
   experimental approach in which an experimenter has complete            opt-in process. If a participant does not opt in to a partic-
   control over the participant’s experience. Considering these           ular question, then we simply do not see that participant’s
   two situations jointly provides an opportunity to understand
   why participants opt in to some stimuli or tasks but not to            response to that question.
   others. We present here a Bayesian model of cognitive and                 A starting point in explaining opt-in behavior is that partic-
   metacognitive processes that uses latent contextual knowledge
   to model how learners use knowledge to make opt-in decisions.          ipants have some meta-knowledge of what it is they already
   We leverage the model to describe how performance on self-             know, and use that knowledge effectively in service of ongo-
   selected stimuli relates to performance on true experimental           ing learning. People provide higher assessments of their abil-
   tasks that deny learners the opportunity for self-selection. We
   illustrate the utility of the approach with an application to a        ity to answer inference questions in domains in which they
   general-knowledge answering task.                                      have greater expertise (Bradley, 1981), and learners often
   Keywords: metacognitive control; Bayesian cognitive model;             choose to engage more effective study techniques for material
   wisdom of the crowd; opt-in; missing not at random                     that is more difficult for them (A. S. Benjamin & Bird, 2006).
                                                                          Memory reports are also considerably more accurate when
                            Background                                    respondents have the option of withholding answers that they
                                                                          are unsure of or of titrating the grain size of their answers to
In traditional approaches to experimental psychology, an ex-              their perceived accuracy (Goldsmith & Koriat, 2007).
perimenter has unilateral control over which stimuli a par-
                                                                             Self-regulated learning often has substantial benefits in
ticipant experiences and the tasks that they complete. Yet
                                                                          educational contexts (Mezirow, 1981; Zimmerman, 1989;
in many real-world situations, such as providing ratings to
                                                                          Boekaerts & Minnaert, 1999; Paris & Paris, 2001). Learn-
videos on the Internet, the participant has some or even total
                                                                          ers use meta-knowledge to allocate time, resources, and ac-
control over the specific stimuli and tasks that they experi-
                                                                          tivities to an array of learning goals, and this application in-
ence. The choice behavior underlying such self-selection is
                                                                          creases overall performance compared to learners who have
an important domain of study called metacognition (Nelson
                                                                          their learning activities dictated by an instructor (Winne &
& Narens, 1990), and the self-selection of activities or stim-
                                                                          Hadwin, 1998; Finley et al., 2010).
uli is specifically called metacognitive control (Fiechter, Ben-
jamin, & Unsworth, 2016; Finley, Tullis, & Benjamin, 2010).                  The benefits of self-control extend beyond these con-
Some work on monitoring and control processes in mem-                     strained tasks, however. In causal reasoning experiments, par-
ory tasks focused on confidence judgments as an indicator                 ticipants can more quickly understand the causal structure of
of self-selection questions (Kelley & Sahakyan, 2003; Ko-                 a network if they intervene in the learning process and de-
riat & Goldsmith, 1996). It is unclear precisely how this                 sign their own “experiments” (Steyvers, Tenenbaum, Wagen-
self-selection is generated, however. To better understand                makers, & Blum, 2003; Sobel & Kushnir, 2006; Lagnado &
metacognitive control behavior, a model is needed that ac-                Sloman, 2004). Human strategy selection can be explained
counts for performance on the task of interest as well as the             in terms of rational metareasoning, wherein humans flexi-
choice behavior that leads participants to select only some               bly choose strategies in accordance with their environment
stimuli for exposure, evaluation, or testing.                             (Lieder & Griffiths, 2015; Lieder et al., 2014).
   The major difficulty of such an endeavor is that participants             The core claim across each of these examples is that self-
select tasks according to their interests and expertise, and so           selection within a task aimed at measuring performance is
                                                                     1623

driven by metacognitive knowledge, which leads to a higher            spond to):
rate of success, expertise, or interest for the selected items.
                                                                                p(θ|c, d, x, y) ∝ p(x|θ, c)p(y|θ, d)p(d|θ)p(θ)     (1)
This process makes it difficult to evaluate the stimuli and
the participants in an unbiased way. One test-taker may, for          In a traditional cognitive model, the important part of the
example, outperform another not because they have greater             model is the specification of p(x|θ, c) and p(y|θ, d), termed
knowledge but rather because they make more a judicious se-           the likelihood functions. These functions directly explain the
lection of problems.                                                  empirical effect of interest by relating latent knowledge to
                                                                      performance on the task given the experimental design. The
                                                                      novel part of the model relates to the specification of the
                                                                      metacognitive control process p(d|θ), which explains how
                                                                      the participant self-designs on the basis of their latent knowl-
                                                                      edge. If we would ignore this model component, we would
                                                                      likely, and incorrectly, conclude that participants who self-
                                                                      designed were more knowledgeable than participants subject
                                                                      to the experimenter’s design because they outperformed their
                                                                      experimenter-designed counterparts. Such an error could be
                                                                      catastrophic if we were trying to compare across individuals
                                                                      or across tests. Because subjects are randomly assigned to
                                                                      conditions, it is highly unlikely that they differ widely. The
                                                                      process by which the participants who self-designed outper-
Figure 1: Outline of our modeling approach. Latent knowl-             formed those who could not lies in the opportunity to self-
edge and design both explain the performance on the task.             design. Here we see the importance of jointly modeling the
In the case of a subject-chosen design, latent knowledge also         selection process and the task at hand in order to understand
explains the design.                                                  the interplay between latent knowledge, opt-in behavior, and
                                                                      performance.
   The aim of this project is to develop a cognitive model               Since this is a task in which many participants give judg-
of the metacognitive aspect of item selection. In doing so,           ments to many questions, we also expect to find that aver-
it also provides a framework to relate performance on self-           aging across participants leads to higher accuracy–an effect
selected materials with performance on an unconstrained set           termed the wisdom of the crowd (Surowiecki, 2004; Steyvers,
of items or stimuli. Here we apply this model to data col-            Miller, Hemmer, & Lee, 2009). Here we have the opportu-
lected from participants answering general knowledge ques-            nity to evaluate whether the opportunity to opt in to a self-
tions, but the model is considerably more general: the same           selected portion of the questions will enhance or attenuate
principles could apply in other metacognitive control tasks,          such benefits associated with averaging. Certainly, many par-
such as study time allocation or selection of items for restudy.      ticipants will gravitate towards the same questions when they
We are aware of one current model of metacognitive con-               can opt in, which would potentially decrease the benefits of
trol, which takes as a given the state of the world, which then       averaging across a crowd by virtue of reducing input to the
causes the observed behaviors (Fleming & Daw, 2017). We               more difficult questions. However, based on what is known
take a different approach, which starts with the latent knowl-        about metacognition, we expect that participants will opt in
edge that the participant is coming to the experiment with            to questions for which they have relevant knowledge, which
and uses that in both the selection process behind opting-in          could lead to a more informed set of responses to average with
and the observed responses to questions, as illustrated in Fig-       the remaining crowd. Crowd behavior provides an additional
ure 1. Performance on the task is explained by both the de-           benchmark against which we can evaluate the performance of
sign—that is, the particular experience of the participant in         the metacognitive model.
the task—and the latent knowledge of the participant. In the
case of a participant who can opt in to certain questions but                                    Experiment
avoid others, the design is also partially informed by the latent     Stimuli The question set consisted of 100 general-knowledge
knowledge. We are interested in estimating the latent knowl-          binary choice questions. The questions were drawn from 12
edge of each participant and evaluating how it relates both           topics: World Facts, World History, Sports, Earth Sciences,
to performance on the task and to opt-in behavior. In order           Physical Sciences, Life Sciences, Psychology, Space & Uni-
to infer latent knowledge from the observed data, we apply            verse, Math & Logic, Climate Change, Physical Geography,
Bayes’ rule in the equation below, where θ is the latent knowl-       and Vocabulary. The question set was created by collecting
edge, c is the experimenter design, d is the subject-chosen           from multiple sources. Two example questions are shown in
design, x are the performance data from a true experimental           Table 1. Based on the empirically observed accuracy levels,
design (where subjects respond to all or to a random subject          the first is difficult and the second is easy.
of probes), and y are the performance data from a subject-               Participants A total of 83 participants were recruited
chosen design (in which subjects choose which probes to re-           through Amazon Mechanical Turk (AMT). Each participant
                                                                  1624

                                                      Table 1: Example questions.
  Difficulty Example
  Hard       The Sun and the planets in our Solar system all rotate in the same direction because: (a) they were all formed from
             the same spinning nebular cloud, or (b) of the way the gravitational forces of the Sun and the planets interact
  Easy       Greenhouse effect refers to: (a) gases in the atmosphere that trap heat, or (b) impact to the Earth’s ozone layer
was compensated $1 for the 30 minutes the experiment was                   For the self-selection condition, we assume that partici-
expected to take, and assigned to one condition.                        pants have a preference to select questions for which they
   Design Participants could view the survey description on             believe they have knowledge. Let c represent the observed
AMT. If they selected the survey they were redirected to an-            question selections with ci, j = 1 if question j was selected
other website. They were first directed to a study informa-             by participant i. For each participant and question block, we
tion sheet which provided details of the survey and compen-             model question selection in the opt-in condition by a sam-
sation. If they agreed to continue, they were instructed to             pling process:
answer some demographic questions. Participants were ran-
domly assigned to either a random condition (N = 44) or a                           ci ∼ SampleWR((δi,1 + κ, ..., δi,K + κ), M)        (3)
self-selection condition (N = 39), determining the subject’s
                                                                        where K is the total number of questions available for selec-
role in selecting which questions to answer. Participants were
                                                                        tion in each block (K=20 in our experiment), M is the number
not aware of the existence of other conditions. Each partici-
                                                                        of questions that need to be selected (M=5 in the experiment),
pant saw the questions in 5 blocks of 20 questions each. In
                                                                        SampleWR(δ,M) represents a sampling without replacement
each block, they were instructed to rate the difficulty of each
                                                                        distribution where M items are sampled with probability pro-
question and then, if they were assigned to the opt-in condi-
                                                                        portional to δ, and κ is a fixed parameter that controls the
tion, instructed to choose 5 of those 20 questions to answer.
                                                                        randomness in the selection process. Higher κ values make
The participants in the random assignment condition were
                                                                        it more likely that questions are selected for which the par-
randomly assigned 5 questions from that block to answer. Af-
                                                                        ticipant has no subjective knowledge. For participants in the
ter rating the difficulty of all 100 questions and answering 25
                                                                        random condition, we assume that the questions are randomly
of them, participants were thanked for their time and given
                                                                        sampled by a process that is under control of the experimenter
instructions on how to receive payment.
                                                                        (where M out of K questions are randomly allocated).
                                                                           Let xi, j represent the observed accuracy for participant i on
                              Model                                     question j. We do not assume a fixed relationship between
The model utilizes an IRT model to generate subjective la-              belief of knowledge and accuracy. For each question, we in-
tent knowledge (the belief of a participant that she can an-            troduce guessing rate parameters ρ j and λ j that control the
swer a question), which informs all aspects of participants’            probability of correct responding if the participant does or
responses including the observed accuracy and difficulty rat-           does not have subjective knowledge about a question:
ings, as well as the metacognitive process of question selec-
tion. We describe participants as opting-in to questions for                           xi, j ∼ Bernoulli(δi, j ρ j + (1 − δi, j )λ j ) (4)
which they believe they have knowledge, answering with ac-
curacy dependent on whether or not they believe they have               For example, with ρ = 0.8 and λ = 0.4, the probability of a
knowledge, and giving lower difficulty ratings when they be-            correct response is 0.8 if a participant has subjective knowl-
lieve they have knowledge.                                              edge, but 0.4 if the participant does not. The guessing param-
   We use an IRT model to generate the subjective latent                eters are given Beta priors, ρ j ∼ Beta(α, β), λ j ∼ Beta(α, β)
knowledge, δi, j , for each participant i (across both the opt-         where α and β are hyperparameters that control the variability
in and random condition) and question j,                                in guessing rates across questions.
                                                                           To model the difficulty ratings, we use an ordered logit
               δi, j ∼ Bernoulli(logit−1 (θi + η j ))          (2)      model (Williams et al., 2006). We assume that subjective la-
                                                                        tent knowledge informs the perceived difficulty of questions.
where θi is the self-perceived skill of participant i, η j is the       Questions for which the participant believes they have knowl-
                                                              ex        edge are perceived as easier. Let φi, j represent the perceived
perceived familiarity of question j, and logit−1 (x) = 1+e       x.
                                                                        difficulty for participant i on question j. We determine the
This latent knowledge is represented as a 0 or 1, indicating
                                                                        perceived difficulty by:
whether or not that participant believes that she has knowl-
edge for that question. We place a Normal prior on the self-                             φi, j = −δi, j − η j ξ + ωi − β j + σi, j     (5)
perceived skill, θi ∼ Normal(0, σ), such that participants are
expected to have the same skill (on average) for both the self-         where β j and ωi capture participant and item level effects
selection and random conditions.                                        (e.g. some participants might find all items easy, some items
                                                                    1625

might be judged as easy) independent of subjective knowl-
edge. In addition, we also allow the perceived familiarity of
a question η j to affect the perceived difficulty weighted by a
fixed scaling parameter ξ. Finally, σi, j represent small pertur-
bations centered around 0 to explain the random variability
in difficulty ratings unrelated to any of the previous factors
mentioned. These perceived difficulties feed into the ordered
logit model to generate the difficulty ratings ri, j ,
                  ri, j ∼ OrderedLogit(φi, j , τi )           (6)
where τi is the set of criteria cutoffs for participant i.
   We used JAGS to perform parameter inference. All pa-
rameters were inferred jointly from the opt-in and random
condition. All model predictions were derived from posterior
predictives where we simulate new participants from the dis-         Figure 2: Latent knowledge is similar between conditions and
tribution and assess how they self-select from a new set of          corresponds to opting-in behavior. Plotted are the opt-in be-
questions.                                                           haviors and average δi, j values across conditions, all sorted by
                                                                     the popularity of the question in the opt-in condition. White
                            Results                                  corresponds to questions that the participant opted in to or the
                                                                     inferred presence of knowledge.
We examine several empirical effects within the data and ob-
serve that the model captures the appropriate trend in most
cases.
   Item selection and latent knowledge. The model cap-               in to easier questions. In order to perform this analysis, we
tures the expected relationship between opting-in behavior           took the product of the evidence that performance is higher
and knowledge (see Figure 2). Participants were more likely          in the opt-in condition than the random assignment condition
to select questions for which they had pre-existing knowl-           for each question and find a Log10 BF of 9.02. So, even when
edge. Each question was randomly assigned to at least four           comparing on an item-by-item basis, opting-in provides an
participants in the random assignment condition. However,            advantage.
in the opt-in condition, there were seven questions that no             Effect of opt-in on model performance. For the model,
participant chose to answer. Question selection strongly cor-        the average accuracy for posterior predictive samples in the
responded with the inferred latent knowledge (δi, j ) for the        self-selection condition (mean = 79.03%) is also significantly
participant-question pair, with participants choosing ques-          higher than in the random condition (mean = 67.07%), both
tions for which they had latent knowledge. Across conditions,        across all questions (99.86 % of samples) and even within
latent knowledge is distributed in a similar manner: most par-       questions (68.93 % of sample-question pairs). We observe
ticipants have knowledge for popular questions, few partic-          this benefit in accuracy despite the average inferred abil-
ipants have knowledge for unpopular questions, and some              ity of individual subjects (θi ) being equivalent across con-
participants are more knowledgeable than others. However,            ditions: θi = 0.00, SD = 0.99 in the opt-in condition versus
the model has substantially more certainty about the localiza-       θi = −0.09, SD = 2.04 in the random assignment condition.
tion of this knowledge in the opt-in condition compared to           This means that the benefit to accuracy that the model pre-
the random condition because it can leverage the opt-in be-          dicts is due to downstream consequences of the metacogni-
havior. In Figure 2, this certainty is expressed as black or         tive selection process and not an (inaccurate) inference that
white squares, while uncertainty is represented in gray. We          participants in one condition were more skillful than in the
see the uncertainty about which participants have knowledge          other.
for which question as a “blurring” of the latent knowledge              Difficulty Ratings. Participants tended to give lower aver-
space.                                                               age difficulty ratings to questions that they opted in to (Log10
   Effect of opting-in on participant performance. Average           BF = 91.89) and higher average difficulty ratings to questions
performance across questions was higher in the self-selection        that they did not opt in to (Log10 BF = 64.09), relative to the
condition (86.05%) than in the random condition (67.27%).            random condition. The model captures, but understates, this
We computed a Bayes Factor (BF) given a binomial distribu-           trend (see Figure 3).
tion with a shared or different rate of correct responding and          Wisdom of the crowd. The left panel of Figure 4 shows
find a Log10 BF of 21.12 in favor of a higher rate of correct        the relationship between crowd size and crowd accuracy for
responding in the opt-in condition. This corresponds to de-          the two conditions in the experiment, as well as a hybrid con-
cisive evidence that average accuracy is higher in the opt-in        dition in which the two groups are combined. The right side
condition than the random assignment condition. This occurs          of the Figure shows that the model captures this effect qualita-
even when taking into account the fact that people tend to opt       tively. Crowd responses were determined by taking the most
                                                                 1626

                                                                      Figure 4: Crowd performance when varying the number of
                                                                      participants (measured by the total number of judgments)
Figure 3: Distribution of difficulty ratings for participants and
model for questions that were selected or not selected in the         (see Figure 5). We find that increasing the heterogeneity of
opt-in condition and the random condition. Lower ratings in-          perceived question difficulty increases self-selection accuracy
dicate lower perceived difficulty.                                    overall, but decreases it at the crowd level since participants
                                                                      tend to avoid answering the same difficult questions. Het-
                                                                      erogeneity in question difficulty does not have an apprecia-
common response across the participants in the crowd. Since           ble impact on performance in the random condition. In both
seven questions went unanswered in the opt-in condition, we           conditions, higher heterogeneity of participant skill leads to
had to consider how unanswered questions impacted crowd               higher crowd performance and gives resilience to heteroge-
performance. To treat the self-selection condition maximally          neously difficult questions in the opt-in condition. However,
conservatively, we graded any question that went unanswered           it detracts from overall accuracy in the self-selection condi-
as incorrect for that crowd. Even with this penalty, the crowd        tion.
composed of the participants from the self-selection condition
(79%) outperformed the crowd of subjects from the random
condition (73%).
   We also considered the impact of crowd size on perfor-
mance. To do this, we evaluated the average performance of
crowds composed of random samples of participants from a
condition and varied the number of participants drawn to form
the sample. We plot average crowd performance as a function
of the total number of judgments, where a judgment is a per-
son’s response to a question. The hybrid condition provides a
means of improving upon both conditions. To create a hybrid
crowd, we first sampled participants that answered the ques-
tion from the opt-in condition. If a question had no responses,
we added the answer from one participant in the random con-
dition in order to guarantee that all questions received at least
one answer. This hybrid crowd has high performance across
all questions. The model captures the general trends in the
                                                                      Figure 5: Simulated performance depending on variability of
data in that larger crowds result in higher crowd accuracy,
                                                                      question difficulty (η j ) and participant skill (θi ).
opt-in crowds outperform random-assignment crowds, and
the hybrid crowds perform well across all questions.
   Additional simulations. Given our model, we investigated
which circumstances would likely lead to changes in the rela-                                   Conclusions
tive performance of the self-selection and random conditions          A comprehensive model of cognition must make allowance
in terms of both average overall accuracy and crowd perfor-           for the fact that cognitive behavior is driven by motivations.
mance. We varied the heterogeneity of perceived question              We choose what we attend to and attempt to encode, and
difficulty (η j ) and latent ability (θi ). We did this by simu-      what we attempt to remember. Metacognitive behavior is at
lating experiments in which we varied the underlying hyper-           the heart of most learning outside the laboratory, and a fair
parameter corresponding to the variability of θi and η j by fac-      amount within it as well (A. Benjamin & Ross, 2008). The
tors of 0.25, 1, and 4 while keeping other parameters constant        joint modeling of metacognitive behavior–like self-selection
                                                                  1627

of items–along with cognitive performance has the potential           Kelley, C. M., & Sahakyan, L. (2003). Memory, monitoring,
to address a wider and more representative range of real-               and control in the attainment of memory accuracy. Journal
world learning and testing behaviors, and can serve as the              of Memory and Language, 48(4).
basis for drawing comparisons across individuals or tests that        Koriat, A., & Goldsmith, M. (1996). Monitoring and control
would otherwise be hopelessly confounded. Additionally, the             processes in the strategic regulation of memory accuracy.
model could be extended to explain various incentives given             Psychological review, 103(3), 490.
to the participant, which would impact how latent knowl-              Lagnado, D. A., & Sloman, S. (2004). The advantage of
edge interacts with the task to generate opt-in behaviors. The          timely intervention. Journal of Experimental Psychology:
model presented here provides a starting point for such an              Learning, Memory, and Cognition, 30(4), 856.
enterprise. It leads to a relatively good description of per-         Lieder, F., & Griffiths, T. L. (2015). When to use which
formance across a variety of metrics. A single latent knowl-            heuristic: A rational solution to the strategy selection prob-
edge state for each participant-question pair permits an ex-            lem. In Cogsci.
plicit representation of the metacognitive process that gov-          Lieder, F., Plunkett, D., Hamrick, J. B., Russell, S. J., Hay,
erns the relationship between opt-in, accuracy, and difficulty          N., & Griffiths, T. (2014). Algorithm selection by ratio-
behaviors. The model is successful in describing the nonran-            nal metareasoning as a model of human strategy selection.
dom missing nature of the data that we observed by relying on           In Advances in neural information processing systems (pp.
principled psychological theories about why someone might               2870–2878).
choose one question over another.                                     Little, R. J., & Rubin, D. B. (2014). Statistical analysis with
   An additional lesson of the current research can be seen             missing data. John Wiley & Sons.
in the crowd data. Opting in is generally beneficial to crowd         Mezirow, J. (1981). A critical theory of adult learning and
accuracy in both the observed data and our model. This re-              education. Adult education quarterly, 32(1), 3–24.
sult indicates that the metacognitive skill of the individuals        Nelson, T. O., & Narens, L. (1990). The psychology of learn-
in self-selection can be leveraged in order to create a smarter         ing and motivation. Metamemory: A theoretical framework
crowd. This effect is sufficiently robust that it appears to out-       and new findings.
weigh the cost associated with small crowd sizes for some             Paris, S. G., & Paris, A. H. (2001). Classroom applications of
questions or no volunteered responses at all for a small num-           research on self-regulated learning. Educational psycholo-
ber of questions. Such a result is particularly important when          gist, 36(2), 89–101.
considering the widespread availability of datasets in which          Sobel, D. M., & Kushnir, T. (2006). The importance of deci-
responses are self-selected.                                            sion making in causal learning from interventions. Memory
                                                                        & Cognition, 34(2), 411–419.
                         References                                   Steyvers, M., Miller, B., Hemmer, P., & Lee, M. D. (2009).
                                                                        The wisdom of crowds in the recollection of order informa-
Benjamin, A., & Ross, B. H. (2008). The psychology of                   tion. In Advances in neural information processing systems
  learning and motivation: Skill and strategy in memory use             (pp. 1785–1793).
  (Vol. 48). Academic Press.                                          Steyvers, M., Tenenbaum, J. B., Wagenmakers, E.-J., &
Benjamin, A. S., & Bird, R. D. (2006). Metacognitive control            Blum, B. (2003). Inferring causal networks from obser-
  of the spacing of study repetitions. Journal of Memory and            vations and interventions. Cognitive science, 27(3), 453–
  Language, 55(1), 126–137.                                             489.
Boekaerts, M., & Minnaert, A. (1999). Self-regulation with            Surowiecki, J. (2004). The wisdom of crowds: why the many
  respect to informal learning. International journal of edu-           are smarter than the few and how collective wisdom shapes
  cational research, 31(6), 533–544.                                    business, economics, society and nations. Little, Brown.
Bradley, J. V. (1981). Overconfidence in ignorant experts.            Williams, R., et al. (2006). Generalized ordered logit/partial
  Bulletin of the Psychonomic Society, 17(2), 82–84.                    proportional odds models for ordinal dependent variables.
Fiechter, J. L., Benjamin, A. S., & Unsworth, N. (2016).                Stata Journal, 6(1), 58.
  16 the metacognitive foundations of effective remember-             Winne, P. H., & Hadwin, A. F. (1998). Studying as self-
  ing. The Oxford Handbook of Metamemory, 307.                          regulated learning. Metacognition in educational theory
Finley, J. R., Tullis, J. G., & Benjamin, A. S. (2010).                 and practice, 93, 27–30.
  Metacognitive control of learning and remembering. In               Zimmerman, B. J. (1989). A social cognitive view of self-
  New science of learning (pp. 109–131). Springer.                      regulated academic learning. Journal of educational psy-
Fleming, S. M., & Daw, N. D. (2017). Self-evaluation                    chology, 81(3), 329.
  of decision-making: A general bayesian framework for
  metacognitive computation. Psychological Review, 124(1),
  91.
Goldsmith, M., & Koriat, A. (2007). The strategic regulation
  of memory accuracy and informativeness. Psychology of
  learning and motivation, 48, 1–60.
                                                                  1628

