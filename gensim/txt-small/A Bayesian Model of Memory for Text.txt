                                        A Bayesian Model of Memory for Text
                                             Mark Andrews (mark.andrews@ntu.ac.uk)
                                         Department of Psychology, Nottingham Trent University
                                                          Nottingham, NG1 4FQ, UK
                               Abstract                                    1972; Graesser, Singer, & Trabasso, 1994; Zwaan & Rad-
                                                                           vansky, 1998; Rawson & Kintsch, 2002, to name but a few).
   The study of memory for texts has had an long tradition of re-
   search in psychology. According to most general accounts of             However, in most studies, even fundamental concepts such as
   text memory, the recognition or recall of items in a text is based      memory schemas are not formally defined (see, e.g. Ghosh
   on querying a memory representation that is built up on the ba-         & Gilboa, 2014), and ostensibly formal models of knowl-
   sis of background knowledge. The objective of this paper is to
   describe and thoroughly test a Bayesian model of this general           edge influences on text representation, such as the well known
   account. In particular, we develop a model that describes how           work of Kintsch (1988), often require hand-coding of back-
   we use our background knowledge to form memories as a pro-              ground knowledge and text structures and can only be applied
   cess of Bayesian inference of the statistical patterns that are
   inherent in a text, followed by posterior predictive inference of       to small and contrived examples. Consequently, there is no
   the words that are typical of those inferred patterns. This pro-        formal or computational account of how background knowl-
   vides us with precise predictions about what words will be re-          edge is used to infer a representation of text content and how
   membered, whether veridically or erroneously, from any given
   text. We then test these predictions using data from a memory           memories are then derived from this representation that is suf-
   experiment using a relatively large sample of randomly chosen           ficiently precise to lead to testable empirical predictions.
   texts from a representative corpus of British English.                     In this paper, following general principles followed by
   Keywords: Bayesian models; Memory; Reconstructive mem-                  Hemmer and Steyvers (2009a, 2009b, 2009c) in their studies
   ory; Text memory;                                                       on memory for visual objects and natural scenes, we describe
                                                                           a probabilistic model that uses Bayesian inference to infer a
                            Introduction                                   representation of a text’s content on the basis of background
The seminal study on memory for text1 is usually attributed                knowledge and then uses posterior predictive inference to
to Bartlett (1932). In this now classic work, Bartlett argued              represent the memories of that text. This provides us with
that a person’s memory for what they read is based on a re-                precise predictions about what words will be remembered,
construction of the information in the text that is strongly               whether veridically or erroneously, from any given text. We
dependent on their background knowledge and experiences.                   then test these predictions using data from a memory experi-
From this seminal work, but especially since the widespread                ment using a relatively large sample of randomly chosen texts
adoption of schema based accounts of text memory begin-                    from a representative corpus of British English.
ning in the 1970’s (e.g., Mandler & Johnson, 1977; Schank
& Abelson, 1977; Bower, Black, & Turner, 1979), there has                                      Probabilistic Model
been something close to a consensus on the broad or general                We begin with the assumption that our background knowl-
characteristics of human text memory. According to this gen-               edge that is relevant for our memory of text is primarily
eral account — which we can summarize by the following                     knowledge of the statistical patterns across spoken and writ-
schematic:                                                                 ten language. Given any probabilistic language model that
                                Knowledge                                  specifies these statistical patterns, as we explain below, we
                                                                           may then use Bayes’s rule to infer which patterns are inher-
                                                                           ent in any given text. From this, we may then predict, via
                                                                           posterior predictive inference, which words are and are not
          Memory                                          Text             typical or compatible with the inferred statistical representa-
                              Representation                               tion of the text. This effectively serves as the memory of the
                                                                           content of the text. As such, this provides a computational
— the recognition or recall of items in a text is based on                 description of the previous schematic, i.e.,
querying a representation of the text that is built up on the
                                                                                                              Knowledge
basis of background knowledge and experience.                                                     (statistical patterns in language)
   Although some variant of this general account is widely
held, it is essentially an informal and untestable theory. Cer-                                                       Bayes’s rule
tainly, there has been ample evidence showing that we use                                     posterior prediction
our background knowledge to make inferences and associa-                         Memory                                                   Text
tions concerning text content and that these inferences then                                               Representation
influence our memory (e.g. Bransford, Barclay, & Franks,                                        (statistical patterns inherent   in Text)
    1 In this paper, we use the term text to refer generally to any co-       In practical terms, we have many options for our choice of
herent or self-contained piece of spoken or written language.              probabilistic language model. However, probabilistic topic
                                                                        69

models (see, e.g. Griffiths, Steyvers, & Tenenbaum, 2007;                          We may then use the posterior predictive distribution to infer
Steyvers & Griffiths, 2007; Blei, 2012) have proved highly                         the words that are typical of the topics inherent in w j0 . The
effective in capturing the statistical patterns that character-                    predicted probability of word w j0 i0 given text w j0 is given by
ize the coarse-grained “discourse topics” across spoken and                                                         Z
written language. Here, we use a type of probabilistic topic                               P(w j0 i0 |w j0 , D ) =    P(w j0 i0 |π j0 , D )P(π j0 |w j0 , D )dπ j0
model known as a hierarchical Dirichlet process mixture
model (HDPMM) (Teh, Jordan, Beal, & Blei, 2006).
                                                                                   Corpus As our language corpus, we used the British Na-
    A HDPMM is a probabilistic generative model of bag-of-
                                                                                   tional Corpus (BNC) (BNC Consortium, 2007). From the en-
words2 language data. It treats a corpus of language data as a
                                                                                   tire BNC, we extracted all sections that were tagged as para-
set of J texts w1 , w2 . . . w j . . . wJ , where text j, i.e., w j , is a set
                                                                                   graphs. This gave us a corpus with a total word count of
of n j words from a finite vocabulary, represented simply by
                                                                                   87,564,696 words. From this, we created a set of 184,271
the V integers {1, 2 . . .V }. From this, we have each w j defined
                                                                                   texts, each between 250 and 500 words long. These were cre-
as w j = w j1 , w j2 . . . w ji . . . w jn j , with each w ji ∈ {1 . . .V }. As
                                                                                   ated by using either single paragraphs in this count range, or
a generative model of this corpus, the HDPMM treats each
                                                                                   concatenating consecutive paragraphs until they were within
observed word w ji as a sample from one of an underlying set
                                                                                   this range. The total word count of this set of texts was
of component distributions, φ1 , φ2 . . . φk . . ., where each φk is
                                                                                   78,723,408 words. We then restricted the word types by ex-
a probability distribution over {1 . . .V }. Each φk effectively
                                                                                   cluding words that occurred less than 5 times in total, and any
identifies a “discourse topic”. For example, here is a sample
                                                                                   words on either of two lists of stopwords, and any words that
of 6 topics from an inferred model, where we show the 7 most
                                                                                   were not listed in a dictionary of ≈ 60K English words. This
probable words in each topic:
                                                                                   lead to a final vocabulary of 49,328 word types. For more
  theatre        music        league         prison         rate          pub
   stage         band           cup           years        cent        guinness    information, see Footnote3 .
    arts          rock        season       sentence      inflation        beer
    play         song          team            jail     recession        drink     Inference We used a Gibbs sampler to infer the posterior
   dance        record         game           home      recovery          bar      distribution over the values of latent variables, i.e., {x ji : j ∈
   opera          pop         match        prisoner     economy        drinking    1 . . . J, i ∈ 1 . . . n j }, as well as the hyper-parameters m, a, b, ψ,
    cast         dance      division        serving         cut         alcohol
                                                                                   and γ. For more information, see Footnote4
The identity of the particular topic distribution from which
w ji is drawn is determined by the value of a discrete latent                      Prediction From the entire set of paragraphs in the BNC, we
variable x ji ∈ {1, 2 . . . k . . .} that corresponds to w ji . The prob-          randomly sampled 50 paragraphs whose length was 150 ± 10
ability distribution over the possible values of each x ji is given                words, where at least 90% of the words are in the aforemen-
by a categorical distribution π j , i.e., π j = π j1 , π j2 . . . π jk . . .,      tioned dictionary of English words, and where at least 75% of
where 0 ≤ π jk ≤ 1 and ∑∞                                                          the words were in a set of words for which word association
                                    k=1 π jk = 1, that is specific to text j.
Each π j is assumed to be drawn from a Dirichlet process prior                     norms exists (see the following section for more details on
whose base distribution, m, is a categorical distribution over                     the word association norms we used). For more information,
the positive integers and whose scalar concentration parame-                       see Footnote5 .
ter is a. The m base distribution is assumed to be drawn from                          For each of the 50 sampled texts, we then used poste-
a stick breaking distribution with a parameter γ. As such, the                     rior predictive inference, as described above, to obtain the
generative model of the corpus is as follows:                                      probability distribution over words that are typical or com-
                                                                                   patible with the topic based representation of each text. As
  w ji |x ji , φ ∼ dcat(φx ji ), x ji |π j ∼ dcat(π j ),           i ∈ 1...nj      explained above, this distribution effectively provides the in-
                                      π j |a, m ∼ ddp(a, m),         j ∈ 1...J     ferred model’s memory of the content of the text. A Gibbs
                                                                                   sampler was used to infer each text’s posterior distribution
                                      m|γ ∼ dstick(γ),                             over π, which is the probability distribution over discourse
where dcat is a categorical probability distribution, ddp is                       topics in that text. Two example texts and their posterior pre-
a Dirichlet process, and dstick is a stick breaking distribu-                      dictive inferences are shown in Figure 1. For more informa-
tion. The prior on the component distributions φ1 . . . φk . . .                   tion, see Footnote6 .
was a Dirichlet distribution with concentration parameter b                             3 Full details about how the corpus was created, in-
and length V location parameter ψ.                                                 cluding all the code used to create it, is available at
    Having inferred a HDPMM on the basis of a corpus of lan-                       https://github.com/lawsofthought/tantalum
                                                                                        4 Full details about the Gibbs sampler for the HDPMM ,
guage data D , given any new text, w j0 , we can use Bayes’s
                                                                                   including the code implementing it, can be found at
rule to infer the posterior probability over π j0 , which is the                   https://lawsofthought.github.io/gustavproject.
probability distribution over the discourse topics in w j0 :                            5 Full details about how we sampled the texts, including the code
                                                                                   implementing the sampling and the sampled texts themselves, can
                 P(π j0 |w j0 , D ) ∝ P(w j0 |π j0 , D )P(π j0 |D ).               be found at https://github.com/lawsofthought/berkelium.
                                                                                        6 Full details about how we sampled from the posterior predictive
    2 According to a bag-of-words model, a language corpus is a set                distribution, including the code implementing the sampling, can be
of texts, where each text is an unordered set, or bag, of words.                   found at https://github.com/lawsofthought/gallium.
                                                                                70

    Improve your mood and counteract stress: Ask anyone who ex-           Developmental norms are an attempt to provide an indica-
    ercises regularly and they will tell you that they always feel        tion of the ages at which one might expect ordinary children
    exhilarated at the end of a session even if they had begun by         to show evidence of certain skills or abilities. Since chil-
    feeling that they were not in the mood for exercise and had al-       dren vary with respect to the ages at which they demonstrate
    most forced themselves to continue. Physical fitness also pro-        any particular behaviour, norms represent an average obtained
    vides considerable protection against stress and the illnesses it     from an examination of the developmental changes occurring
    can cause. So, however busy your life, perhaps you could try          in a large number of children. Data from a large sample will
    and fit some regular exercise into your day. Let it be some-          show the earliest age at which a child would be expected to
    thing which is in complete contrast to the way you normally           gain control of a particular aspect of language, and the age
    spend your time. One word of warning though: if you are some-         by which 90 per cent or 95 per cent of non-handicapped chil-
    one whose daily life involves a strong competitive element, you       dren might be expected to show evidence of the same abil-
    would do well to avoid too much in the way of competitive             ity. If children who have already been diagnosed as suffer-
    sport (squash, tennis and so on) as your form of exercise as          ing from some specific handicapping condition are included,
    these will only tend to maintain an already high level of stress.     the data will show the expected age delay before this group
                                2222222222                                matches the performance of the normally developing children.
                                                                                                               2222222222
                    feel mind    exercise
        relaxation
          exercising stretching walking stamina build
        routine walk swimming fit training weight
                                                          people
                                                           energy           data time carried play
                                                                            scores cent
                                                                                                     individual children
                                                                                                              found measured information average school samples
                                                                                                                                                                    items
           aerobics  health yoga anxiety programme rest session                  sample extent adults family                reliability set population behaviour
       fitness increase life running week jogging rate
     aerobic tension exercises regular stress start
                                                               level        test   adult
                                                                                              parent       ability
                                                                                              score low childhood
                                                                                                                     testing
                                                                                                                            increase level  result
                                                                                                                                                     aged
                                                                                                                                                    provide
                                                                                                                                                           assessment
                                                                                                                                                              scale
     begin muscles gym minutes mood heart strength                                performance          tested parents measure
           body           muscle physical day time
                                                                           results mother age                     compared child                          home  validity
                                                                                                                tests
Figure 1: An example of two of the texts used in the memory experiment, and samples from the HDPMM’s posterior prediction
for each one. The predicted words are scaled as a function of their predicted probability, and we show the 50 most highly
predicted words (excluding stopwords and words not in the vocabulary) for each text. Words in italics are predicted words that
were not in the text itself. These, in effect, are the model’s false memories.
Comparison models                                                        the same BNC corpus as was used above, i.e. with the same
The focus of our analysis is whether the probability of rec-             184,271 texts each between 250 and 500 words. From this,
ognizing or recalling any given word having read a partic-               we can calculate
ular text is predicted by our HDPMM’s posterior predictive                                                                 PC (wk , wl )
distribution over words for that text. To properly evaluate                                            PC (wk |wl ) =                             ,
                                                                                                                               PC (wl )
the model’s predictions, it is necessary to compare them to
those of other plausible models. Here, we will compare the               which is the conditional probability of observing wk in any
Bayesian model to predictions made by two associative mod-               text given that wl has been observed. From this, if text j =
els. Both of these models predict that the words that are                w j1 , w j2 . . . w jn j , the predicted association probability of word
remembered from a text are those that are most associated,               wk according to text j is
on average, with the text’s content. Associative models are
                                                                                                                              n
strong models to compare to the Bayesian model because as-                                                               1 j
sociative strength has been repeatedly shown to a strong pre-                                  PC (wk |text j ) =            ∑ PC (wk |w ji ).
                                                                                                                        n j i=1
dictor of memory for words in word lists (e.g., Roediger, Wat-
son, McDermott, & Gallo, 2001; Gallo, 2006).                             We can interpret this value intuitively as the average associa-
   The statistical co-occurrence probability of two words, wk            tion between wk and text j , with association defined in terms
and wl , which we will denote PC (wk , wl ), is defined as the em-       of statistical co-occurrences in the language.
pirical probability of observing word wk and wl in the same                   An alternative means to calculate the average association
text7 in the language. Here, we calculate PC (wk , wl ) using            between wk and text j is using word association norms, rather
   7 Here,  as above, we use the term text to denote any coherent and    self-contained piece of language.
                                                                      71

than statistical co-occurrences. If Akl is the frequency that                Materials The texts used as stimuli for this experiment
word wk is stated as associated with word wl , then the condi-               were the above mentioned 50 texts.
tional probability of word wk given wl is                                       For the recognition tasks, test word lists with 20 words
                                                                             each were created. Of the 20 words in each list, 10 were
                                         Akl                                 present in the to-be-memorized text, while the remaining 10
                      PA (wk |wl ) =    V
                                                ,
                                       ∑i=1 Ail                              were not present in it. For each text, the list was created as fol-
                                                                             lows. Key words were extracted from each text and also from
where V is the total number of words in our vocabulary of                    the surrounding paragraphs to that text in the BNC. This was
response words. Now, given text j = w j1 , w j2 . . . w jn j , we can        done by calculating the tfidf (term frequency, inverse docu-
calculate                                                                    ment frequency) value for each word, and then applying a
                                     n
                                1 j                                          threshold to exclude the less informative words. 10 words
             PA (wk |text j ) =     ∑ PA (wk |w ji ),
                                n j i=1                                      were then randomly selected from the key words of each text.
                                                                             A further 10 words were randomly sampled from the key
which we can interpret as the average association between wk                 words of the surrounding paragraphs excluding any words the
and text j , with association now defined in terms of word asso-             in the main text itself. This set of 10 words were therefore not
ciation norms rather than statistical co-occurrences. Though                 present in the text to be memorized, but given that they were
a large set of English word association norms are available                  selected from surrounding paragraphs, they were likely to be
from the widely used Nelson norms (Nelson, McEvoy, &                         meaningfully related to it. As such, they would serve a useful
Schreiber, 2004), we used an even larger set that is a pre-                  items on the recognition memory test as they could not easily
release of the English small world of words association norms                be dismissed without a proper search of memory. For more
(De Deyne & Storms, 2017). This provided word associates,                    information, see Footnote9 .
produced by 101,119 participants, to 10,050 word types. For
more information, see Footnote8 .                                                                                                        ...
                                                                                                                                5s
                         Experiment                                                                                     2s
Our aim in this experiment is to measure participants’ mem-                                                      5s
ory of the 50 sampled texts described in the previous section.                              60s
Participants read these texts at their normal reading speed and
then their memory for what they have read is tested using both
recall and recognition tasks. We will then compare the pat-                    45s - 90s
tern of results from our participants with the predictions of                                                  Recognition test
the models.                                                                                       Tetris
Methods
                                                                                     Text
Participants 216 people (113 female, 103 male) partici-
pated in the experiment. The ages ranged from 17 to 78 years,
with a median of 34 years. Participants were recruited from
the student and general populations, with the only restriction                                                    Recall test
being that they be native English speakers.
                                                                             Figure 2: The task diagram of one block in the experiment:
Design Pre-experiment sample size determination calcu-                       Participants read a randomly assigned text, perform a filler
lations showed that, given the reasonable assumptions of                     task, and then have their memory tested using either a recog-
both inter-text and inter-subject variability in memory per-                 nition or recall test, with the test type being randomly chosen.
formance, a relatively large number of texts and participants                This process is repeated three times for each participant.
was necessary. In particular, we showed that there is a high
probability of detecting effects, even when these effects are                Procedure Each experiment session proceeded as follows
relatively weak, if we have at least 50 texts and at least 150               (see also Figure 2):
subjects are used. Importantly, these results hold even when
each subject sees only a small subset of total number of texts,              • After initial information and instructions, which informed
and this subset can be as low as 3 texts per each participant.                 participants that they would be engaging in memory tasks,
We therefore used all 50 texts described above, and initially                  one of the sample texts appeared on screen. Participants
aimed for approximately 200 participants, with each partici-                   were instructed to read this text at their normal reading.
pant being tested with a randomly sample of 3 texts.                           The text stayed on screen for a maximum of 90 seconds,
   8 Full details about how these two associative models were cre-              9 Full details about the recognition test word lists were cre-
ated, including the code implementing them, can be found at                  ated, including the code implementing this, can be found at
https://github.com/lawsofthought/gallium.                                    https://github.com/lawsofthought/berkelium.
                                                                        72

   but after 45 seconds, participants were able to move on the       recognition memory data using the same random effects lo-
   next screen if they so wished.                                    gistic regression but using a different predictor variable in
                                                                     each case. The logistic regression model is
• On the following screen participants were asked to play the                         
   computer game Tetris for exactly 60 seconds.                                    pi
                                                                         log              = α + αsi + αti + (βsi + βti + β)φi + bxi ,
                                                                                1 − pi
• At the completion of the game, participants proceeded to
   the memory task. For each participant and for each text,          where i indexes the experiment trial, pi is the probability of
   the memory test was randomly chosen to be either a recog-         the participant responding “present” to the word presented on
   nition or a recall task.                                          trial i, si is the identity of the participant on trial i, ti is the
                                                                     identity of the text on trial i, φi is the log of the model’s pre-
   – For the recognition test, the 20 test items were presented      dicted probability of the word on trial i, xi indicates if the
      on screen, one word at a time, with an inter-stimulus-         word on trial i was present in text ti . The random effects re-
      interval of 2 seconds. They remained on screen for 5           gression coefficients are αsi , αti , βsi , βti , which are modelled
      seconds or until the subject indicated with a button press     as drawn from zero-mean Normal distributions.
      whether the word shown was present or absent from the             Having fit the logistic regression model using the predic-
      text. No feedback was given after each response.               tions of the HDPMM topic model, the co-occurrence based
   – If the participant was assigned to the recall test, a screen    model, the association norm based model, and a null model
      of a list of small empty text boxes was presented where        (where φi is set to 0 for all i), we calculate model fit statis-
      and they were asked to type as many words as they could        tics such as BIC, AIC, and Deviance. They are shown in the
      remember, one word into each text box. Initially, 10           following table:
      empty texts boxes were presented, and more boxes could
      be added with a button press.                                                    HDPMM         Co-occur          Assoc     Null
                                                                               BIC     5775.68        5824.33         6083.58   6212.77
• Upon completion of the memory test, participants were
                                                                               AIC     5715.97        5764.62         6023.87   6186.23
   given the option of pausing or proceeding to the next test.
                                                                         Deviance      5697.97        5746.62         6005.87   6178.23
   Each participant performed three tests in total, with the
   three texts to which they were assigned being always ran-         We will concentrate on the BIC results as the loge of the
   domly sampled from the set of 50 texts.                           Bayes Factor comparing any model M0 to model M1 can be
                                                                     approximated by half the difference of the BIC of models M1
   The experiments were presented using the Wilhelm10 web-           and M0 . Thus, the loge of the Bayes factor comparing the
browser based experiment presentation software that was              HDPMM predictions to those of the co-occurrence based asso-
hosted at https://www.cognitionexperiments.org. This soft-           ciation model is 24.32. By any standard, this is overwhelming
ware allowed the experiment to be done any web-browser               evidence in favour of the predictions of the HDPMM relative
based device, e.g., phones, tablets, laptops and desktops.           to those of the co-occurrence model. For example, Kass and
                                                                     Raftery (1995) argue that a log Bayes factor on a log10 scale
Results
                                                                     that is greater than 2.0 is already decisive evidence in favour
For more information about the results, see Footnote11 .             of the better model. In our case, our loge result of 24.32 is
Descriptives In the recognition memory tests trials, the             10.42 on a log10 scale. As the BIC of the association norm
overall accuracy rate was 76%. Overall, the false positive           model is even greater than that of the co-occurrence model,
rate, i.e. where participants responded “present” to words           there is overwhelming evidence in favour of the HDPMM rel-
that were not present in the text they read, was 27%. The            ative to the comparison models.
false negative rate, i.e. where participants responded “absent”         For the recall memory task results, each set of recalled
to words that actually were present in the text, was 22%. For        words by a participant on any given test j, which we will de-
the recall tests, the median number of recalled words per each       note by ω j = ω j1 , ω j2 . . . ω jn , can be reasonably viewed as
test was 7, with between 2 and 15 words recalled in 95% of           draws from a subjective probability distribution that is the
tests. The overall accuracy of recall was 70%, and thus there        participant’s memory representation of the contents of the
was an overall false recall rate of 30%.                             text. We can calculate the likelihood of this data according
                                                                     to the probability distribution defined by any of our models,
Model evaluation For the recognition memory data, we                 denoted generically by ψ, as follows:
model how well each model predicts the behavioural results
using a random effects logistic regression model. In other                                        n    V                 V
                                                                                                             I(ri =v)        r
words, for each of the models being evaluated, we fit the                           P(ω j |ψ) = ∏ ∏ ψv                = ∏ ψv jv
                                                                                                i=1 v=1                 v=1
  10 This    is open-source software and is available at
https://github.com/lawsofthought/wilhelmproject                      where I(·) is an indicator variable that takes the value of 1 if
  11 All raw data, and code for all analyses, can be found at        its argument is true, and r jv is the number of times that word
https://github.com/lawsofthought/gallium.                            wv occurs in ω j , which in this case will be either r jv = 1 if
                                                                  73

word wv was recalled and r jv = 0 otherwise. The loge of the         Gallo, D. (2006). Associative illusions of memory: False
likelihood of all the recall memory task data is                       memory research in DRM and related tasks. Psychology
          L                    L  V          L  V
                                                                       Press.
                                      r                              Ghosh, V. E., & Gilboa, A. (2014). What is a memory
    loge ∏ P(ω j |ψ) = loge ∏ ∏ ψv jv = ∑ ∑ r jv loge ψv .
         j=1                   j v=1         j v=1                     schema? A historical perspective on current neuroscience
                                                                       literature. Neuropsychologia, 53, 104–114.
These results are presented in the following table:
                                                                     Graesser, A., Singer, M., & Trabasso, T. (1994). Construct-
                       HDPMM        Co-occur         Assoc             ing inferences during narrative text comprehension. Psy-
                                                                       chological review, 101(3), 371-395.
          logLik    -14109.02     -15100.94     -16039.98
                                                                     Griffiths, T. L., Steyvers, M., & Tenenbaum, J. B. (2007).
       Deviance       28218.03      30201.88      32079.96
                                                                       Topics in semantic representation. Psychological Review,
Given that the deviance is equal to the BIC plus a constant            114(2), 211-244.
term, the difference of the deviances is identical to the differ-    Hemmer, P., & Steyvers, M. (2009a). A Bayesian account of
ence of the corresponding BIC’s. Approximating the loge of             reconstructive memory. Topics in Cognitive Science, 1(1),
the Bayes factor by half this difference, we therefore calculate       189–202.
a log10 Bayes factor for the evidence for the HDPMM predic-          Hemmer, P., & Steyvers, M. (2009b). Integrating episodic
tions relative to those of the nearest model, the co-occurrence        and semantic information in memory for natural scenes. In
based association model, as 430.79. On the basis of the in-            Proceedings of the 31th annual conference of the cognitive
terpretation described above, this is again overwhelming evi-          science society (pp. 1557–1562).
dence in favour of the HDPMM.                                        Hemmer, P., & Steyvers, M. (2009c). Integrating episodic
                                                                       memories and prior knowledge at multiple levels of ab-
                          Discussion                                   straction. Psychonomic Bulletin & Review, 16(1), 80-87.
In this paper, we have proposed — and then tested using a            Kass, R. E., & Raftery, A. E. (1995). Bayes factors. Journal
high powered behavioural experiment — a Bayesian account               of American Statistical Association, 90(430), 773-795.
of how we form memories for spoken and written language.             Kintsch, W. (1988). The role of knowledge in discourse com-
This account models how we use our background knowledge                prehension - A construction integration Model. Psycholog-
to form memories as a process of Bayesian inference of the             ical Review, 95(2), 163-182.
statistical patterns that are inherent in each text, followed by     Mandler, J. M., & Johnson, N. S. (1977). Remembrance of
posterior predictive inference of the words that are typical           things parsed: Story structure and recall. Cognitive psy-
of those inferred patterns. We have implemented this model             chology, 9(1), 111–151.
specifically as a HDPMM and applied it to an approximately           Nelson, D., McEvoy, C., & Schreiber, T. (2004). The uni-
80m word corpus of texts taken from the BNC. This allowed              versity of south florida word association, rhyme and word
us to make predictions of the probability of remembering any           fragment norms. Behavior Research Methods, Instruments,
given word in each text from a sample of texts taken from the          & Computers, 36, 408-420.
BNC . We tested these predictions in a behavioural experiment        Rawson, K. A., & Kintsch, W. (2002). How does background
with 216 participants. The results of the analysis from both           information improve memory for text content? Memory &
the recognition and recall data provided overwhelming evi-             cognition, 30(5), 768–778.
dence in favour of the Bayesian model relative to non-trivial        Roediger, H. L., Watson, J. M., McDermott, K. B., & Gallo,
alternative models.                                                    D. A. (2001). Factors that determine false recall: A mul-
                                                                       tiple regression analysis. Psychonomic Bulletin & Review,
                          References                                   8(3), 385-407.
Bartlett, F. C. (1932). Remembering: A study in experimental         Schank, R. C., & Abelson, R. P. (1977). Scripts, plans,
   and social psychology. Cambridge: Cambridge University              goals, and understanding: An inquiry into human knowl-
   Press.                                                              edge structures. Hillsdale, NJ: Lawrence Erlbaum Asso-
Blei, D. M. (2012). Probabilistic topic models. Communica-             ciates.
   tions of the ACM, 55(4), 77–84.                                   Steyvers, M., & Griffiths, T. (2007). Probabilisitic topic
BNC Consortium. (2007). The British National Corpus, ver-              models. In T. Landauer, D. McNamara, S. Dennis, &
   sion 3 (BNC XML Edition). Oxford University Computing               W. Kintsch (Eds.), Handbook of latent semantic analysis.
   Services: http://www.natcorp.ox.ac.uk/.                             Psychology Press.
Bower, G., Black, J., & Turner, T. (1979). Scripts in memory         Teh, Y. W., Jordan, M. I., Beal, M. J., & Blei, D. M. (2006).
   for text. Cognitive Psychology, 11(2), 177-220.                     Hierarchical Dirichlet processes. Journal of the American
Bransford, J. D., Barclay, J. R., & Franks, J. J. (1972).              Statistical Association, 101(476), 1566-1581.
   Sentence memory: A constructive versus interpretive ap-           Zwaan, R. A., & Radvansky, G. A. (1998). Situation models
   proach. Cognitive psychology, 3(2), 193–209.                        in language comprehension and memory. Psychological
De Deyne, S., & Storms, G. (2017). Small world of words,               bulletin, 123(2), 162.
   www.smallworldofwords.org.
                                                                  74

